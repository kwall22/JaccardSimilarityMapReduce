[{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/10/3/24259479/senator-laphonza-butler-ai-california-workforce-harris","category":"Policy","date":"Two hours ago","author":"Lauren Feiner","title":"Senator Laphonza Butler thinks supporting Big AI or human workers is a ‘false choice’","content":"Representing California in Congress comes with a unique challenge: navigating national politics while reflecting the interests of the most populous state in the US, including a large constituency from the tech industry. It’s a challenge both current California Sen. Laphonza Butler and Vice President Kamala Harris — who previously held that title — have taken on. And right now, governing the tech world means addressing AI. Congress hasn’t made much headway on a national framework for regulating generative AI. But California is the epicenter of the AI industry, home to companies like OpenAI and Google. On the national stage, Harris has acted as an AI czar within the Biden administration, leading discussions with industry players and civil society leaders about how to regulate it. Butler, who has a long history with the VP, is focusing on a specific problem: how AI systems impact labor and social equity. Butler spoke with The Verge about balancing the interests of AI companies and the people their products impact, including workers who fear being automated out of a job. “It all starts with listening,” says Butler, a former labor leader. “It starts with listening to both the developers, the communities potentially impacted negatively, and the spaces where opportunity exists.” A balancing act Like many officials, Butler says she wants to help protect Americans from the potential dangers of AI without stifling opportunities that could come from it. She praised both Schumer and the Biden administration for “creating spaces for communities to have [a] voice.” Both have brought in labor and civil society leaders in addition to major AI industry executives to educate and engage on regulation in the space. Butler insists lawmakers don’t need to make “false choices” between the interests of AI company executives and the people who make up the workforce. “Listening is fundamental, balancing everyone’s interest, but the goal has to be to do the most good for the most people. And to me, that is where a policymaker will always tend to land.” California state Senator Scott Wiener made similar statements about his hotly contested state-level bill, SB 1047. The bill, which would have required whistleblower protections and safeguards for potentially disastrous events at large AI companies, made it all the way to Gov. Gavin Newsom’s desk before being vetoed, with companies like OpenAI warning it would slow innovation. In August, Wiener argued that “we can advance both innovation and safety; the two are not mutually exclusive.” So far, however, lawmakers are struggling to find a balance between the two. More work to do Butler praises the steps both Schumer and the Biden-Harris administration have taken so far to create appropriate guardrails around AI but says “there’s always more to do.” Schumer laid out a roadmap earlier this year about how to shape AI policy (though it didn’t specifically introduce actual legislation), and the White House has secured voluntary commitments from AI companies to develop the technology safely. One of Butler’s recent contributions is the Workforce of the Future Act, which she introduced with Sen. Mazie Hirono (D-HI). The bill would direct the Department of Labor, National Science Foundation, and Department of Education to study the impact of AI across job sectors, and it would create a $250 million grant program to prepare workers for the skills they’ll need in the future, especially in industries likely to see job displacement. “Hopefully, by both readying the work workforce of today but also preparing the workforce of tomorrow, we’ll be able to catch the full opportunity that is the deployment of artificial intelligence,” Butler says. Butler sees this as a moment in US history where policymakers could “get ahead of what we know is going to be eventual disruption and try to create a pipeline of opportunities that can again help to both stabilize our economies by creating equitable opportunity.” But Butler is realistic about the dynamics of Congress and the upcoming election in just over a month. “You and I both know that this 118th Congress is rapidly coming to a close, with a lot of business in front of it right now,” she says. And Butler believes legislators still need to have important conversations with people representing different sides of the issue before advancing comprehensive AI legislation. And there’s also, she notes, the small issue of “getting through the next presidential election this November.”"},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/10/3/24261238/meta-ray-ban-update-reminders-voice-messages","category":"Meta","date":"Two hours ago","author":"Emma Roth","title":"A new Ray-Ban Meta update adds reminders and voice messages","content":"Meta is bringing reminders to its Ray-Ban smart glasses, a feature it previewed during its Connect event last month. With the update, you can ask Meta AI to remember your surroundings, like where you parked, and even time up reminders to make a phone call. The other features coming with the update include the ability to send and record voice messages on WhatsApp or Messenger without taking out your phone. You can now ask Meta AI to scan QR codes or call phone numbers that you come across, too. Meta is also updating how you invoke Meta AI while wearing the smart glasses. Instead of saying “Hey Meta” before each question, you now only need to say “Hey Meta” for your first question and then ask any additional questions without the prompt. You also no longer have to tell Meta AI to “look and” when you’re asking about something you see. Meta’s AI assistant is currently only available in the US and Canada. You can access the new features by updating the Meta View app on iOS and Android to version 186, which started rolling out on Wednesday. Along with this update, Meta revealed that its limited edition translucent Ray-Bans have already sold out online (but some may still be available in stores). It’s been almost one year since the launch of the $299 Ray-Ban Meta glasses, and they’ve shown a lot of promise, especially when Meta rolled out the ability to process images, text, and audio earlier this year. In future updates, Meta will launch the ability for its smart glasses to translate speech in real time, starting with English, French, Italian, and Spanish. It’s not clear if Meta plans to address the possibility of bad actors using the smart glasses as a way to dox people’s identities, which is an issue two college students recently called attention to."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/10/3/24261161/microsoft-copilot-future-hardware-ai-notepad","category":"Microsoft","date":"4:00 PM UTC","author":"Tom Warren","title":"How Microsoft is thinking about the future of Copilot and AI hardware","content":"It’s been a big week for Copilot and Microsoft’s AI efforts. Microsoft unveiled a redesigned Copilot that’s aimed at consumers, complete with voice capabilities and the ability to understand what you’re looking at on a computer. These new Copilot Voice and Vision features feel like a key evolution in Microsoft’s effort to make Copilot a more personal companion and hint at what’s to come for AI experiences. I got to speak to Windows and Surface chief Pavan Davuluri and consumer chief marketing officer Yusuf Mehdi to better understand the future of AI for Copilot and Windows. Microsoft is thinking about totally reimagined apps on Windows thanks to AI and the possibility of dedicated Copilot hardware in the future. The redesigned Copilot is unlike anything I’ve seen Microsoft release in recent years. After hiring key Inflection AI staff earlier this year, including Microsoft AI CEO Mustafa Suleyman, the new team has moved to quickly and clearly exert its influence on Microsoft’s consumer efforts. The marketing videos are slicker, more friendly, and have less of a corporate feel. The new Copilot experience itself also looks more like Inflection AI’s Pi chatbot, with new visual elements and prompts that are designed to get you interacting with Copilot more. It’s all very different from how Copilot started out inside of the Bing search engine."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/10/3/24260637/googles-ai-overview-ads-launch","category":"Google","date":"4:00 PM UTC","author":"Emma Roth","title":"Google’s AI search summaries officially have ads","content":"Google is rolling out ads in AI Overviews, which means you’ll now start seeing products in some of the search engine’s AI-generated summaries. Let’s say you’re searching for ways to get a grass stain out of your pants. If you ask Google, its AI-generated response will offer some tips, along with suggestions for products to purchase that could help you remove the stain. The products will appear beneath a “sponsored” header, and Google spokesperson Craig Ewer told The Verge they’ll only show up if a question has a “commercial angle.” Google has been testing ads in AI Overviews since May but says it’s moving forward with a full rollout because it helps people “quickly connect with relevant businesses, products and services to take the next step at the exact moment they need them.” For now, ads are only coming to AI Overviews in the US on mobile. Microsoft similarly includes ads in its Copilot chatbot and recently changed how they surface in responses. Google is also making some tweaks to the formatting of AI Overviews. Following a test in August, Google will now display cited webpages more prominently on the right side of the summary, as it found this “has driven an increase in traffic to supporting websites compared to the previous design.” It’s rolling out AI-organized search pages as well — a feature that displays a custom results page with relevant information instead of showing just a list of links. This is currently only available on mobile in the US for searches related to recipes and meal ideas."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/10/3/24259759/google-lens-search-video-ai-launch","category":"Google","date":"4:00 PM UTC","author":"Emma Roth","title":"Google Lens now lets you search with video","content":"If you can’t capture what you want to search for with just a picture, Google Lens will now let you take a video — and even use your voice to ask about what you’re seeing. The feature will surface an AI Overview and search results based on the video’s contents and your question. It’s rolling out in Search Labs on Android and iOS today. Google first previewed using video to search at I/O in May. As an example, Google says someone curious about the fish they’re seeing at an aquarium can hold up their phone to the exhibit, open the Google Lens app, and then hold down the shutter button. Once Lens starts recording, they can say their question: “Why are they swimming together?” Google Lens then uses the Gemini AI model to provide a response, similar to what you see in the GIF below. When speaking about the tech behind the feature, Rajan Patel, the vice president of engineering at Google, told The Verge that Google is capturing the video “as a series of image frames and then applying the same computer vision techniques” previously used in Lens. But Google is taking things a step further by passing the information to a “custom” Gemini model developed to “understand multiple frames in sequence... and then provide a response that is rooted in the web.” There isn’t support for identifying the sounds in a video just yet — like if you’re trying to identify a bird you’re hearing — but Patel says that’s something Google has been “experimenting with.” Google Lens is also updating its photo search feature with the ability to ask a question using your voice. To try it, aim your camera at your subject, hold down the shutter button, and then ask your question. Before this change, you could only type your question into Lens after snapping a picture. Voice questions are rolling out globally on Android and iOS, but it’s only available in English for now."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/10/3/24261160/elon-musk-xai-recruiting-party-openai-dev-day-sam-altman","category":"Elon Musk","date":"3:54 PM UTC","author":"Kylie Robison","title":"Inside Elon Musk’s AI party at OpenAI’s old headquarters","content":"It had all the makings of a typical recruiting event for a tech startup in San Francisco. There was free food, drinks, and live music generated via code being written in real time. But there were also mandatory metal detector screenings, ID checks, and security guards everywhere. It was held by Elon Musk at the original Mission District headquarters of OpenAI, which Musk cofounded before leaving after (reportedly) failing to take it over. And Musk was there to convince people to join his latest startup, xAI. The timing felt intentional. That same day, OpenAI was hosting its annual Dev Day across town, where CEO Sam Altman had spoken hours earlier to a packed auditorium of developers. The Silicon Valley rumor mill was buzzing about OpenAI closing in on the largest round of funding ever for a startup, surpassing the amount Musk himself had just raised for xAI four months earlier. Around 8:30PM, the AI-generated music cut off, and Musk, surrounded by bodyguards, climbed onto a table in a sectioned-off area to address the room of engineers. He began talking about why he started xAI and moved it to the same office where he helped launch OpenAI nearly a decade ago. “We want to create digital superintelligence that is as benign as possible,” Musk said at the Tuesday gathering, according to a partial recording of his remarks shared with The Verge. He then called on those in the crowd “to join xAI and help build the intelligence and build useful applications to derive from that intelligence.” For about an hour and a half, Musk took questions from the (predominantly male) audience, according to people in attendance. He said he believes we’ll reach artificial general intelligence (AGI) in a couple of years; he hopes to build a “supersonic jet company” next; he plans to open-source xAI’s models roughly nine months after they’re released; and most importantly, he wants to move fast. He compared xAI’s growth to the SR-71 Blackbird, an airplane that flew three times the speed of sound and provided the US with enemy information during the Cold War. “No SR-71 Blackbird was ever shot down, and it only had one strategy: to accelerate,” Musk told the room per a post on X from an attendee. He predicted that OpenAI, Anthropic, Google, and xAI will be the main players in the AI race for the next five years. The goal of the party was to find engineers for xAI’s API, one attendee said. Ultimately, he said he aspires for xAI to be as dominant in AI as SpaceX is in rockets. At 10PM, the fire marshal put an end to the recruiting event. Musk was briskly escorted out a backdoor with his security detail. Attendees, including some wearing OpenAI backpacks, walked out into the night with pizza slices. Maximum, truth-seeking AI xAI began in March 2023 on the 10th office floor of X, Musk’s social media platform formerly known as Twitter. He assembled a team drawn from his other companies, including Tesla and SpaceX, as well as his 17-year-old son, his cousins, and the son of Jared Birchall, who runs his family office, The Verge has learned. The mission: surpass OpenAI and deliver a competitive large language model in just three months. Since then, xAI has expanded from a single floor at X to a larger office in the Stanford Research Park in Palo Alto. Musk tapped Igor Babuschkin, a former Google DeepMind researcher, to lead xAI. He also recruited researchers from OpenAI, Microsoft, and Meta. In May, xAI secured $6 billion in funding from several high-profile investors, including Andreessen Horowitz, Lightspeed Venture Partners, and Sequoia Capital, valuing the company at $24 billion. Investors in X own 25 percent of xAI, which benefits from the wealth of training data the social media platform produces every day. Under pressure from Musk, xAI’s first model was launched in late 2023 via Grok, a chatbot for paid subscribers to X. It has since released three updates: Grok-1.5, Grok-2, and Grok-2 Mini. But unlike competitors who had the luxury of time to develop their own systems, xAI’s lean team had to move fast and find a quick solution. One person familiar with the development of the first model described it as a patchwork product that relied on Microsoft’s Bing for search and Meta’s open-source Llama model for query rewriting. Musk is still relying on outside technology for core Grok features. Just over a month ago, xAI announced a deal with Black Forest Labs to power image generation. The feature lacked guardrails put in place by other image generators, allowing people to generate photos of anything from Taylor Swift in lingerie to Kamala Harris with a gun. Musk said on X that xAI was working on its own generator but that the Black Forest partnership let it launch one in Grok more quickly. A person familiar with what xAI is working on tells The Verge that voice and search features are in the works. The idea is that, like OpenAI and Meta’s voice modes, Grok will be able to talk back. Musk also wants it to provide summaries of news stories shared on X and trending topics. Musk is now grappling with fierce competition in the AI race for top engineering talent and GPUs. As the richest person in the world, money isn’t an issue for him, though — despite the financial pressure that X’s plummeting value has created. While he’s raised billions of dollars for xAI, Musk doesn’t exactly need to create a profitable AI company any time soon — for him, taking down his runaway rival OpenAI seems to be satisfying enough. Musk cofounded OpenAI with its CEO Sam Altman and a group of partners in 2015, but Musk quit the board only three years later. At the time, he cited “potential future conflict” due to Tesla’s focus on AI. Later, he claimed he quit due to disagreements with the OpenAI team. And in March, he sued the company, alleging (fairly dubiously) that it broke a contract with him and abandoned its mission. In response, OpenAI shared emails between its leadership and Musk that revealed a power struggle where Musk planned to take sole control over the company. “I just don’t trust OpenAI for obvious reasons,” Musk said during the recruiting party. “It is closed, for-maximum-profit AI.” The falling out between Musk and OpenAI has evolved into a stiff game of one-upmanship. This week, OpenAI raised $6.6 billion at a $157 billion valuation, just outpacing Musk’s historic funding round. Musk leveraged Tesla GPUs to build a data center nicknamed “Colossus” and reportedly brought online 100,000 advanced Nvidia chips last week. Meanwhile, Altman is on a global mission, meeting with UAE leaders, Asian chipmakers, and US officials to raise $7 trillion for 36 semiconductor plants and data centers, all aimed at advancing OpenAI’s pursuit of AGI. After Altman’s latest funding round, he reportedly asked backers to not invest in competitors like xAI. Perhaps just as tough as nailing down funding or compute power is securing the top AI talent in Silicon Valley. The best researchers can easily earn millions, and the timing has never been better for them to launch their own startups. Many are driven by their own altruistic visions for AI’s future, making their choice of company often based on its mission and leadership. While Musk’s fame and bold visions give him an edge, it doesn’t mean the battle for talent is any easier. So, Musk gathered a few hundred young engineers from rivals OpenAI and DeepMind — some fresh from attending his competitors’ developer conference that very day — to do what he does best: sell his vision of the future. In Musk’s world, AGI isn’t controlled by companies like OpenAI or Google, who keep their best models private. Instead, it’s owned by him and shared with the world. One potential draw for working at xAI, compared to a larger competitor like OpenAI, might be the opportunity to move faster and take bolder risks. With a small team and shorter product timelines, xAI offers a chance to innovate quickly, unlike larger, more cautious companies like OpenAI and Google. It may attract those eager to see rapid AI proliferation or who align with Musk’s techno-libertarian leanings — people who reject Silicon Valley’s “wokeness” in favor of a “super hardcore” work environment that prioritizes ambition and agility over corporate safeguards. “My personal belief is that the best way to achieve AI safety is to have a maximum, truth-seeking AI,” Musk said at the recruiting party. Like a lot of things at Musk companies, the event came together rapidly. “He said that he had the idea for this event last Wednesday and that the office was unfurnished then,” said one source who attended the party. But the source called it an “overall chill event” focused on answering questions about not just AI but also Musk’s many other enterprises. “Elon’s committed to winning and accelerating xAI,” another source said."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/10/3/24260561/gemini-live-language-support","category":"Google","date":"1:00 PM UTC","author":"Allison Johnson","title":"Gemini Live will support more languages for its AI voice chat","content":"Google initially rolled out Gemini Live, its conversational AI voice chat, in just one language: English. Today, the company is expanding the service to a handful of other languages, starting with French, German, Portuguese, Hindi, and Spanish. And while support for these languages does appear to be imminent for a lot of people, the company is still couching promises of other Gemini features with fuzzy “coming weeks” timelines. Google expects that the languages starting to roll out today will be available to all users “in a couple of weeks.” As for other languages, Google says it will have more than 40 languages over “the coming weeks,” which is harder to pin down. Still, Google’s timeline from announcement to full rollout for Gemini Live has been unusually swift: it was revealed with the Pixel 9 series in mid-August as a subscriber-only feature. Just a month later, the company opened it up as a free feature for all Android users. Meanwhile, expanded capabilities for the Gemini virtual assistant — the non-voice mode, that is — have been on a slower track. Earlier this year at I/O, Google announced that Gemini extensions for Calendar, Tasks, and Keep were on the way. The company’s update today reinforces that promise, reminding us that these extensions will allow Gemini to do things like add a bunch of dates from a flyer to your calendar or build a shopping list from a picture of a recipe. Really useful stuff! But when’s it all arriving? “Over the coming weeks.” Sounds familiar."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/10/2/24260457/openai-funding-round-thrive-capital-6-billion","category":"Tech","date":"Oct 2","author":"Kylie Robison","title":"OpenAI just raised $6.6 billion to build ever-larger AI models","content":"OpenAI just closed a historic funding round, taking in a $6.6 billion investment at a $157 billion valuation, to continue pursuing its mission to build artificial-general intelligence according to a company blog post. The funding round was led by Thrive Capital, which committed $1 billion, according to the Financial Times. It was also reported that Thrive got a special deal (not offered to other investors) that allows it to invest another $1 billion next year at the same valuation if the AI firm hits a revenue goal, Reuters reported. These funds are apparently contingent on OpenAI going through with a rumored restructure as a for-profit company. The company’s for-profit wing is currently overseen by a nonprofit research body, and investor profits are capped at 100x. If OpenAI doesn’t restructure itself as a for-profit company within two years, Axios reported, investors can ask for their money back. Last week, Reuters reported that the company is considering becoming a public benefit corporation (like Anthropic). In a rare move, OpenAI also asked investors to avoid backing rival start-ups such as Anthropic and Elon Musk’s xAI, the Financial Times reported. It’s worth noting that OpenAI’s latest funding round just barely surpasses xAI, which raised $6 billion in May. This funding round values OpenAI at roughly 40 times its reported revenue, an unprecedented figure that highlights just how much hype surrounds AI in Silicon Valley. The New York Times reported that OpenAI’s monthly revenue hit $300 million in August, and the company expects about $3.7 billion in annual sales this year (and estimates that its revenue will reach $11.6 billion next year.) These billions will go toward the incredibly expensive task of training AI frontier models. Anthropic CEO Dario Amodei has said AI models that cost $1 billion to train are in development and $100 billion models are not far behind. For OpenAI, which wants to build a series of “reasoning” models, those costs are only expected to balloon — making fresh funding rounds like this one critical."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/10/2/24260392/amazon-fire-hd-8-tablet-ai","category":"Amazon","date":"Oct 2","author":"Jay Peters","title":"Amazon’s new Fire tablets have AI inside","content":"Amazon just announced a new Fire HD 8 tablet, and like many new gadgets in 2024, generative AI-powered tools are among the big new features. But you won’t necessarily need to buy the new Fire HD 8 to try them; Amazon says they’re coming to Fire HD 10 and Fire Max 11 tablets, too. The new AI tools are things you’ve probably seen before. There’s a writing assist feature, which will help you polish up your writing and is built into the device’s keyboard. You’ll be able to get webpage summaries when using Amazon’s Silk browser. And you’ll be able to create a wallpaper from a prompt. Amazon says the tools will start rolling out to “all compatible Fire tablets later this month.” Writing Assist and Wallpaper Creator will come to the Fire Max 11 (2023), Fire HD 10 (2023), and Fire HD 8 (2022 and 2024), according to Amazon spokesperson Jackie Burke. Webpage Summaries will come to the Fire Max 11 (2023), Fire HD 10 (2019, 2021, 2023), and Fire HD 8 (2018, 2020, 2022 and 2024). Amazon is also rumored to be launching an upgraded Alexa that you might have to pay for, but that wasn’t included as part of this announcement. If you’re interested in the new Fire HD 8, it has some improved specs that seem to have trickled down from 2022’s Fire 8 HD Plus, including 3GB of RAM (up from 2GB — the boost might help with the new AI features) and a 5MP back camera (up from 2MP). Like with the previous Fire 8 HD, Amazon is also promising up to 13 hours of battery life and is offering 32GB or 64GB of storage (with the option to expand that with a microSD card). You can buy the new Fire 8 HD right now in black, emerald, and hibiscus. The tablets technically have a starting price of $99.99, but they’re already on sale ahead of Amazon’s Prime Big Deal Days, meaning you can get the base model for nearly half off at $54.99. Amazon might also launch updated Kindle e-readers soon, as a Spanish retailer recently listed a new entry-level model with a brighter screen. Update, October 2nd: Added information about which Amazon tablets will get certain AI features."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/10/1/24259261/microsoft-event-copilot-ai-windows","category":"Microsoft","date":"Oct 1","author":"Umar Shakir","title":"All the news on Microsoft’s latest Copilot and Windows AI features","content":"At Microsoft’s New York City event today, it’s announcing an all-new Copilot experience. The new Copilot design includes a new card-based look across mobile, web, and Windows. Copilot is getting more personalized with features like Copilot Vision, which adds the ability to see what you’re looking at, an OpenAI-like natural voice conversation mode, and a virtual news presenter mode that can read the headlines to you. Windows 11 is getting new features like Phone Link status in the start menu that can show notifications and your phone’s battery life. And both Paint and Photos are getting fun new features like Generative Fill and Erase. Copilot Plus PCs are getting a revamped AI-powered Windows Search that includes a Google Circle to Search-like “Click to Do” feature and the ability to search for a photo using just a text description. Earlier this year, an exec reorganization put Pavan Davuluri in charge of Windows and Surface and made Mustafa Suleyman the new CEO of AI. Now, a full year after Panos Panay’s abrupt departure, we will find out more about where Microsoft’s “AI PC” push is headed. You can read all the updates from the event below. Microsoft is launching a redesigned version of Copilot today, intent on becoming an AI assistant or companion. To celebrate this, Microsoft’s new AI CEO, Mustafa Suleyman, has penned a 700-plus-word memo on what he describes as a “technological paradigm shift” toward AI models that can understand what humans see and hear. Suleyman joined Microsoft earlier this year as the CEO of its new Microsoft AI division, amid the software giant’s hiring of a number of key Inflection AI staff. In June, Suleyman sparked controversy after brazenly claiming that anything published on the web is “freeware” that can be copied, recreated, and reproduced by AI models. Now, he’s optimistic that AI — under Microsoft’s stewardship — will create a “calmer, more helpful and supportive era of technology, quite unlike anything we’ve seen before.” Microsoft is bringing some new AI-powered Paint and Photos features to Copilot Plus PCs that could make creatives less reliant on more powerful image editing software. Generative Fill and Generative Erase — which appear to be heavily inspired by similar AI tools in Adobe Photoshop — are being introduced to Paint, allowing users to precisely add or remove objects in their images. Both tools utilize a size-adjustable brush to “paint” over specific areas of an image to edit. Generative Erase will remove unwanted figures, objects like background clutter, and other distractions, similar to the Magic Eraser feature on Google’s Pixel phones. Generative Fill allows Paint users to add new AI-generated assets to an image using a text description and select precisely where they should be placed — much like the Photoshop tool that shares the same name. Microsoft is using AI models to greatly improve Windows search on its new Copilot Plus PCs, including the addition of a new Click to Do feature that’s very similar to Google’s Circle to Search. These search improvements will make it easier to find and interact with images, emails, documents, and even videos and are just a few of the AI-based features coming to Copilot Plus PCs starting in November. The improved Windows search will first show up in File Explorer on Copilot Plus PCs next month, allowing you to search for pictures using words, even if the search word isn’t found in the photo or file name. Microsoft is starting to release its Windows 11 2024 update today, also known as version 24H2. This update includes a number of small but useful additions to Windows 11 that improve the Start menu, File Explorer, Settings, and much more. The Start menu has a big change for those using Microsoft’s Phone Link software. The Windows 11 2024 update adds a side panel to the Start menu that provides information on your phone’s battery status and notifications as well as quick access to messages, calls, and photos. The floating panel is a neat addition for Phone Link users. Microsoft is unveiling a big overhaul of its Copilot experience today, adding voice and vision capabilities to transform it into a more personalized AI assistant. As I exclusively revealed in my Notepad newsletter last week, Copilot’s new capabilities include a virtual news presenter mode to read you the headlines, the ability for Copilot to see what you’re looking at, and a voice feature that lets you talk to Copilot in a natural way, much like OpenAI’s Advanced Voice Mode. Copilot is being redesigned across mobile, web, and the dedicated Windows app into a user experience that’s more card-based and looks very similar to the work Inflection AI has done with its Pi personalized AI assistant. Microsoft hired a bunch of folks from Inflection AI earlier this year, including Google DeepMind cofounder Mustafa Suleyman, who is now CEO of Microsoft AI. This is Suleyman’s first big change to Copilot since taking over the consumer side of the AI assistant. Microsoft is working on an overhaul of its Copilot mobile app that includes a new feature that will transform the AI assistant into a virtual news presenter. Multiple sources familiar with Microsoft’s plans tell me that the software giant has been testing a completely redesigned Copilot app in recent weeks that looks unlike any of Microsoft’s other apps. The Copilot redesign surfaces topics you can choose from based on your own interests or your history of asking Copilot questions. The AI assistant might offer to generate a story for you one day, ask if you to do a workout the next, or simply surface the latest sports scores without you having to ask for them."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/10/1/24259209/microsoft-ai-ceo-mustafa-suleyman-copilot-ai-companion-memo","category":"Microsoft","date":"Oct 1","author":"Tom Warren","title":"Read Microsoft’s optimistic memo about the future of AI companions","content":"Microsoft is launching a redesigned version of Copilot today, intent on becoming an AI assistant or companion. To celebrate this, Microsoft’s new AI CEO, Mustafa Suleyman, has penned a 700-plus-word memo on what he describes as a “technological paradigm shift” toward AI models that can understand what humans see and hear. Suleyman joined Microsoft earlier this year as the CEO of its new Microsoft AI division, amid the software giant’s hiring of a number of key Inflection AI staff. In June, Suleyman sparked controversy after brazenly claiming that anything published on the web is “freeware” that can be copied, recreated, and reproduced by AI models. Now, he’s optimistic that AI — under Microsoft’s stewardship — will create a “calmer, more helpful and supportive era of technology, quite unlike anything we’ve seen before.” “Some people worry that AI will diminish what makes us unique as humans,” says Suleyman. “My life’s work has been to ensure it does precisely the opposite.” Microsoft’s new Copilot experience looks a lot like Inflection AI’s Pi product, and it’s clear Suleyman is now pushing Microsoft in a more personalized AI direction. “At Microsoft AI, we are creating an AI companion for everyone,” says Suleyman in his memo. “Copilot will be there for you, in your corner, by your side, and always strongly aligned with your interests.” Microsoft is launching new Copilot Vision and Voice features today to make the AI assistant a lot more personalized, alongside a refreshed design that’s focused on surfacing useful information. “Over time it’ll adapt to your mannerisms and develop capabilities built around your preferences and needs,” says Suleyman. “We are not creating a static tool so much as establishing a dynamic, emergent, and evolving interaction.” Here’s Suleyman’s memo in full: We’re living through a technological paradigm shift. In a few short years, our computers have learnt to speak our languages, to see what we see and hear what we hear. Yet technology for its own sake counts for nothing. What matters is how it feels to people and what impact it has on societies. It’s about how it changes lives, opens doors, expands minds, and relieves pressure. It is perhaps the greatest amplifier of human well-being in history, one of the most effective ways to create tangible and lasting benefits for billions of people. And yet technology is, and must always remain, in service to humanity: an enabler, a path to deepening our common bonds and shared understanding, our energy and imagination, our creativity and capacity for everything from invention to forming relationships. In the field of AI, we often get caught up in the technical details. We spend our time talking about parameters and compute. The focus is on training runs, data centers and the latest techniques. This is natural and inevitable when operating on the frontiers of something new, where the details do really matter. But I think it’s important that in doing all of this, getting stuck right into the technical weeds, that we don’t lose sight, not just of what we are building, but why we are building it. At Microsoft AI, we are creating an AI companion for everyone. I truly believe we can create a calmer, more helpful and supportive era of technology, quite unlike anything we’ve seen before. Great technology experiences are about how you feel, not what’s under the hood. It should be about what you experience, not what we are building.Copilot will be there for you, in your corner, by your side, and always strongly aligned with your interests. It understands the context of your life while safeguarding your privacy, data, and security, remembering the details that are most helpful in any situation. It gives you access to a universe of knowledge, simplifying, and decluttering the daily barrage of information, and offering support and encouragement when you want it. Over time it’ll adapt to your mannerisms and develop capabilities built around your preferences and needs. We are not creating a static tool so much as establishing a dynamic, emergent, and evolving interaction. It will provide you with unwavering support to help you show up the way you really want in your everyday life, a new means of facilitating human connections and accomplishments alike. With your permission, Copilot will ultimately be able to act on your behalf, smoothing life’s complexities and giving you more time to focus on what matters to you. It will be an advocate for you in many of life’s most important moments. It’ll accompany you to that doctor’s appointment, taking notes and following up at the right time. It’ll share the load of planning and preparing for your child’s birthday party. And it’ll be there at the end of the day to help you think through a tricky life decision. Some people worry that AI will diminish what makes us unique as humans. My life’s work has been to ensure it does precisely the opposite. We choose what we create. This is something we must do together. Our task is to ensure it always enriches people’s lives and strengthens our bonds with others, whilst supporting our uniqueness and endlessly complex humanity. This is a new era of technology that doesn’t just “solve problems”, it’s there to support you, teach you, help you. In this sense, Copilots really are different to that last wave of the web and mobile. This is the beginning of a fundamental shift in what’s possible for all of us. It’s a long journey that will take years. With our latest updates to Copilot, you are seeing only the first careful steps in this direction. Patience and care with our deployments are at the very foundation of our approach. My commitment is to be accountable at every stage, to work with you and listen to you. Respect and deep compassion for our users and for society is the core purpose behind everything we do. It comes first. This is a journey we promise to take together. I couldn’t be more excited to embark on it with you.Mustafa Suleyman"},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/10/1/24259239/microsoft-paint-generative-erase-fill-photos-upscaling","category":"Microsoft","date":"Oct 1","author":"Jess Weatherbed","title":"Microsoft Paint is getting Photoshop-like generative AI fill and erase features","content":"Microsoft is bringing some new AI-powered Paint and Photos features to Copilot Plus PCs that could make creatives less reliant on more powerful image editing software. Generative Fill and Generative Erase — which appear to be heavily inspired by similar AI tools in Adobe Photoshop — are being introduced to Paint, allowing users to precisely add or remove objects in their images. Both tools utilize a size-adjustable brush to “paint” over specific areas of an image to edit. Generative Erase will remove unwanted figures, objects like background clutter, and other distractions, similar to the Magic Eraser feature on Google’s Pixel phones. Generative Fill allows Paint users to add new AI-generated assets to an image using a text description and select precisely where they should be placed — much like the Photoshop tool that shares the same name. These build on the Cocreator tool for Paint announced for Copilot Plus PCs earlier this year that can generate images using a combination of text prompts and reference sketches. The company says the diffusion-based model powering these features has been updated to improve output quality and speed and now includes “built-in moderation” to help prevent it from being abused. Microsoft’s Photos app is also getting the Generative Erase tool, alongside a new Super-Resolution feature that uses on-device AI to upscale blurry or pixelated images. Users can upscale images by up to eight times their original resolution and adjust the level of upscaling with a slider, matching the capabilities of tools like Canva’s image upscaler and exceeding Adobe Lightroom’s 4x Super Resolution enhancement. It’s available for free and is speedy enough to upscale images “up to 4K within seconds,” according to Microsoft."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/10/1/24258337/microsoft-windows-ai-features-click-to-do-super-resolution-photos","category":"Microsoft","date":"Oct 1","author":"Tom Warren","title":"Microsoft is using AI to improve Windows search","content":"Microsoft is using AI models to greatly improve Windows search on its new Copilot Plus PCs, including the addition of a new Click to Do feature that’s very similar to Google’s Circle to Search. These search improvements will make it easier to find and interact with images, emails, documents, and even videos and are just a few of the AI-based features coming to Copilot Plus PCs starting in November. The improved Windows search will first show up in File Explorer on Copilot Plus PCs next month, allowing you to search for pictures using words, even if the search word isn’t found in the photo or file name. “AI-powered search makes it dramatically easier to find virtually anything,” says Yusuf Mehdi, executive vice president and consumer chief marketing officer at Microsoft. “You no longer need to remember file names and document locations, nor even specific names of words. Windows will better understand your intent and match the right document, image, file, or email.” This improved search will also be available “in the coming months” in the main Windows search interface and through the search box that appears in the Settings interface. You can type things like “add my headphones” into the Settings search box and it will help you find the right settings. Search hasn’t been great in Windows for years, so this AI-powered natural language search should greatly improve things — as long as it works as well as Microsoft promises. Microsoft is leveraging the NPU chips on new Copilot Plus PCs to enable local search on OneDrive content without having to be connected to the internet. Alongside Windows search, Microsoft will also start rolling out Click to Do on Copilot Plus PCs next month, which is very similar to Google’s Circle to Search feature. With Click to Do, you hit the Windows key on a keyboard and left-click on a mouse to see an interactive overlay appear on your screen, which lets you select images or text to perform clickable actions. “Click to Do works by first understanding everything you’ve seen on your screen and enabling useful shortcuts to actions to help you more quickly search, learn, edit, shop, or act on those items,” explains Mehdi. “It works on any windows, document, image, or even video.” You can use Click to Do on things like a YouTube video you’re watching to do a visual search on an item that appears in the video using Bing. Click to Do is also context-aware, so it can help with text-related actions like rewriting or summarizing documents or explaining text and sending emails. Microsoft will start testing these new search and Click to Do features with Windows Insiders on Copilot Plus PCs in October, followed by a gradual rollout in November. The previously announced Recall feature is also coming to testers in October on Qualcomm-powered devices, before being available to Windows Insiders on Intel- or AMD-powered Copilot Plus PCs in November. Microsoft says “timing details on the broad availability of Recall will be shared soon.” Microsoft is also adding generative fill and erase to Microsoft Paint as part of these new Windows AI features. You can read more about the Paint and Photos AI additions right here."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/9/30/24258134/raspberry-pi-ai-camera-module-sony-price-availability","category":"Tech","date":"Sep 30","author":"Jess Weatherbed","title":"Raspberry Pi and Sony made an AI-powered camera module","content":"Raspberry Pi and Sony have co-developed a Raspberry Pi AI Camera module that’s launching today for $70. It comes with onboard AI processing that can help Raspberry Pi users develop “edge AI solutions that process visual data” with ease, according to the tiny computer maker. The new camera builds on Raspberry Pi’s plans to offer chips and add-ons for AI developers, having previously released several non-AI camera modules since its first 5-megapixel offering in 2013. “AI-based image processing is becoming an attractive tool for developers around the world,” Raspberry Pi CEO Eben Upton said in a press release. “We look forward to seeing what our community members are able to achieve using the power of the Raspberry Pi AI Camera.” The AI camera is compatible with all Raspberry Pi single-board computers, and pairs the company’s RP2040 microcontroller chip with Sony’s IMX500 image sensor — the latter of which handles AI processing. The combination eliminates the need for additional components like accelerators or a graphics processing unit (GPU), which are typically required for camera modules to handle large-scale visual data. The 12.3 megapixel Raspberry Pi AI Camera can capture footage at either 10 frames per second in 4056 x 3040, or 40fps at 2028 x 1520. It also has a manually adjustable focus, a 76-degree field of view, and measures 25 x 24 x 11.9mm — making it almost identical in size to the Camera Module 3 that Raspberry Pi released last year."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/9/29/24232172/california-ai-safety-bill-1047-vetoed-gavin-newsom","category":"Tech","date":"Sep 29","author":"Emma Roth","title":"California governor vetoes major AI safety bill","content":"California Governor Gavin Newsom vetoed the Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047) today. In his veto message, Governor Newsom cited multiple factors in his decision, including the burden the bill would have placed on AI companies, California’s lead in the space, and a critique that the bill may be too broad. “While well-intentioned, SB 1047 does not take into account whether an AI system is deployed in high-risk environments, involves critical decision-making or the use of sensitive data. Instead, the bill applies stringent standards to even the most basic functions — so long as a large system deploys it. I do not believe this is the best approach to protecting the public from real threats posed by the technology.” Newsom writes that the bill could “give the public a false sense of security about controlling this fast-moving technology.” “Smaller, specialized models may emerge as equally or even more dangerous than the models targeted by SB 1047 - at the potential expense of curtailing the very innovation that fuels advancement in favor of the public good.” The Governor says he agrees that there should be safety protocols and guardrails in place, as well as “clear and enforceable” consequences for bad actors. However, he states that he doesn’t believe the state should “settle for a solution that is not informed by an empirical trajectory analysis of Al systems and capabilities.” Here is the full veto message: In a post on X, Senator Scott Wiener, the bill’s main author, called the veto “a setback for everyone who believes in oversight of massive corporations that are making critical decisions” affecting public safety and welfare and “the future of the planet.” “This veto leaves us with the troubling reality that companies aiming to create an extremely powerful technology face no binding restrictions from U.S. policymakers, particularly given Congress’s continuing paralysis around regulating the tech industry in any meaningful way.” In late August, SB 1047 arrived on Gov. Newsom’s desk, poised to become the strictest legal framework around AI in the US, with a deadline to either sign or veto it as of September 30th. It would have applied to covered AI companies doing business in California with a model that costs over $100 million to train or over $10 million to fine-tune, adding requirements that developers implement safeguards like a “kill switch” and lay out protocols for testing to reduce the chance of disastrous events like a cyberattack or a pandemic. The text also establishes protections for whistleblowers to report violations and enables the AG to sue for damages caused by safety incidents. Changes since its introduction included removing proposals for a new regulatory agency and giving the state attorney general power to sue developers for potential incidents before they occur. Most companies covered by the law pushed back against the legislation, though some muted their criticism after those amendments. In a letter to bill author Senator Wiener, OpenAI chief strategy officer Jason Kwon said SB 1047 would slow progress and that the federal government should handle AI regulation instead. Meanwhile, Anthropic CEO Dario Amodei wrote to the governor after the bill was amended, listing his perceived pros and cons and saying, “...the new SB 1047 is substantially improved, to the point where we believe its benefits likely outweigh its costs.” The Chamber of Progress, a coalition that represents Amazon, Meta, and Google, similarly warned the law would “hamstring innovation.” Google spokesperson Jenn Crider said in an emailed statement to The Verge: “We join many small and larger developers in thanking Governor Newsom for helping California continue to lead in building responsible AI tools that benefit California, our nation, and the world. We look forward to working with the Governor’s responsible AI initiative and the federal government on creating appropriate safeguards and developing tools that help everyone.” Meta public affairs manager Jamie Radice emailed Meta’s statement on the veto to The Verge: “We are pleased that Governor Newsom vetoed SB1047. This bill would have stifled AI innovation, hurt business growth and job creation, and broken the state’s long tradition of fostering open-source development. We support responsible AI regulations and remain committed to partnering with lawmakers to promote better approaches.” The bill’s opponents have included former House Speaker Nancy Pelosi, San Francisco Mayor London Breed, and eight congressional Democrats from California. On the other side, vocal supporters have included Elon Musk, prominent Hollywood names like Mark Hamill, Alyssa Milano, Shonda Rhimes, and J.J. Abrams, and unions including SAG-AFTRA and SEIU. The federal government is also looking into ways it could regulate AI. In May, the Senate proposed a $32 billion roadmap that goes over several areas lawmakers should look into, including the impact of AI on elections, national security, copyrighted content, and more. Update September 30th: Updated attribution for Google’s statement to Google spokesperson Jenn Crider."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/9/29/24257391/apple-smart-home-display-homeos-apple-intelligence-homepod-ipad-robot","category":"Apple Rumors","date":"Sep 29","author":"Wes Davis","title":"Apple may release an iPad-like smart home display next year","content":"Apple is preparing to take a fresh run at the smart home that starts with a rumored smart display that it may release next year. That’s according to Bloomberg’s Mark Gurman, who writes in his Power On newsletter today that the display will use a new operating system, called homeOS, that’s based on the Apple TV’s tvOS (much like the software that drives HomePods now.) Gurman reports that the display will run Apple apps like Calendar, Notes, and Home, and that Apple has tested prototypes with magnets for wall-mounting. And it will support Apple Intelligence — something Apple’s HomePods don’t currently do. Rumors of such a device have been going around for some time now, with form factors ranging from a HomePod with a screen to a display attached to a robotic arm that swivels to face you on video calls. But products along those lines have been sounding more real, lately. Another recent rumor suggested that a “HomeAccessory” device coming soon would be square-shaped, and that users might be able to use hand gestures from afar to control it, as 9to5Mac wrote earlier this week. And MacRumors has reported on apparent code references to the device and homeOS. A display like this sounds more down-to-Earth than Apple’s robotic screen idea. It could also be less fiddly and hopefully less expensive than trying to use an iPad as a dedicated smart home controller (I’ve tried; it’s not a great experience!) We’ll find out if and when it launches — which doesn’t sound terribly far off."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/9/27/24255721/microsoft-windows-recall-ai-security-improvements-overhaul-uninstall","category":"Microsoft","date":"Sep 27","author":"Tom Warren","title":"Microsoft’s more secure Windows Recall feature can also be uninstalled by users","content":"In response to security concerns, Microsoft is detailing how it has overhauled its controversial AI-powered Recall feature that creates screenshots of mostly everything you see or do on a computer. Recall was originally supposed to debut with Copilot Plus PCs in June, but Microsoft has spent the past few months reworking the security behind it to make it an opt-in experience that you can now fully remove from Windows if you want. “I’m actually really excited about how nerdy we got on the security architecture,” says David Weston, vice president of enterprise and OS security at Microsoft, in an interview with The Verge. “I’m excited because I think the security community is going to get how much we’ve pushed [into Recall].” One of Microsoft’s first big changes is that the company isn’t forcing people to use Recall if they don’t want to. “There is no more on by default experience at all — you have to opt into this,” says Weston. “That’s obviously super important for people who just don’t want this, and we totally get that.” A Recall uninstall option initially appeared on Copilot Plus PCs earlier this month, and Microsoft said at the time that it was a bug. It turns out that you will indeed be able to fully uninstall Recall. “If you choose to uninstall this, we remove the bits from your machine,” says Weston. That includes the AI models that Microsoft is using to power Recall. Security researchers initially found that the Recall database — that stores snapshots taken every few seconds of your computer — wasn’t encrypted, and malware could have potentially accessed the Recall feature. Everything that’s sensitive to Recall, including its database of screenshots, is now fully encrypted. Microsoft is also leaning on Windows Hello to protect against malware tampering. The encryption in Recall is now bound to the Trusted Platform Module (TPM) that Microsoft requires for Windows 11, so the keys are stored in the TPM and the only way to get access is to authenticate through Windows Hello. The only time Recall data is even passed to the UI is when the user wants to use the feature and authenticates via their face, fingerprint, or PIN. “To turn it on to begin with, you actually have to be present as a user,” says Weston. That means you have to use a fingerprint or your face to set up Recall before being able to use the PIN support. This is all designed to prevent malware from accessing Recall data in the background, as Microsoft requires a proof of presence through Windows Hello. “We’ve moved all of the screenshot processing, all of the sensitive processes into a virtualization-based security enclave, so we actually put it all in a virtual machine,” explains Weston. That means there’s a UI app layer that has no access to raw screenshots or the Recall database, but when a Windows user wants to interact with Recall and search, it will generate the Windows Hello prompt, query the virtual machine, and return the data into the app’s memory. Once the user closes the Recall app, what’s in memory is destroyed. “The app outside the virtualization-based enclave is running in an anti-malware protected process, which would basically require a malicious kernel driver to even access,” says Weston. Microsoft is detailing its Recall security model and exactly how its VBS enclave works in a blog post today. It all looks a lot more secure than what Microsoft had planned to ship and even hints at how the company might secure Windows apps in the future. So, how did Microsoft nearly ship Recall in June without a high amount of security in the first place? I’m still not super clear on that, and Microsoft isn’t giving much away. Weston confirms that Recall was reviewed as part of the company’s Secure Future Initiative that was introduced last year, but being a preview product, it apparently had some different restrictions. “The plan was always to follow Microsoft basics, like encryption. But we also heard from people who were like ‘we’re really concerned about this,’” so the company decided to fast-track some of the additional security work it was planning for Recall so that security concerns weren’t a factor in whether someone wanted to use the feature. “It’s not just about Recall, in my opinion we now have one of the strongest platforms for doing sensitive data processing on the edge and you can imagine there are lots of other things we can do with that,” hints Weston. “I think it made a lot of sense to pull forward some of the investments we were going to make and then make Recall the premier platform for that.” Recall will also now only operate on a Copilot Plus PC, stopping people from sideloading it onto Windows machines like we saw ahead of its planned debut in June. Recall will verify that a Copilot Plus PC has BitLocker, virtualization-based security enabled, measure boot and system guard secure launch protections, and kernel DMA protection. Microsoft has also conducted a number of reviews on the upgraded Recall security. The Microsoft Offensive Research Security Engineering (MORSE) team has “conducted months of design reviews and penetration testing on Recall,” and a third-party security vendor “was engaged to perform an independent security design review” and testing, too. Now that Microsoft has had more time to work on Recall, there are some additional changes to the settings to provide even more control over how the AI-powered tool works. You’ll now be able to filter out specific apps from Recall alongside the ability to block a custom list of websites from appearing in the database. Sensitive content filtering, which allows Recall to filter out things like passwords and credit cards, will also block health and financial data from being stored. Microsoft is also adding the ability to delete a time range, all content from an app or website, in addition to everything stored in Recall’s database. Microsoft says it remains on track to preview Recall with Windows Insiders on Copilot Plus PCs in October, meaning Recall won’t be shipping on these new laptops and PCs until it has been further tested by the Windows community."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/9/27/24255557/meta-orion-quest-smart-glasses-ar-connect-vergecast","category":"Vergecast","date":"Sep 27","author":"David Pierce","title":"Meta’s new smart glasses look like the future","content":"You can’t buy Meta’s most impressive new product, the smart glasses codenamed Orion. You might be able to buy something sort of like them a few years from now, but most of us will never get to so much as wear them. That doesn’t necessarily make them less impressive, though, or less important. Orion is a statement of purpose from Meta: that AR glasses really are the future and that we’re eventually going to get there. On this episode of The Vergecast, The Verge’s Alex Heath joins the show to tell us all about his experience with Orion — two hours in the glasses of the future, playing Pong with Meta CEO Mark Zuckerberg and making smoothies and doing all sorts of other things. He also tells us about his conversation with Zuckerberg (subscribe to Decoder!) about AR, AI, and the future of just about everything. The occasion for all this news was Meta Connect, so we also go through all the other announcements from Connect. There are new smart glasses, new VR headsets, new celebrity AIs to replace the old celebrity AIs, voice modes, and more. But let’s be honest: we mostly talk about Orion. It’s either vaporware, a science project, a prototype, the unquestionable future, or somewhere in the center of the Venn diagram of all those things. And the fact that they exist at all says a lot about where we are in the evolution of technology. Once we finally get done with Meta, we touch on all the executive and corporate changes going on at OpenAI. Then it’s time for a lightning round in which we talk about that new Jony Ive profile, pixel-peep the PS5 Pro, praise Google’s remarkable recent gadget run, and wonder exactly how many people are still using their Rabbit R1. It’s a lot of AI gadgets today. If you want to know more about everything we discuss in this episode, here are some links to get you started, beginning with Orion and Meta’s other new gadgets: Meta Connect 2024: the biggest news and announcementsHands-on with Orion, Meta’s first pair of AR glassesWhy Mark Zuckerberg thinks AR glasses will replace your phoneMeta’s Ray-Bans will now ‘remember’ things for youMeta’s cheaper Quest 3S might just be an upgrade And in other Meta Connect news: Meta’s going to put AI-generated images in your Facebook and Instagram feedsMark Zuckerberg: creators and publishers ‘overestimate the value’ of their work for training AIMeta’s AI can now talk to you in the voices of Awkwafina, John Cena, and Judi DenchKristen Bell told Instagram to ‘get rid of AI’ before she became its official voice And on OpenAI: OpenAI CTO Mira Murati is leavingOpenAI’s for-profit switch could include equity for Sam Altman And in the lightning round: Alex Heath’s pick: Jony Ive confirms he’s working on a new device with OpenAIAlex Cranz’s pick: I played the PS5 Pro, and it’s clearly betterDavid Pierce’s pick: Google Pixel Buds Pro 2 review: big upgrade, much smaller earbudsNilay Patel’s pick: Just 5,000 people use the Rabbit R1 every day"},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/9/27/24255722/google-gemini-gmail-contextual-smart-replies-availability","category":"Google","date":"Sep 27","author":"Jess Weatherbed","title":"Gemini is making Gmail’s smart replies smarter","content":"Google is rolling out a Gemini-powered update to Gmail for Android and iOS that will tailor smart replies more specifically to emails. First announced back in May, Google says its new contextual Smart Replies will “offer more detailed responses to fully capture the intent of your message” by taking the entire content of the email thread into consideration. Users can hover over each of the suggested contextual smart replies to preview the text, and select the option that best matches their needs or writing style. Suggested replies can be edited or sent immediately. The idea is that this will both save time (especially if you’re often buried in your Gmail inbox) and improve the variety of automated responses available beyond a simple “Yes, I’m working on it” or “No worries, thanks for the heads up!” — even adding an initial greeting and a signoff message. The new contextual Smart Replies are now rolling out for Gemini Business, Enterprise, Education, Education Premium, and Google One AI Premium subscribers. The feature is currently only available in English and builds on the original Smart Replies added to Gmail in 2017."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/9/26/24255176/google-notebooklm-summarize-youtube-videos-ai","category":"Google","date":"Sep 26","author":"Emma Roth","title":"Google’s NotebookLM can help you dive deeper into YouTube videos","content":"NotebookLM, Google’s AI note-taking app, can now summarize and help you dig deeper into YouTube videos. The new capability works by analyzing the text in a YouTube video’s transcript, including autogenerated ones. Once you add a YouTube link to NotebookLM, it will use AI to provide a brief summary of key topics discussed in the transcript. You can then click on these topics to get more detailed information as well as ask questions. (If you’re struggling to come up with something to ask, NotebookLM will suggest some questions.) When I dropped in The Verge’s iPhone 16 Pro review, NotebookLM generated topics discussed in the video like “Apple Intelligence,” “iPhone 16 Pro Camera,” and “Photo processing” in its chat window. After clicking on some of the topics, I found that NotebookLM backs up the information provided in its chat window with a citation that links you directly to the point in the transcript where it’s mentioned. You can also create an Audio Overview based on the content, which is a podcast-style discussion hosted by AI. I found that the feature worked on most of the videos I tried, except for ones published within the past two days or so. While Google’s Gemini and Microsoft’s Copilot can both summarize YouTube videos with transcripts, NotebookLM gives you some jumping-off points you can use to learn more about a topic. In addition to adding support for YouTube videos, Google announced that NotebookLM now supports audio recordings as well, allowing you to search transcribed conversations for certain information and create study guides."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/1","link":"https://www.theverge.com/2024/9/26/24255179/deepfake-call-ukraine-senator-cardin-dmytro-kuleba","category":"Tech","date":"Sep 26","author":"Gaby Del Valle","title":"A deepfake caller pretending to be a Ukrainian official almost tricked a US senator","content":"The head of the Senate Foreign Relations Committee took a Zoom call with someone using deepfake technology to pose as a top Ukrainian official, The New York Times reports. Sen. Ben Cardin (D-MD) received an email last Thursday that appeared to be from Dmytro Kuleba, Ukraine’s former foreign minister, asking him to meet on Zoom. The person on the other end of the call reportedly looked and sounded like Kuleba but was acting strangely. He asked Cardin “politically charged questions in relation to the upcoming election,” according to an email Senate security officials sent legislators that was obtained by the Times. The fake Kuleba demanded that Cardin give his opinion on foreign policy questions, including whether he supported firing long-range missiles into Russian territory. The tenor of the conversation made Cardin suspicious, according to the Times report, and he reported it to the State Department. Officials there confirmed that Cardin hadn’t spoken to the real Kuleba but to an imposter, though it’s still unclear who was behind the call. In a statement to the Times, Cardin said that “in recent days, a malign actor engaged in a deceptive attempt to have a conversation with me by posing as a known individual.” Cardin’s statement didn’t disclose who the “known individual” was, but the Senate security email did. Senate security officials told lawmakers to be on the lookout for similar attempts and warned “it is likely that other attempts will be made in the coming weeks.” “While we have seen an increase of social engineering threats in the last several months and years, this attempt stands out due to its technical sophistication and believability,” the Senate security office email obtained by the Times read. As AI tools become easier and cheaper to use, politically motivated deepfakes have increased in frequency — and effectiveness. In May, the Federal Communications Commission proposed levying multimillion-dollar fines on a political consultant behind a robocall campaign that impersonated President Joe Biden. On the call — which targeted New Hampshire voters ahead of the state’s primary election — the fake Biden told voters not to show up to the polls. Elon Musk shared a deepfake video of Vice President Kamala Harris on X in which Harris appeared to call herself “the ultimate diversity hire” who “had four years under the tutelage of the ultimate deep state puppet, a wonderful mentor, Joe Biden.” And former President Donald Trump posted an AI-generated “endorsement” from Taylor Swift on Truth Social in August — which Swift later cited in her real endorsement of Harris."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/2024/9/25/24254042/mark-zuckerberg-creators-value-ai-meta","category":"Tech","date":"Sep 25","author":"Adi Robertson","title":"Mark Zuckerberg: creators and publishers ‘overestimate the value’ of their work for training AI","content":"Meta CEO Mark Zuckerberg says there are complex copyright questions around scraping data to train AI models, but he suggests the individual work of most creators isn’t valuable enough for it to matter. In an interview with The Verge deputy editor Alex Heath, Zuckerberg said Meta will likely strike “certain partnerships” for useful content. But if others demand payment, then — as it’s done with news outlets — the company would prefer to walk away. “I think individual creators or publishers tend to overestimate the value of their specific content in the grand scheme of this,” Zuckerberg said in the interview, which coincides with Meta’s annual Connect event. “My guess is that there are going to be certain partnerships that get made when content is really important and valuable.” But if creators are concerned or object, “when push comes to shove, if they demanded that we don’t use their content, then we just wouldn’t use their content. It’s not like that’s going to change the outcome of this stuff that much.” Meta, like nearly every major AI company, is currently embroiled in litigation over the limits of scraping data for AI training without permission. Last year, the company was sued by a group of authors, including Sarah Silverman, who claimed its Llama model was unlawfully trained on pirated copies of their work. (The case currently isn’t going great for those authors; last week, a judge castigated their legal team for being “either unwilling or unable to litigate properly.”) The company — again, like nearly every major AI player — argues that this kind of unapproved scraping should be allowed under US fair use law. Zuckerberg elaborates on the question: I think that in any new medium in technology, there are the concepts around fair use and where the boundary is between what you have control over. When you put something out in the world, to what degree do you still get to control it and own it and license it? I think that all these things are basically going to need to get relitigated and rediscussed in the AI era. The history of copyright is indeed a history of deciding what control people have over their own published works. Fair use is designed to let people transform and build on each other’s creations without permission or compensation, and that’s very frequently a good thing. That said, some AI developers have interpreted it far more broadly than most courts. Microsoft’s AI CEO, for instance, said earlier this year that anything “on the open web” was “freeware” and “anyone can copy it, recreate with it, reproduce with it.” (This is categorically legally false: content posted publicly online is no less protected by copyright than any other medium, and to the extent you can copy or modify it under fair use, you can also copy or modify a book, movie, or paywalled article.) While the issue is still being debated in lawsuits, a number of AI companies have begun paid partnerships with major outlets. OpenAI, for instance, has struck deals with several news publishers and other companies like Shutterstock. Meta recently signed an agreement with Universal Music Group that included provisions around AI-generated songs. Meanwhile, some artists have turned to unofficial tools that would prevent their work from being used for AI training. But especially for anything posted on social media before the rise of generative AI, they’re sometimes stymied by terms of service that let these companies train on their work. Meta has stated that it trains its AI tools on public Instagram and Facebook posts. Zuckerberg said Meta’s future AI content strategy would likely echo its blunt response to proposed laws that would add a fee for links to news stories. The company has typically responded to these rules by blocking news outlets in countries like Australia and Canada. “Look, we’re a big company,” he said. “We pay for content when it’s valuable to people. We’re just not going to pay for content when it’s not valuable to people. I think that you’ll probably see a similar dynamic with AI.” We’ve known for some time that news isn’t particularly valuable to Meta, in part because moderation of it invites controversy and (according to Meta) it makes users feel bad. (“If we were actually just following what our community wants, we’d show even less than we’re showing,” Zuckerberg said in the interview.) The company’s generative AI products are still nascent, and it’s not clear anyone has figured out what people want from these tools. But whatever it is, most creators probably shouldn’t expect that it will get them paid."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/2024/9/25/24254190/meta-connect-2024-ai-auto-dubbing-lip-syncing","category":"Tech","date":"Sep 25","author":"Jay Peters","title":"Meta is working on recreating influencers with AI","content":"Meta has big ambitions for using AI to help creators, and it showed two impressive demos of what that could look like onstage at Connect today. One version of this involves fully recreating real influencers as AI figures. Meta CEO Mark Zuckerberg presented a live demo of a creator-based AI persona, which looked like the creator, talked like the creator, and tried to respond to questions like the creator would. It was pretty wild to watch. Another tool it’s developing takes Reels and automatically dubs them into another language, maintaining the creator’s voice and even changing the movements of their mouth to match. Zuckerberg presented two videos onstage of Reels by Spanish creators being translated and dubbed into English. The demo wasn’t live, so it’s unclear how well this tech works right now on a typical video, but it was still incredibly impressive to watch. At scale, it seems like this technology has the potential to make videos reach a lot more people, which is something Meta definitely wants to do so it can better compete with platforms like TikTok and YouTube. (Speaking of YouTube, the company has been investing in auto-dubbing, too.) Meta also announced a bunch of other AI-related features at Connect today, including that its AI assistant will be able to talk in the voices of celebrities."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/24254101/meta-connect-2024-announcements-products","category":"Tech","date":"Sep 25","author":"Umar Shakir","title":"The biggest news from Meta Connect 2024","content":"Meta has a bunch of new hardware and AI news coming out of its Meta Connect event today, including a new Quest 3S VR headset, an expansion of Meta AI features, a new Llama model, and a first look at the new Orion augmented reality glasses. CEO Mark Zuckerberg took the stage on Wednesday with a new style and demonstrated new features including live translation between English and Spanish. Here’s everything announced at Meta Connect: Orion AR glasses Meta has revealed its Orion augmented reality glasses, and they look almost like a trendy pair of frames you could pick up without all the tech inside. Orion uses Micro LED projectors inside the frame and beams images in front of your eyes via waveguides in the lenses. Orion has the same kinds of generative AI capabilities as the current Ray-Ban smart glasses — but adds a visual element to make it more helpful, like adding labels on top of ingredients you’re looking at on a table. The glasses pair with a wireless compute puck and a “neural wristband” you wear on your arm that responds to gestures like punches. Orion isn’t quite ready as a product, though, so it isn’t going on sale. The cheaper Quest 3S VR headset Meta’s latest VR headset is the Quest 3S, and it’s launching at the Quest 2’s original starting price of $299.99. The new headset has many of the same features as the more expensive Quest 3, including the same Snapdragon XR2 Gen 2 chip, and uses the same Touch Plus controllers. The lower price point is possible thanks to the Quest 3S’s lack of depth sensor, lower resolution screens, narrower field of view, and less compact package than the 3. With the 3S now in place, Meta is simplifying its VR lineup by discontinuing the Quest 2 and Quest Pro. Meta’s Ray-Ban glasses add new features Meta’s very cool Ray-Ban smart glasses are getting new software updates that add a Reminders feature that can help you keep track of your day like remembering your grocery list. There are also improvements to Meta AI responsiveness, and the company is working on real-time speech translation. On the hardware front, Meta is launching new styles like a transparent frame and a new range of transition lenses. AI images in your Facebook and Instagram feeds — featuring you Soon, your Facebook and Instagram feeds will include Meta AI-generated content “based on your interests or current trends” in a section called “Imagined for you.” It can incorporate your face into made-up scenarios like “an enchanted realm,” and you can do things like imagine yourself as a video game character or astronaut. The feature is a test for now. Meta AI in celebrity voices Meta is continuing its venture to make celeb chatbots after shutting down its alternate persona feature on Instagram, which had weird options like Tom Brady playing as “Bru.” Now, Meta is connecting new celebrities with AI versions of themselves on Meta AI, including Awkwafina, John Cena, Keegan-Michael Key, Kristen Bell, and Dame Judi Dench. You can get access to these Meta AI voices on Instagram, WhatsApp, and Facebook in the US. New Llama model with image processing Meta has a new Llama model with one key update: the ability to process visuals. This is something competitors have had for the past year, so it’s an important step forward for Meta’s model. The new version also comes in smaller packages designed for mobile."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/2024/9/25/24253712/meta-rayban-ai-features-reminders-translation-transparent-style","category":"Meta","date":"Sep 25","author":"Kylie Robison","title":"Meta’s Ray-Bans will now ‘remember’ things for you","content":"Meta’s Ray-Ban smart glasses are already one of the best cracks at AI hardware to date. Now, Meta is pushing out a series of software updates, along with a new limited-edition translucent Ray-Ban style, that bring the smart glasses closer to actually feeling smart. The company announced several updates to the Ray-Ban Meta glasses at its Connect conference on Wednesday, introducing new features like “Reminders,” which has the glasses take a photo of what you’re looking at and remind you about it later through a notification on your phone. You’ll also be able to scan QR codes and call phone numbers you’re looking at directly from the glasses. The Meta Ray-Bans can already translate a handful of languages from still images. Now, the company says it’s working on a real-time language translation feature that will play back what you’re hearing in real time via speakers in the glasses. When this feature is made available in the coming months, it will be able to translate between English, French, Italian, and Spanish. I got to test out the new updates firsthand, and the improvements in AI were clear. Back in July, when I first tried the Ray-Ban Metas, the AI often struggled — it couldn’t even set a simple timer or reliably identify objects. This time, however, the glasses accurately recognized everything I asked and were also far more responsive, handling follow-up questions with ease. However, there’s still no timer, which feels like a big miss to me but Meta spokesperson Elana Widmann says it’s “coming soon.” A Meta product lead in the demo booth said that the goal of this update was to make conversations with the glasses feel more natural, and to his point, I did find this version of the AI more useful. The company also plans to add the ability for the Ray-Bans’ AI to support real-time video processing so it can immediately understand what’s around you — that feature isn’t coming until later this year, though. PreviousNext1/3 Photo by Vjeran Pavic / The VergePreviousNext1/3 Photo by Vjeran Pavic / The Verge Alongside the software updates, Meta is also releasing a new range of transition lenses with Ray-Ban’s parent company, EssilorLuxottica. There are also new limited-edition clear frames that reveal all the tech inside, reminiscent of an old-school Game Boy Color. At the demo booth, I got to see Li-Chen Miller, head of wearables at Meta, sporting a pair. The clear frames are on sale today, but limited to 7,500 units. AI-powered devices have had a particularly rough year. Humane’s AI-powered Pin struggled with poor sales, and the Rabbit R1 faced terrible reviews. While Meta hasn’t released sales figures, CEO Mark Zuckerberg told investors the smart glasses have exceeded expectations, leading EssilorLuxottica to ramp up production to meet high demand. According to estimates from IDC, Meta has shipped more than 700,000 pairs, with orders more than doubling from the first to the second quarter of this year. It seems like Zuckerberg has high hopes for smart glasses. “I think Meta AI is becoming a more and more prominent feature of the glasses, and there’s more stuff that you can do,” he said in an interview with The Verge’s Alex Heath this week. “It’s not like we’re going to throw away our phones, but I think what’s going to happen is that, slowly, we’re just going to start doing more things with our glasses and leaving our phones in our pockets more.” Whether these glasses will truly change how we interact with AI remains to be seen, but Meta’s latest updates show a clear path toward making AI assistants on your face a practical reality. Update, 4:11PM ET: Added that the clear frames are limited to 7,500 units."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/2024/9/25/24253420/meta-ai-celebrity-voices-awkwafina-john-cena-judi-dench-connect","category":"Tech","date":"Sep 25","author":"Wes Davis","title":"Meta’s AI can now talk to you in the voices of Awkwafina, John Cena, and Judi Dench","content":"Meta is adding conversational voices by celebrities to its AI chatbot in Instagram, WhatsApp, and Facebook. The company announced at its Connect event today that you can now talk to Meta AI and hear it respond in one of several voices, including celebrity soundalikes such as Awkwafina, John Cena, Keegan-Michael Key, Kristen Bell, and the only one I truly care about: Dame Judi Dench. These celebrity voices will only be available to US users of Meta’s apps to start. And if you prefer a voice that is a little more mundane, you can also pick from non-celeb voices with names like “Aspen,” “Atlas,” or “Clover.” Google and OpenAI also now offer similar conversational experiences that ostensibly aren’t based on celebrity voices. If your first thought here was “Gee, that sounds a lot like that Scarlett Johansson thing that OpenAI did,” you’re right — expect that, rather than debuting an AI voice that just coincidentally sounds like one of the world’s highest-paid actors, Meta is explicitly announcing celebrity partnerships, which likely involve payment or some other deal. Meta hasn’t shared those details, but the company has paid each celebrity “millions of dollars” for their voices, according to The Wall Street Journal. And in negotiations, some of the people reportedly wanted to limit what their voices could say and to make sure they weren’t liable if Meta AI was used. Meanwhile, Meta recently shut down its chatbots that emulate celebrities like Tom Brady on Instagram, while Amazon used celebrity voices like Samuel L. Jackson or Shaq for Alexa before shutting them down last year, too. Meta’s AI updates aren’t just about voice conversations. Its chatbot will also now “answer questions about your photos” when you upload images. Send a picture of a cake, ask how to make it, and it’ll grab you a recipe that hopefully does just that. And if you want something “added, changed, or removed” from an image, Meta says you can describe anything from “changing your outfit to replacing the background with a rainbow,” and it’ll carry out that request. Before: a black shirt. Photo by Vjeran Pavic / The VergeAfter: a red shirt! And... a messed-up version of the shirt’s text and logo. Photo by Vjeran Pavic / The Verge In the above examples from our hands-on testing, Meta AI changed the color of a T-shirt. It’s not clear what, if any, guardrails there are on this experience. With Google AI, we have plenty of examples of what this sort of tech can do, for better or worse, and we’ll see sometime in the next year how Apple manages it, too."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/2024/9/25/24253329/meta-ai-generated-images-facebook-instagram-feeds","category":"Meta","date":"Sep 25","author":"Emma Roth","title":"Meta’s going to put AI-generated images in your Facebook and Instagram feeds","content":"If you think avoiding AI-generated images is difficult as it is, Facebook and Instagram are now going to put them directly into your feeds. At the Meta Connect event on Wednesday, the company announced that it’s testing a new feature that creates AI-generated content for you “based on your interests or current trends” — including some that incorporate your face. When you come across an “Imagined for You” image in your feed, you’ll see options to share the image or generate a new picture in real time. One example (embedded below) shows several AI-generated images of “an enchanted realm, where magic fills the air.” But others could contain your face... which I’d imagine will be a bit creepy to stumble upon as you scroll. The examples at the very top of this article include captions that say you can “imagine yourself” as a video game character or an astronaut exploring space. Both images appear to use a person’s photos to create an AI-generated version of them in made-up scenarios. In a statement to The Verge, Meta spokesperson Amanda Felix says the platform will only generate AI images of your face if you “onboarded to Meta’s Imagine yourself feature, which includes adding photos to that feature” and accepting its terms. You’ll be able to remove AI images from your feed as well. Last week, 404 Media found that using Snapchat’s AI selfie feature gives the company permission to use your face in ads seen only by you (unless you disable the option). It looks like Facebook and Instagram will similarly only show the AI-generated content to you, while sharing remains optional. During an interview with The Verge’s Alex Heath, Meta CEO Mark Zuckerberg said that adding AI images to your feeds is the next “logical jump” for Facebook and Instagram. “I think there’s been this trend over time where the feeds started off as primarily and exclusively content for people you followed, your friends,” Zuckerberg said. “And you just add on to that, a layer of, ‘Okay, and we’re also going to show you content that’s generated by an AI system that might be something that you’re interested in’ ... how big it gets is kind of dependent on the execution and how good it is.” Meta says the feature is just a test for now, so it’s unclear how widely or quickly it’s going to roll out. But tossing AI-generated images into our feeds sounds like it will probably take things even further from the space for keeping up with friends that these platforms used to be. Update, September 25th: Added a statement from Meta."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/2024/9/25/24253774/meta-ai-vision-model-llama-3-2-announced","category":"Meta","date":"Sep 25","author":"Kylie Robison","title":"Meta releases its first open AI model that can process images","content":"Just two months after releasing its last big AI model, Meta is back with a major update: its first open-source model capable of processing both images and text. The new model, Llama 3.2, could allow developers to create more advanced AI applications, like augmented reality apps that provide real-time understanding of video, visual search engines that sort images based on content, or document analysis that summarizes long chunks of text for you. Meta says it’s going to be easy for developers to get the new model up and running. Developers will have to do little except add this “new multimodality and be able to show Llama images and have it communicate,” Ahmad Al-Dahle, vice president of generative AI at Meta, told The Verge. Other AI developers, including OpenAI and Google, already launched multimodal models last year, so Meta is playing catch-up here. The addition of vision support will also play a key role as Meta continues to build out AI capabilities on hardware like its Ray-Ban Meta glasses. Llama 3.2 includes two vision models (with 11 billion parameters and 90 billion parameters) and two lightweight text-only models (with 1 billion parameters and 3 billion parameters). The smaller models are designed to work on Qualcomm, MediaTek, and other Arm hardware, with Meta clearly hoping to see them put to use on mobile. There’s still a place for the (slightly) older Llama 3.1, though: that model, released in July, included a version with 405 billion parameters, which will theoretically be more capable when it comes to generating text. Alex Heath contributed reporting."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/2024/9/24/24253452/microsoft-correction-ai-safety-tool-fix-errors","category":"Microsoft","date":"Sep 24","author":"Emma Roth","title":"Microsoft claims its AI safety tool not only finds errors but also fixes them","content":"Microsoft is launching a new feature called “correction” that builds on the company’s efforts to combat AI inaccuracies. Customers using Microsoft Azure to power their AI systems can now use the capability to automatically detect and rewrite incorrect content in AI outputs. The correction feature is available in preview as part of the Azure AI Studio — a suite of safety tools designed to detect vulnerabilities, find “hallucinations,” and block malicious prompts. Once enabled, the correction system will scan and identify inaccuracies in AI output by comparing it with a customer’s source material. From there, it will highlight the mistake, provide information about why it’s incorrect, and rewrite the content in question — all “before the user is able to see” the inaccuracy. While this seems like a helpful way to address the nonsense often espoused by AI models, it might not be a fully reliable solution. Vertex AI, Google’s cloud platform for companies developing AI systems, has a feature that “grounds” AI models by checking outputs against Google Search, a company’s own data, and (soon) third-party datasets. In a statement to TechCrunch, a Microsoft spokesperson said the “correction” system uses “small language models and large language models to align outputs with grounding documents,” which means it isn’t immune to making errors, either. “It is important to note that groundedness detection does not solve for ‘accuracy,’ but helps to align generative AI outputs with grounding documents,” Microsoft told TechCrunch."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/2024/9/24/24253272/james-cameron-stability-ai","category":"Tech","date":"Sep 24","author":"Charles Pulliam-Moore","title":"James Cameron is joining Stability AI’s board of directors","content":"Though director James Cameron recently expressed concern about the potential of artificial intelligence to leave people without a sense of purpose, the longtime early adopter has now joined Stability AI’s board of directors. Today, Stability AI — the company behind Stable Diffusion — announced that Cameron has signed a deal to become the newest member of its executive leadership team. In a statement, CEO Prem Akkaraju described Cameron’s coming on as a “monumental statement” for the AI industry as a whole and described the director as someone who “lives in the future and waits for the rest of us to catch up.” Cameron cited his long history of experimenting with emerging technologies as part of what inspired him to join Stability AI’s ranks and pointed to “the intersection of generative AI and CGI image creation” as the next big breakthrough set to revolutionize the filmmaking industry. It’s not currently clear how Cameron’s arrival at Stability AI might influence the company, but the news comes at a time when studios have begun to show interest in working with AI firms more closely. Last week, Lionsgate announced a partnership with Runway to develop a generative AI model trained on its catalog of films and TV series, and earlier this year, Sony Pictures Entertainment CEO Tony Vinciquerra said that the company intends to deploy the technology as a massive cost-saving measure. Though AI companies and the studios working with them have been insistent that their collaborations will ultimately be a boon to the industry, the shift represented by these kinds of deals was part of what drove Hollywood’s actors and writers to go on strike last year. Cameron himself has recently caught heat for using AI to remaster some of his classic films like The Abyss, True Lies, and Aliens. Stability is also currently dealing with multiple lawsuits for allegedly training its models on copyrighted material, which might complicate the company’s plans depending on how things shake out. But for all of the uncertainty that’s defining the AI space, Cameron sounds especially keen on throwing himself into it while it’s buzzing."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/2024/9/24/24253252/figma-ai-make-designs-first-draft-app-ui-generator","category":"Design","date":"Sep 24","author":"Jay Peters","title":"Figma’s AI-powered app generator is back after it was pulled for copying Apple","content":"Figma announced Make Designs in June as a way for designers to use generative AI to help start app designs, but early users quickly showed how it could mock up a weather app that looked remarkably close to Apple’s iPhone weather app. Although Figma insisted that the feature wasn’t trained on customer data — in fact, Figma said it didn’t train the off-the-shelf generative AI models used — the company removed the feature to give it more testing. Now it’s coming back, called “First Draft,” and like Figma’s other AI features, it’s available currently in a limited beta. Figma is also adding features to First Draft as part of the relaunch: We’re also introducing some key updates, like letting you choose from one of four libraries depending on your needs—whether it’s a wireframing library to help you sketch out less opinionated, lo-fi primitives, or higher-fidelity libraries to provide more visual expressions or patterns to explore. This offers a looser, more exploratory counterpoint to the utility of our Visual Search feature, which allows you to search your Figma files via prompt or image to find existing files or components. In its blog post about the relaunch, the company also spelled out how First Draft works: First Draft doesn’t train on customer content. It uses off-the-shelf AI models (like OpenAI’s GPT-4 and Amazon Titan) with three key elements: model, context, and prompt. The context includes proprietary mobile and desktop design systems with numerous components and assembly examples. Users input their design goals as the prompt. The AI then selects, arranges, and customizes design system components based on these inputs, creating a starting point for designs. Figma says it’s relaunched the feature after “extensive analysis, iteration, and testing.” Figma’s other AI features also include the ability to auto-generate text for your designs."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/2024/9/24/24253107/warner-bros-discovery-caption-ai-google-cloud-closed-captioning","category":"Google","date":"Sep 24","author":"Umar Shakir","title":"Max is getting Google AI-generated closed captions","content":"Warner Bros. Discovery (WBD) is partnering with Google Cloud to generate closed captioning for content on the studio’s Max streaming service in the US. WBD will use a new “caption AI” solution that runs on Google Cloud’s Vertex AI platform to generate speech-to-text for some shows, starting with “unscripted programming.” Captions generated for the Max shows will have human oversight for quality assurance. According to Google, the workflow will reduce caption file creation time by up to 80 percent and cut costs by up to 50 percent compared to manual methods. WBD will continuously refine and train caption AI to make fewer mistakes. Notably, most content on Max already has closed captioning tracks, and WBD hasn’t said how extensively it will use caption AI. Automatic speech-to-text services for video captioning have been available for years, but with a large studio like WBD now testing the latest AI for the job, we might see a future where the tech is the norm for this task."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/2024/9/24/24252979/google-maps-updated-street-view-images","category":"Google","date":"Sep 24","author":"Emma Roth","title":"Google is updating Street View images across dozens of countries","content":"Google is updating Street View imagery in nearly 80 countries, such as Australia, Brazil, Denmark, Japan, the Philippines, Rwanda, Serbia, South Africa, and more. It’s also bringing Street View to a handful of countries where it’s never been available, Bosnia and Herzegovina, Namibia, Liechtenstein, and Paraguay. Google says its more portable Street View camera, which launched in 2022, will help offer images of “even more places in the future.” Meanwhile, Google Maps and Google Earth are getting sharper satellite imagery as well thanks to the company’s cloud-removal AI tool that takes out clouds, shadows, haze, and mist. This should result in “brighter, more vibrant” images, according to Google. Additionally, you’ll soon be able to view historical imagery on Google Earth’s web and mobile apps — a feature that was previously only available through the Google Earth Pro desktop app. This should make it easier to compare satellite and aerial images of a location throughout the years. As someone who loves checking out new places in Street View, I’m excited to explore the streets and landscapes of Bosnia and Namibia whenever Google decides to roll out this update. The Verge reached out to Google to see why it removed its blog post but didn’t immediately hear back. Update, September 24th: Added Google’s official announcement."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/2024/9/24/24252723/openai-o1-ai-god-tiktok-google-adtech-vergecast","category":"Vergecast","date":"Sep 24","author":"David Pierce","title":"They think they’re building God","content":"OpenAI’s new model, called o1, appears to think and ponder as you use it. But is it thinking? Or pondering? And what does it mean if it is? Would that make it worth the risks, which appear to be both greater and more plausible than ever? How do you balance the risks of destroying humanity with the possibility of improving it? This is the thing about talking about artificial intelligence: it has this nasty penchant of getting all existential on you. On this episode of The Vergecast, we get all existential about AI. The Verge’s Kylie Robison joins the show to discuss why OpenAI built o1, why it’s launching the way it is, what to make of the folks who are worried about what they’re seeing from the model, and how we should think about this moment in AI as companies pivot toward trying to build “agents” that can do more and more on our behalf. (We recorded this just before Sam Altman published his recent blog post on The Intelligence Age, but it all feels pretty timely.) After that, The Verge’s Gaby Del Valle and Adi Robertson come on to catch up on some legal and political stuff. We talk about the latest on the don’t-call-it-a-ban TikTok ban, which is now being tested in court — but maybe not in exactly the way you think. We talk about President Trump’s crypto announcement... if you can even call it an announcement. And we check in on the Google adtech trial, which could be another shock to the entire tech ecosystem. Finally, we answer a question on the Vergecast Hotline (call 866-VERGE11, or email vergecast@theverge.com!) about an issue everybody has: what do you do with all the stuff that accumulates on your devices? If you want to know more about everything we discuss in this episode, here are some links to get you started, beginning with OpenAI: OpenAI releases new o1 reasoning modelOpenAI’s new model is better at reasoning and, occasionally, deceivingOpenAI is launching an ‘independent’ safety board that can stop its model releasesOpenAI rates its new model “medium” risk.From Sam Altman: The Intelligence Age And on TikTok / Google / Trump: TikTok ban: all the news on attempts to ban the video platformTikTok oral arguments will weigh security risks against free speechTikTok faces a skeptical panel of judges in its existential fight against the US government Donald Trump is hawking tokens for a crypto project he still hasn’t explainedUS v. Google redux: all the news from the ad tech trialHow Google got away with charging publishers more than anyone else And a few tools for cleaning up your devices: SwipewipeDisk Inventory XWinDirStat"},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/2024/9/24/24252971/spotify-ai-playlist-builder-beta-available-us-canada","category":"Spotify","date":"Sep 24","author":"Jess Weatherbed","title":"Spotify’s AI playlist builder is now available in the US","content":"Spotify is expanding an AI feature that creates customized playlists from your text descriptions to additional English-speaking regions. Starting today, people in the US, Canada, Ireland, and New Zealand with Premium Spotify subscriptions (which start at $5.99 per month for students or $11.99 for individuals) can access AI Playlist in beta, following its initial launch in the UK and Australia earlier this year. Subscribers can locate the feature within the mobile app by tapping the “+” button at the top right of their Spotify library. Selecting “AI Playlist” from the drop-down menu will then open a chat box to describe the playlist you want, such as “spooky songs to play during Halloween” or “soothing folk music for a chill bath time.” The feature will also provide suggested prompts. The AI Playlist beta isn’t currently available on the Spotify desktop or web apps. The generated playlists contain 30 songs and can be adjusted with additional prompts to better match the desired vibe, such as asking for more upbeat music. Descriptions that specify things like genres, decades, moods, or artists will see better results, but in my own testing, I found it was impressively capable of matching songs to even niche descriptions like “make me feel like a vampire hunter from Blade (1998).” Spotify says it’s still “actively learning and iterating with each exchange” while the feature is in beta and may introduce changes to refine AI Playlist in the future."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/2024/9/24/24252934/google-workspace-gemini-ai-security-assistant","category":"Google","date":"Sep 24","author":"Jess Weatherbed","title":"Google’s Gemini AI might soon appear in your corporate Workspace","content":"Google is making Gemini AI a core part of its Workspace productivity suite, which could see the chatbot adopted by millions more users. In its latest blog posts, the search giant announced that the standalone Gemini app is being included as standard on Workspace Business, Enterprise, and Frontline plans starting sometime in Q4, replacing the need to purchase a separate Gemini add-on. Google says that Gemini is subject to the same enterprise terms as other core Workspace services like Gmail and Docs, and won’t use an organization’s data, generated responses, or user prompts to train or otherwise improve its Gemini AI model. Workspace Administrators will also “soon” be able to manage if Gemini stores generated responses and user prompts, and limit how long these will be stored for. Gemini for Workspace now also carries SOC 1/2/3 and ISO 27701 industry security and privacy standards certifications, giving organizations some peace of mind when implementing the chatbot for corporate use. And to bolster security against malware, phishing, and other online threats, Google is introducing a new “Security Advisor” tool that “delivers insights directly to an IT administrator’s inbox.” Security Advisor includes a range of safe browsing and data protection features for Chrome, Gmail, and Google Drive, and will be rolled out to paying Workspace customers “over the next few weeks.”"},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/2024/9/23/24252664/openai-x-account-crypto-scam-newsroom","category":"OpenAI","date":"Sep 23","author":"Jay Peters","title":"An official OpenAI X account was taken over to peddle a crypto scam","content":"An official OpenAI account on X was taken over to peddle a cryptocurrency scam on Monday evening. On Monday at 6:24PM ET, the @OpenAINewsroom account, which shares news from OpenAI and has nearly 54,000 followers, made a now-deleted post advertising an “$OPENAI” token. “We’re very happy to announce $OPEANAI: the token bridging the gap between Al and blockchain technology,” the post said. “All OpenAI users are eligible to claim a piece of $OPENAI’s initial supply. Holding $OPENAI will grant access to all of our future beta programs.” The post also included a link to a spoofed version of OpenAI’s website at a URL that wasn’t openai.com. When I visited the site, there was a section for claiming the $OPENAI cryptocurrency. When I clicked the button, it asked me to connect a wallet, which I didn’t do. OpenAI and X didn’t immediately reply to a request for comment. As of this writing, the OpenAI Newsroom account hasn’t posted anything to explain what might have happened. The account launched at the beginning of this month."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/24247369/the-browser-company-ceo-josh-miller-arc-google-chrome-ai-search-web-decoder-interview","category":"Decoder","date":"Sep 23","author":"Nilay Patel","title":"Arc creator Josh Miller on why you need a better browser than Chrome","content":"Today, I’m talking with Josh Miller, cofounder and CEO of The Browser Company, a relatively new software maker that develops the Arc browser. David Pierce, my Vergecast cohost and Verge editor-at-large, is a big fan of Arc and has written about it quite a bit for us. You can read his review here. Basically, Arc is a ground-up rethinking of the web browser. Most modern browsers started as simple document viewers and grew to support running complex apps. Arc’s main conceit is that it’s designed to make running and using all those apps as simple as possible. You’ll hear Josh describe it as an operating system several times, which is a pretty big claim to make, and he and I got into what that actually means for a web browser. There are some AI tools built into the Arc browser, but the company also has a mobile app called Arc Search that does AI summaries of webpages. That puts it in competition with OpenAI’s forthcoming SearchGPT and Google’s Gemini-powered AI Overviews in its search results. At the same time, it also puts Arc right in the middle of one of the fiercest debates in tech and media today: whether AI companies and products are boosting content from the open web and then turning around and selling it to consumers — all without paying the people who produced that work anything at all. We’ve been talking about these topics pretty much nonstop for the last year here on Decoder. So I was really excited to have Josh on the show to explore why he built Arc, what he hopes it will accomplish, and what might happen to browsers, search engines, and the web itself as these trends evolve. I wanted to know how Josh is thinking about competing with Chrome on the desktop and Apple’s Safari on mobile, and especially how he plans to monetize Arc. Chrome and Safari are a lot of things, but mostly, they’re developed by some of the richest companies in the world and given away for free. Josh says the plan is to keep Arc free but monetize a mix of customization, automation, and productivity tools that will make users’ lives so much easier that they, or the company they work for, pay a subscription fee. It’s a bold idea to bring competition back to the browser market, and early reception to Arc has been positive. But you’ll hear Josh and I go over some of the major challenges they’ve faced so far, like having to teach people all-new sets of metaphors and design language around what browsers should be doing and why you would even want to use a web browser to run apps the way Arc is suggesting. (Or why you’d want to use a new browser at all.) I also asked Josh about his take on the controversies swirling around generative AI and whether the web as an information distribution system is going to survive a major plundering of all its pages. Josh is pretty candid about what he does and doesn’t know about how this might play out, and he’s also more open to changing his mind than arguably any tech CEO I’ve talked to about this subject. It’s a good back-and-forth, and I’m curious for your feedback on it. One quick note before we start: after we recorded this conversation, The Browser Company disclosed a pretty severe security vulnerability in Arc that could have let attackers insert code into other users’ browser sessions. It was patched a day after a researcher made the company aware of it in late August, and the company says no users were affected. But it’s a significant issue, and in a statement released last week, the company said it marks “the first serious security incident in Arc’s lifetime.” We tried to get Josh back on the show to talk about it, but he was unavailable the day the flaw was disclosed to the public. The company does say it’s making a lot of big security improvements. And in a separate statement on X, cofounder Hursh Agrawal said, “A heartfelt thanks for all the concern (and even outrage) you’ve all expressed about this incident, and for holding us to a high standard.” He went on to say that he and the company will “be using this opportunity to grow as a company, as an engineering organization, and personally as a founder.” Okay, The Browser Company CEO Josh Miller. Here we go. This transcript has been lightly edited for length and clarity. Josh Miller, you’re the cofounder and the CEO of The Browser Company. Welcome to Decoder. Thank you for having me. I’m excited. We’re in the studio together in New York. It’s a rare occurrence on Decoder. Thank you so much for being in person with me. Oh, it’s so fun. I was hoping you’d go easier on me, but I was told that it is absolutely not true. Oh, no. When you’re in person, it’s even harsher because I can smile at you while— While plotting. Yeah. Well, it’s great to be here. I think I’ve probably listened to almost every Vergecast and Decoder interview episode for the past few years. Oh, you’re ready. The studio is as nice as you said it was. It is. We have a fancy new upgraded studio. I’m happy you’re here. There’s a lot to talk about. The Browser Company runs a browser called Arc. You run a mobile app called Arc Search, which is browser-adjacent, I would say. It lets you browse the web in a new and different way. You’re obviously competing with Google. Google appears to be in a moment of change — regulatory change, self-imposed change — and then there’s AI. And obviously, Arc Search is built in as an AI product. But let’s start at the very beginning. What is The Browser Company? What is Arc? The Browser Company is making a web browser called Arc, which the simplest way to explain it is The Verge called it “The Chrome replacement that they’ve been waiting for.” So, don’t take it from me. David Pierce called it that. I just want to be clear. [Laughs] David Pierce said that. No, Arc is the best browser for laptop people. If you’re someone whose livelihood is clicking and clacking on your keyboard every day, we make the best browser for you [that] keeps you focused, organized, and increasingly, we want to do your busy work for you. Let me ask you a question about that. So, if you are somebody who makes money on a laptop, you’re presumably using a lot of applications, not looking at a lot of content. I would love to be a person who made a lot of money on my laptop just by looking at a lot of other people’s content, but I suspect what you’re getting at is this is a productivity application. So, the origin of The Browser Company is I was a political appointee in the Obama White House and after the 2016 election, I was personally devastated by the result. I felt like technology and the technology industry had an impact on the things I didn’t like, and I was very motivated to try to do something about it. My takeaway was, if you are not an operating system, if you’re not a platform by which your applications and content sits on top of, you don’t really have leverage to change for the better or worse the way that society uses technology. So, we decided not to start a company and do something else. And then it was in 2019 — my wife works in the art world for artist James Turrell in Flagstaff, Arizona — that I noticed that she never left Chrome. She was on this high-powered MacBook Air and never left the confines of Chrome. So, the original observation of The Browser Company was actually our operating systems, in 2019 then and definitely in 2024 today, are actually our web browsers for laptop people. You’re sitting in applications in a browser. Your files are now URLs, too. So, the founding inside of the company was, “Wait a minute: browsers were designed for the information highway. They were designed when the web was a publishing platform. That has changed. Browsers have not. Why is that?” Spoiler alert: money. “Can we make your quality of life on the internet better?” So, you are correct in that relative to the origins of the web and the origins of browsers, people are not spending as much time with content as they are with opening their browser and doing their work. So, it’s an application environment. That’s what I’m getting at, and one of the things we talk about in Decoder all the time is how the application model moved from Windows to the web to mobile, and then maybe back to the web. There’s something happening there that seems big, and it’s kind of landed on the web. Most people who want to deploy a desktop application turn to the web first. I don’t think a lot of people are deploying Win32 first anymore. Do you see your browser as having a meaningful impact on that class of developers? Because if you’re an operating system, you have a lot of power, right? You’re like, “Here’s some APIs. Here’s some capabilities of my operating system that a developer can use.” This is what all the major operating system vendors say to their developers all the time. You’re saying my browser is an operating system and people are deploying applications to the web. Are you in conversation with those applications? Do you offer those developers new capabilities, or is it really just about the end user? It’s a great question, and I actually think this is where Google deserves a lot of credit. I think if there’s one thing Chrome and the Chromium team specifically has done a fantastic job of is building an operating system, or an application platform, that developers love, generally speaking, and they make it more and more powerful. In fact, you had Dylan Field on this podcast; Figma would not exist if it weren’t for Google, Chrome, and Chromium making the web fantastic for application platforms. What we’re focused on is the individual and the person at the other end. So, what we think about is the focus on developers, and the focus on publishers as Google describes them, has left the individual on a Tuesday at 2PM lacking a lot of powerful tools to make them better and faster. So, of course, we have integrations with different third-party application developers. I would love it if we could offer stuff that makes them love Arc more. But in fact, we think that what was missing was looking at my wife using her laptop on Tuesday at 2PM [and realizing], “Wait, that’s what she’s doing? We can do better. Computers can do more than that.” So, that’s the orientation we take to our work. One of the big questions when you’re starting a new browser company is, one, how will you take share from Google and Microsoft in Safari, particularly in iOS? And then two, what engine are you using? Because you’re not going to write a new browser engine that seems like a massive undertaking. You’ve landed on Chromium. It seems like the whole industry is headed towards Chromium. Microsoft famously uses Chromium now. Was that a big decision? Was that a little decision? To be honest, it was an intentional decision, but it was a little decision. And for better and worse, the theme of my answers in this podcast will probably be, “We come back to the individual at 2PM on a Tuesday.” If there’s one thing they want from their rendering engine, if they’re familiar with what a rendering engine is, it’s that their web apps work. So, for example, for whatever reason, we have a lot of teachers and people in education using Arc. A lot of software at school districts are optimized and actually only work in Chrome-based browsers. It was intentional in that we wanted to make sure we could strip out a lot of the kind of tracking and nefarious parts of Chromium that at least don’t align with our values. But once we realized we could do that, we thought, “Hey, almost every website will work on this rendering engine. We want to make your day better at 2PM. Let’s jump to that part,” to the end person-facing part of the software. So, you’ve got Chromium as a rendering engine that’s the same as Chrome. Arc itself is the Chrome around Chromium. This is just the language. So, you built a wrapper around Chrome — that’s a pretty familiar idea. And then the idea is all of those things will make productivity, particularly productivity for knowledge workers, better on the web. But you’ve invented a lot of terminology. There’s a sidebar, there are spaces — there’s just metaphor after metaphor in Arc that are different from Chrome, right? There’s boosts. There’s just a lot of words and concepts in this browser, which are interesting, but a lot of them are, “We have to teach people a new metaphor for using the web or thinking about this browser as an application layer in their computer as opposed to just a web browser.” Where did the genesis of this come from, and how did you go about, honestly, just picking all these names? Yes. To be totally honest, I regret many of those words. I wish we didn’t have so many new concepts. And I think it’s too complicated of a piece of software for many people, and I think we have to make it a lot more simple. But where it came from was, “Wait a minute: if you look at someone’s tab bar and they have 50 tabs open, and they’re really teeny tiny, and there are a lot of duplicates — why? You don’t need five versions of the same Google Doc. How can we solve that problem?” So, most of the new concepts came from the perspective of “what is broken, what is wrong with the way people use their browsers today, and can we invent a way to alleviate that problem?” And what that led to, for better or for worse, was a lot of small features and a lot of small ideas that make your day just a little bit better, save you a couple of clicks, that I think is built a very cult-like following in the software but has made it a bit too unapproachable for the average person in that it is a lot of new ideas. That’s part of what we’re working on now: how do we strip away a lot of the experiments that didn’t work or didn’t work as strongly as we hoped they would to something a lot more focused and a lot more essential? Which, right now, is focused on how we do your busy work for you. Because people I think love our features, like when you’re playing a YouTube video and you click away, we automatically open a picture-in-picture player. Or if we notice you have tabs open that you’ve had open for a long time that you haven’t used, let’s just tuck them away neatly for you. And so we’re going to be focused on trimming down the product even more and really try and enhance the bit that does your busy work for you and has these little moments of delights. That seems like the challenge. You have identified one set of users that already knows they’re using a web browser as a productivity platform, that already knows that all their apps are in a web browser. And then there’s another class of users that is just using Safari because it’s what came on their Mac, and you’ve got to get more of those people in order to grow your user base. How do you balance the two? It feels like you already have the power user problem. The way that we started building this product was through the lens of problem statements and that’s how we ended up with so many different solutions and so many different words. But I think the byproduct of that is four years later, I think we have a much crisper understanding for the average laptop person — who, again, doesn’t know what a rendering engine is and honestly probably isn’t reading The Verge and isn’t an early adopter — what are the most painful, annoying, tedious parts of their day on the internet? Where if we just focused on them they’d say, “Wait, I want that.” I do think as much as there are some things that may be power user-y, there are other ideas in there that you talk to 10 out of 10 people in this demographic, and they go, “Yeah, I have seven windows and 87 tabs, and it’s a mess, and it’s chaotic, and I feel overwhelmed.” And so we’re going to be focused on trying to build an antidote to a few specific problems. I feel like web apps in general require people to understand new metaphors. We often write and talk about how younger people are not as aware of file systems as a concept — they grow up on iPhones and iPads and ChromeOS devices using something like Figma, which requires a bunch of people to accept a bunch of new metaphors. And then you’re trying to change the metaphor around all of those metaphors. Is that going better or worse than you expected? Honestly, it’s going better than I expected, but I think we’re going to hit a plateau. Our ambition really is to change the way people use the internet and improve it. And if we really want to reach out of that early adopter crowd, we have to simplify. But I think one of the really exciting things is the most-used text box on a Mac is the URL bar in Safari. And so what we’ve realized is we kind of spread out and we built all these new surfaces and all these new nouns and all these new spaces, but if we just focus in on a few points that people are familiar with and use a lot like the text box, like the URL bar, there’s a lot that a lot of power we can pack in that. And actually that Verge article shared a lot where, no, people don’t really want to manually organize stuff in file systems anymore. They want to tell the computer what they need, and they want the computer to go get it for them. So, I think you’ll see us pack a lot of the ideas behind some of our power user features in a much more approachable and familiar interface, which is the Command+T text box that you go to all the time to ask for things, now you can ask for a lot more. Well, you started this whole conversation by saying you were distraught that an election had been lost and computers were maybe responsible or not, and the operating system is where the leverage is. How do you turn all of that into the leverage you’re seeking? Is it, “We’re not going to show you some websites.” Is it, “We’re going to make you have a healthier relationship with Instagram?” Are you just going to pop up a warning that’s like “You’re on Instagram?” How do you actually use the leverage of owning something that feels like an operating system? In the same way that your background as a copyright lawyer informs a lot of the work that you do, I want to take a minute just to talk a little bit about my origin story because it relates to the answer to the question. When I was a senior in college, I didn’t know what I wanted to do, and I was a sociology major, and I went to a lecture by a professor named Robert Putnam about his book Bowling Alone. After the lecture I went up to him, I said, “Professor Putnam, what should I do with my life?” He’s like, “I don’t know you, so I have no idea, but if you like my book, there’s an entrepreneur named Scott Heiferman that started a company in New York City called Meetup after he read the book. Maybe you should go work for him.” So, I went to get a job at Meetup, and on my first day of the internship, Scott gets up, and he says, “We’re going to turn away from the banks, and we’re going to turn to each other on Kickstarter, and we’re going to turn away from big box retailers, and we’re going to turn to each other on Etsy.” And he went on and on, and it was deeply inspiring, and it was that part of me that fell in love with tech and the idealism behind it. To me, that shows two things: one, I have always been motivated by people at the other end, and two, Scott was totally wrong. I love him,but I think, honestly, the part of me after the election that said, “I got to fix something, we got to do something, we got to fix democracy with technology” — I’m still an optimist; I still care about people, but I think we now have right-sized what our role should be, which is instead of saying in that moment, “How do we as some tech company with 20 people fix democracy or improve our civic society?” It’s just as worthy and ambitious to say, “My sister-in-law who’s a teacher and spends hours every day copying and pasting between different software to be a teacher, let’s get rid of that busy work for her.” That is just as ambitious, and that’s just as worthy. So, honestly, there’s kind of been this personal transformation from early ‘20s, the internet is going to fix everything to, “Hey, let’s just make our friends and our family and our lives a little better every day.” So, don’t get me wrong, I still have that part of me that is as idealistic and hopeful that the web and the ideas behind the internet can improve these top-level ideas, but we are much more interested in almost like the anthropological approach to “Nilay’s day, how do we make it a little bit better?” and find worth in that. There’s a little bit of tension here. You described Arc as being an operating system. You obviously want, in some end state, for application vendors to be talking to Arc as an operating system and maybe leveraging some of your capabilities. You’re talking about end users making their lives better. But you live on another operating system; the applications inside Arc or whatever other browser are doing whatever they’re going to do. How do you balance that role? It feels like there’s only one stakeholder whose experience you can actually improve or adjust, and Apple might just make it much harder for you because you run on a Mac, or Microsoft is going to put Edge pop-ups all over Windows, or Figma is going to strike a deal with Chrome to use some cutting edge API that you don’t have access to. There’s a lot of dependencies there. How are you balancing all that? This is where I’m just a big believer in the web. As tricky a moment as it is in many ways, I believe the web has won, is winning, and will win. And I think in the web, there are enough parties involved and there are enough incentives where it’s not really about The Browser Company — it’s about betting that the web is an application platform, and the decentralized nature of it will mean that people will still keep building for the web. As long as people are building applications for the web and the center of gravity — especially in this world of AI, love it or hate it, is heading even more to the web — I think there’s enough incentives in the industry, in the ecosystem, to suggest that if we build one user agent for it, there’s really good work we can do there. I want to talk about the web in detail, but I think this brings me to the Decoder questions. This is a big ambition. How big is The Browser Company now? Eighty people. And how is that structured? We have kind of functional teams — design, engineering — but we really like to organize in deeply cross-functional pods. So, we hire people that tend to be mutts, as we like to say in endearing ways. They come from different backgrounds with different skill sets beyond just whatever their title is, and then we put them together in these little pods of five people and give a prompt like, “How can we help make the experience of Shopify sellers, how do we make it easier to use their tools every day?” And we give them six weeks and say, “Go.” And they try a bunch of things, and we see what happens. When you have a prompt like that, do you say, “Okay, you came back, you have an answer. We’re going to go find a bunch of Shopify sellers and try to market Arc to them specifically.” Or is it, “We’re going to abstract the solution to a bunch of other use cases and market the abstract product that you’ve invented”? It depends, but actually, it’s reversed in the order we do it. So, one of our first hires was a woman named Adena [Nadler], and she runs a team now called the membership team. So, what we start with is actually conversations with Shopify sellers, and we watch them use their computers. We ask them about their problems, the things they do every day, and we actually try to abstract solutions for them based on that. Sometimes we focus on individual tools. So, we built this feature called GitHub Live Folders that, if you’re a software engineer and someone needs a code review from you, it’ll just automatically pop up and say, “Hey, Nilay needs you to review his code.” That’s something specific for GitHub. And other times, we’ll take an idea and abstract it to something that can work everywhere. We heard the story from a teacher last week actually, where she said she spends an hour every week taking attendance logs from a Google Sheet that she has and copy and pasting them into a school district-wide CMS of some sort for attendance records — and it takes her an hour. That makes me so mad. We can send reusable rockets to space apparently, but we have teachers spending an hour doing copy-paste, copy-paste, tab switching. So, Nate on our team last week prototyped this mass-paste idea where in one fail swoop you can take a bunch of data from one tab and paste it in a very formatted structured way into another tab. So, there’s an example relative to GitHub where the seed of the idea was this teacher with this very specific piece of software she has to use for her very specific job, but in it is this much larger relatable idea of we can all relate to copy and pasting back and forth between tabs incessantly. So, it’s a little bit of both, but it always starts with a person. It always starts with people and always starts with going out into the world and trying to understand. Sometimes, it’s a family member. Sometimes, it’s a cousin. Sometimes, it’s a stranger. What are they experiencing on the web every day? You’ve got kind of an interesting challenge there because mass-paste seems pretty abstract. “I’ve got two tabs, I’ve got two sources of data. I just need to move them over.” Maybe Chrome will build that feature — maybe they won’t. At least you’re competing with another browser entirely. With something like a GitHub notification, it seems likely that GitHub might build that feature and send you a notification to a mobile app or send you a notification to whatever web-based notification system that the industry will eventually adopt. How do you think about that? That your features might get adopted by the very applications that you’re trying to support? If you talk to these application developers, one of their complaints is actually browser vendors are pretty restrictive about what they can do in the browser. So, one of our popular features is in our Command+T text box, you can type “new Notion document,” and you can hit enter, and it’ll create a new Notion document. Notion loves that. Notion can’t do that in Chrome or Safari because Google’s trying to protect its search ad revenue. So, there are examples of places where we’re actually giving developers more access than they would in other browsers because we’re not optimizing for search ads. And then there are other examples where they’re actually things that you can only do at the browser layer that exist across multiple tabs. So, if you think about the teacher example, the things that the developer of Google Sheets and the obscure public school district CMS application would need to do to have an integration, that’s never going to happen, but at the browser layer, because we sit underneath all of it, we can actually do those things very easily. So, it obviously depends on the feature, but generally speaking, because other browsers are designed to be, essentially, big search boxes for the search ad business model, there hasn’t been as much innovation at the interface layer or the operating system level of a browser such that application developers, I think, are very excited about the access that they will be able to have, and there are things we can do across web applications that would be difficult otherwise. You’re really describing the browser as an application layer. This is the model for apps going forward, and you’re drawing a pretty stark contrast to Google, which is “search for some stuff and we’ll show you some documents.” The web is in a moment of pretty intense tension between these ideas. You mentioned AI — all the AI applications are deployed to the web because they want to skip the app stores in one way or the other. Crypto, for better or worse, was mostly a web phenomenon because they didn’t want to pay app store taxes, either. Do you think the web is headed toward being more of an application system as opposed to a document storage system? I’m curious, what do you think? Well, I have a lot of feelings about the web as a publishing medium, but I think the pressures on the web as a publishing medium are not insurmountable, but unavoidable and certainly changing the economics of the business there. Whereas the pressures of app stores, on mobile phones in particular, are potentially devastating, and that’s why you see so many applications on the web. So, it feels like unless someone actively stops it, documents will move off the web and applications will move off the phone, but I’m not 100 percent sure it’s actually happening. You have a vantage point — I’m curious if you see it. I would say unequivocally, putting aside my own feelings about it, that the web, since we started the company five years ago and the trend lines have continued, is becoming more and more of an application platform. I think that’s undeniable. I think it’s very exciting. I think it poses some problems in the context of publishing. I also think, as you mentioned, there are these words, there are these phrases [like] application platform. My wife, in her job, has things she has to do. I don’t think it is going away that sometimes she needs information, and actually, frequently she needs information. So, I think what has changed is, as you know, the origins of the web, were a publishing platform — they’re actually closer to TikTok or Twitter in many ways than an application platform at the time. What has changed is that the mix has moved toward more applications, but the idea that as part of your job, as part of your personal life, you need to find something out or learn about something, that’s not going away. But I think the trend lines are toward it as an application platform. Do you think that mix is shifting? If I were to start a tech website today, I probably actually wouldn’t start a website. I would almost certainly start a TikTok channel and just show people whatever I was covering. I see that as some amount of platform economics but also a lot of web economics. The desire to put new information on the web first is fading, whereas the desire to deploy applications to the web is rising, and that mix is shifting, and maybe it feels like your entire company is a response to that mix shifting, but I’m wondering if you actually see it day-to-day in how people are using the browser. Yes, absolutely. And in fact, keep in mind, I’m 33, I grew up on the desktop web. That’s where I got lost as a child in my curiosities. And so, in fact, it’s been a process for me to admit to myself that this thing that I loved about the web and I wanted from the web that — if you look out again from a sociology, from a human perspective — we’re not seeing it as much. A thing you said that I also think is true and makes me so mad is, yes, if you are going to start, put a piece of information out, you probably should start a TikTok channel. I don’t like that, but I think that is true. I think one of the interesting things, though, is if you go back to the origins of the web as a publishing platform, what we’ve learned about publishing platforms in retrospect is it missed two big things: distribution and discovery. We now know that the most powerful part of any publishing platform is discovery, and the web publishing platform didn’t have that built-in. Google’s a hack in many ways for that. TikTok’s a hack for that. The second thing it didn’t have baked in is payments. Can you imagine the iOS ecosystem If Apple didn’t have native payments that were easy and seamless? Think about what that’s done for subscriptions and purchasing apps. Yes, there are a lot of challenges with 30 pecent taxes, but it enabled this thriving marketplace. And so if I look at the trajectory of the mix shift on the web toward applications, there are reasons people are rushing toward it. And if I look at the reasons that information or publishing has faded, I think it can really come down to those two missing elements. I wish I knew what you could do about that because, again, the web is a decentralized protocol, but I think you can look at those two factors and explain a lot. I’m curious if you agree or if you have thought about that. Well, I agree on the diagnosis. I’m not sure what the cure is, but I asked you that question because if the browser is the operating system and you control that, well, you could be the Apple that introduces a payments layer to the web. Famously, Marc Andreessen thought the web would be powered by micropayments when he did Netscape, and it just never occurred, and then crypto arrived, and we had to listen to it. Probably not the right idea, but the idea is cyclical. The idea that we’ll have payments on the web in some way is cyclical. And if you are controlling the browser, I’m wondering if that’s something you could introduce to fix the document-side model of it or if you’re staying focused on the application side? I would love nothing more than to get involved with that. Because another thing we think about are the fundamental economics of browsers and the web itself, which is so dependent on ads, and I think, often, these conversations are binary “ads are bad” or “[ads are] good.” That’s not what I’m saying, but I think there’s so much more potential in the ways that browsers and publishers to the web and applications to the web could monetize if payments were built in. I think that’s extremely exciting. It’s a great example of somewhere where it’s sort of a win-win-win. If you make payments easier, the individual’s happy because it’s easier to make payments — you don’t have to pull out your credit card. The merchant’s happy because you grease the wheels — it’s easier to have transactions, and whoever’s connecting the two is making money as well. So, I find payments fascinating. I think it could do so much good for the web. The flip side of believing in the web is we are a minnow. We’re barely a minnow, and so one of the interesting tensions we feel in this conversation — I’m sure we’ll talk about Arc Search — is we’ve got ideas we’re excited, but we’re not at Chrome scale, we’re not at Safari scale. So if we ever have the privilege of getting to a place where our voice can move the ecosystem in some way, I think adding payments natively to the browser in that layer of the stack would do wonders for the ecosystem. And I hope that we or someone else gets there because I think it would be fantastic. How does The Browser Company make money today? We don’t currently charge for anything, but we, as part of this kind of 2.0 product that’s coming out soon, we’re going to be charging individuals and businesses for a plan that does more of your busy work for you than the default plan. But we don’t have anything concrete to announce. So a subscription. A subscription browser is where we’re going. Potentially. When you say plan, that usually means recurring revenue, not “we’re going to sell you a browser one time for $49 in a box.” Yeah. So, the honest answer is we don’t have the specific details yet, but what we are sure of is we want an exchange of value, which is we do your busy work for you, we save you time, we save you clicks, we help you through your day, and either you or your employer pays us. Whether or not that is through a subscription model or a usage-based or some sort of token system is something we’re still figuring out, but we’re really excited about the ambition to say, “Hey, can you truly save that much time for someone that either them or their boss would fork over money for it?” What are the pros and cons of the different choices? A very long conversation, but I think subscription is easier in many ways. It’s more familiar. What I really like about something closer to usage-based pricing is that I really want a direct exchange of value. I want it to feel as much like the more you use it, the more you pay us because the more value we’re delivering to you. There’s some tricky things to think about in terms of you also want people to really develop a habit with your product because they have all this inertia from Chrome and Safari, and you don’t want to push people away from using it more and more. But I’m confident or at least hopeful that we can get around that. We’re always going to have a free plan. We hope to put as much in the free plan as possible, but it’s a tricky one. Other CEOs have gotten in lots of trouble on the show suggesting that they will make something that was previously free into a subscription product. Do you have any hesitation there? There’s nothing in the product today that we are going to charge people for. So we’re really excited about this next evolution. How can we take the idea behind this automatic picture-in-picture player automatically cleaning up and managing your tabs for you? Can we take that to the extreme and do more and more busy work for you, such that that additional time savings, that additional work we take off your plate, that additional tedious, monotonous stuff that you have to do and you no longer have to do, you can imagine some of that being stuff that we charge for. Also, this is a danger of doing this in person because I was not supposed to talk about this, but you loosened me up a little bit, so I’m going to get in trouble for talking about this later. That’s why we bring people to the office. I just want to stick on it a little bit longer. So, you’ve got products today. You’ve got Arc Search and the Arc Browser. Will Arc Search be paid on the phone? That is not currently the plan. And it’s worth noting we really think of Arc Search as the companion app to the desktop product. So, we definitely have a challenge with words and branding as a theme I’m taking from this conversation, but the intention of Arc Search: it is the mobile browser to the desktop browser. Sure. Arc Search is an AI product. I want to talk about that a little bit, but the economics of AI products are pretty simple. Someone does a search in Arc Search. You have to go talk to a cloud provider, do some inference and come back — that costs you money. If you intend to keep it free, how much money can you spend before you have to change your mind? So, our intention is that the paid offering — which, again, we’ll apply on mobile, too, not the Arc Search that you see today, but the additional functionality on top of it — is what will subsidize the free version for folks. So, then the goal is you make useful free versions and people convert to the paid? Yeah. What people do in Arc today doesn’t actually cost us all that much money, and our ambition is to make this free for as many people as possible. As we get into more AI inference-intensive tasks for people that take off more and more busy work, that’s where… I think we want to be a sustainable business that exists for a long time — it’s about time — but also I think the costs get more prohibitive. You’re obviously competing with Google. Google loves to give things away for free. That search ad revenue is a cash machine basically. That search ad revenue is a cash machine for them. How do you think about competing against a competitor that will undercut you on price in the most ruthless way possible, which is giving it away for free? In some sense, it’s terrifying. We have, on paper, absolutely no advantage. They have more money. They have more people. They have more all of the things. I think over time, as we’ve built more and more features and gotten this question more and more, I think what we’re realizing is if we’re truly going to build the successor to the browser, what comes after it — I’m going to avoid branding it since I’ve branded too many things — that is really a holistic rethinking of our interface to the internet. I think that, and the care and the detail that goes into that, is not as simple as popping on an AI sidebar chat onto Chrome. There are examples of other browser vendors that have clearly taken ideas from us and done their own versions of it, and it hasn’t gotten in the way of our growth or success so far. So, I think if you look at it from a top-down perspective, how are we going to beat Google or Apple or Microsoft? It’s tricky to give you an answer that is convincing. I think the lived experience so far is that we keep our heads down, we optimize for building something that people love and truly helps them in their day-to-day, and we think about this from a blank-page perspective of not “what did browsers do yesterday?” but “how can we build a cohesive day on the internet that saves you time and does your busy work for you?” I think it’ll be difficult for the other vendors to just bolt that onto their existing products. Now at some scale, might they do what happened to Slack with Teams? Of course, we’re in a capitalistic society — that will happen. I think there is the room for us to run if we are focused and we are fast and we really do what we’re best at, but time will tell. There’s the Chrome of it. There’s also the Safari of it. Apple really wants people to use its integrated applications, particularly on mobile. Do you find that trying to ship a new browser on an iPhone is a lost cause? Do you think that that is a market you can actually get into, or is that just closed off to you? I think the fascinating thing about Safari in general is that Safari — and we have this on good sources — is the most used application in the Apple ecosystem. More time is spent in Safari than any other application. But if you go look at the size of the team and the things they’re working on, there’s a mismatch there because Apple doesn’t want the center of gravity to move toward the web on desktop. On mobile, it’s more difficult because the browser plays a different role. On desktop, it is increasingly the application environment, and on mobile, it’s a place where you go to quickly look something up, get some information really quickly, quickly read an article. And there’s some things that Apple does or doesn’t do that makes it more difficult. They don’t let you bring keychain passwords over. It’s more difficult to check out. And so there are some structural challenges created by Apple on iPhones that make it more difficult. But I’d say the bigger thing is the role of the browser on your phone is that it’s almost a different product than what it is on desktop, and that’s the thing that we think about the most. But I think as we’ve seen with Arc Search, there is a desire if you build something truly new for people to change, and it’s just a question of what is the ceiling there on mobile versus desktop? This brings me to the other Decoder question. You have a lot of challenges. You’ve got huge browser competitor that gives away its product for free. You’ve got operating systems that will and will not let you do certain things. You’ve got the changing nature of the browser itself. You’ve got pricing to figure out. How do you make decisions? What’s your framework? I knew you’re going to ask this question because you always ask this question. I wish I had a framework. We think of our work as optimizing for feelings and instinct. I don’t know if this is a response to the technology industry that I was brought up in where you’re supposed to be neutral and unopinionated and have frameworks, but our approach is: What are we trying to express here? What feels right to us? What do we want to do for ourselves and our parents and our siblings and people that we care deeply about? So, generally, of course, we have a data science team. We look at the data, we reason in all the ways that we should, but I think at the end of the day, [you have a] big decision to make, I’d say it’s more of a personal expression and a personal reflection of our hopes, wishes, and desires for our work than it is anything else. One of the comparisons you made was to Google. You said it’s not just as easy as bolting on an AI chat box to the side of the browser. I could be pretty reductive, and I could say, “You’ve just described Google shipping its org chart. There’s a Chrome product manager. There’s a Gemini product manager. Just be next to each other. Don’t integrate the product.” That sounds like you’re betting on Google not figuring it out, to some extent. The Google product culture will ship and kill things in the way the Google product culture does, and it will never make the turn toward integrating the AI products. You can feel however you want about that bet. I’m sure the people at Google feel some way about that bet, but is that what you’re thinking, that they’re big and slow and you can actually just be more nimble? It’s worth noting I think the people at Google are very smart, and I’m not just saying that as what I’m supposed to say. I truly believe that. We hired Darin Fisher, who started Chrome and ran Chrome for 16 years. He worked at The Browser Company. It’s more about the incentive structure. I like to think a lot about incentives. It’s one of the things I wish I thought about more earlier in my career. There’s a story that Darin told me that really stuck with me, which is Chrome had this idea that, when you go to the “new tab” page (one of the most popular surfaces in any piece of software you use), if they show you an icon for the webpage that you go to a lot, you might be able to notice it much more quickly — “Oh, it’s the Twitter icon. I’ll click on Twitter — versus just a screenshot of the webpage. And they ship that, and overnight, Google search ad revenue dropped by 5 percent, and they weren’t sure why. It was this big freak out. Now, that resolved in the way that it did, but that is the sort of thing that you have to contend with if you— Because people were no longer doing navigational searches for Twitter? Yeah, because they don’t want you to go to Twitter; they want you to go to search. Now, the Chrome team doesn’t — the Chrome team wants you to get to Twitter as fast as you can, but at a company like Google, in this moment, in the public markets, in this moment of AI even more, there are these incentives with the search ad model and the way that Chrome and the search ecosystem works so far that are just a huge… it’s inertia. So, it’s not just shipping the org chart; having worked at Facebook, there are real challenges there. But I think on top of that, there is the incentive structure of how the company makes money and has for a long time. And then there’s also the risk. If you think about it, if we start with a blank page, if you give me the most generous reading of everything I said, it may not work, and if it does, we don’t only need it to work for a 100 million people. If we do something radically different and we find a hundred million people that love what we do, that is a raging success. For Google that’s an utter failure, and that’s if it goes right. So, I think there’s also the risk aversion to the scale they need to hit the number of people it needs to work for to be worthy, putting aside all of the product risk that comes with doing something truly new. Google’s in a state of what I would call regulatory scrutiny. They just lost the antitrust case against the United States Department of Justice that said there was an illegal monopoly in search and in certain part of its ad business. The ad tech part of its business is going to an antitrust trial very shortly here. As part of the search trial, we found out that Google’s paying Apple $20 billion a year to make Google the default search engine. This stuff feels like it’s coming apart. It’s a big moment. There are opportunities here. Which of those opportunities is most right for The Browser Company, and how are you going to attack them? Candidly, the way I think about it is there’s more pressure on them not to do anticompetitive practices or things that can be perceived that way. So, I think there are a lot of subtle things that these players do that make it harder for an upstart like us to compete. So, I would say it’s less a specific decision, though these are all big in their own right, and more generally that there are eyes on these companies not to do things that are monopolistic or perceived to be monopolistic, and that culture and climate, I think, is advantageous to people like us. Do you think the Department of Justice should break up Google? Yes. How would you break up Google? Come on, Nilay. You’re a lawyer. That is way above… I’m just asking. [Laughs] That is way above… Well, there’s an obvious answer here, which is split out Chrome, which has been floated. Do you think you would have a better chance against the independent Chrome company? I’m not a lawyer. I have no idea. But what I— I’m asking you competitively. If Chrome did not have the pressure of Google search — you can put in the Twitter icon or whatever application icon without hurting the search revenue — do you think you’d have a better shot at competing with an independent Chrome? Honestly, hard to say. I’m not trying to be evasive. I honestly don’t know. Do you think that the deals Google has been making to make its search engine the default in different places, if they came to you and said, “We’ll pay you $20 billion a year to set Google search as the default in Arc,” would you take the money? $20 billion was an unfair number to pick. $5. We’re just going to keep going by fives. $10. Would you say yes to $10? No. $15? Maybe this comes back. Maybe I should— $20, Josh. Maybe I should have a framework for optimizing for this stuff, but at the end of the day, I just want my day on the internet. I’ll go to $100. Just at the end of the day, Nilay, I want my quality of life on the internet to be much, much better. Do you take money to set a default in search on Arc? No. Is there a default? The default currently is Google. You got to make a phone call, man. The money’s on the table. That may or may not change soon. The default, or the money? The default. Okay. No, we are not going to... If we take money for the default search engine, then ultimately our customers, our search engines and advertisers, and that is conflicting to why we started the company, what we set out to do. However, I do think one of the things that is very exciting about this moment in AI, alongside all the challenging things, is AI has this ability to route us to different places more intelligently and take us more directly to places we want to go that are not always Google, and oftentimes, it’s never Google. So, we’re going to replace the default search engine, but not with another search engine that’s... One example I like to think of is I just moved to a new place in Brooklyn, and I was trying to decide if we should buy a HomePod. Valerie and I love to dance around the house and we didn’t have a speaker. I want to type in “The Verge HomePod review.” If I hit enter, that takes me to Google. In our 2.0 product, if you hit enter, that’ll just take me to The Verge’s HomePod review. So, there are things that we can do in this moment that weren’t possible before that I think make Google vulnerable both in search and browsers. That means this question of default search engine is no longer just going to be Google vs. Bing and who’s going to pay you. It can be, “Let’s take you to the exact right place based on what you’re looking for.” So, you’re building a search-like functionality. Again, it may sound tired, but the way we think about this is what are the things you need to do every day? There are these new technologies that make it more possible to blur the lines between what is a browser, a search engine, into something that more holistically end-to-end helps someone do something. And yes, as part of that, when you type in the most popular text box on your computer, we can now take you and route you to lots of different places that oftentimes are much more direct and on the nose for what you want and don’t just funnel you into the Google ecosystem because that’s how it’s always worked, because that’s what their business model is. One of the things we’ve seen a lot with AI in general, and you’re certainly talking about it now, is the idea that that text box, Command+T, is actually the user interface of your computer. You’re going to just tell the computer what you want, and the computer is going to go off and do it. And if you have the entire web behind you, you can do a lot of things, especially if you can take actions on web applications. Yes. Are you trying to build that kind of automation layer where you say, “Hey, just go to my calendar and bring all the dates out and put them over here?” Yes. Again, you’re getting me in a mode where I’m sharing more than I should. But we have this internal prototype I tried last week where my son had his first day of preschool today. They sent us a PDF, which I opened in my browser with all the different dates for holidays and whatnot, and I could say, in one gesture, add all of these to my calendar, and it would do that. And so what we’re doing is building the layer underneath all the applications to understand what is going on in your life, what are you looking at right now, what have you been working on previously, and the connective tissue between all of the applications and tabs that you use and rely on, and on top of that, we can take a lot of busy work like that off of your plate much more easily. And sometimes, that’ll come through Command+T, and I’ll ask it. And other times, if I’m on Apple looking at a HomePod, we might say, “Hey, you really like The Verge. You read The Verge a lot. Here’s the HomePod review.” So, I’m using the text box as, yes, the most popular interface, but I think it should feel like your entire experience on the web is more personalized and more proactive to you, not just when you explicitly ask for something. This idea that a robot’s going to go click around the web for you is very popular. We’ve seen a number of startups say they can do it. I don’t think they’re actually doing it, but they say they’re going to take AI and do it. Then, there’s just a set of follow-on problems to this. The browser has to see everything in all of the websites. It has to see my data, it has to read that data, it has to interpret it presumably using an AI system in a cloud somewhere. It has to click on things for me without getting anything wrong, and then it has to not hallucinate. That’s a lot of steps. How do you protect people’s data and actually hit the level of, essentially, 100 percent reliability that people are going to demand from products like this? The first thing is we really think about right-sizing AI. There’s a lot of discourse about AI right now, and it tends to be of the martini-sipping version where we’re going to replace teachers and doctors and there’s going to be the superintelligence being, and that’s, in our opinion, not the right way to think about this stuff. I think the equivalent there as it relates to clicking is you’re going to tell the computer what you want to do and it’s just going to do a bajillion things for you with 100 percent accuracy. Today, that’s not possible. That’s not how it’s going to work. But what is possible is in these small ways, again, saying “add these to my calendar,” we can do that, and we can do that with close to 100 percent reliability. Our approach is — as much as possible, which is increasingly very possible, especially on high-end MacBooks — doing that on-device. Data does not leave your device — it’s all done locally and, when it can’t be done locally, making sure that the person says, “Hey, I’m okay with that tradeoff of sending the contents of this PDF to an LLM provider in order to add it to my calendar” and let them make that decision. But I think the large point here is what we are not saying is the robots are going to do all of your work for you. That is not our belief, but what it can do is it can save people from a lot of the mundanity that relates to futzing around with boxes on the internet all day. Do you think that that is a separate set of use cases from what Arc Search is doing? Absolutely. In fact, Arc Search was really a first prototype. There’s so many things that I wish we’d done differently and we’ve now since learned, but really, that was the first experiment of this larger idea of us playing with this new Play-Doh, which is, “Okay, we can click on things for you. We can read things for you. Wow. We definitely can’t... the writing’s really bad. Oh, but interestingly, we can transform one type of data format into another type of data format.” Just feeling out the edges of what it can do today. As part of that, one small thing that you do is you want to find out a quick answer to... I got a skirt steak the other day, and the guy at the butcher was like, “You should make chimichurri sauce.” I don’t know how to make chimichurri sauce, and sometimes I want to know that. A lot more frequently, there’s something for my job or my livelihood where I have to go click a bunch of buttons in the same order every single time. I think we’re much more excited about doing that sort of busy work for you because, candidly, that’s what people complain about the most when we interview them about their jobs. “I want to make chimichurri sauce” is a great example because what Arc Search will do is it’ll go read a bunch of webpages, it’ll summarize them, it’ll show you the answer with some links. That is a very controversial move across the web right now. When I say there’s a lot of pressure on the web as a document or consumption medium, that’s the pressure. In particular, a bunch of AI companies are scraping the hell out of the web, remixing the web, and the people who actually made the information are getting nothing for it. Arc Search is right in the middle of that. That is the thing you are doing. Do you think that that is a sustainable thing to do? No. And I think this is a really complicated one, so I want to try to share both sides, and let’s take it head on. That’s part of the reason I’m here. From the perspective of an individual, I want the chimichurri recipe, I show up to the website, I got 17 trackers tracking me all of a sudden. I get a newsletter pop-up saying, “Do you want to subscribe to our newsletter?” I wade through five paragraphs about the author’s grandmother and the history of her chimichurri recipe, and all the way at the bottom is the recipe. That doesn’t feel good to the individual. It feels like we can do better, and it feels like for pretty much everyone that uses the web, a much better thing would be, “I want to know the ingredients and the recipe steps. Get it to me as quickly as possible.” And on the other side, it breaks the model of the web historically. Now I think we are not going around any paywalls. We are not training our own models. A lot of the stuff that I think is more problematic is not anything that we do, but I do think it’s fair to say that those trackers, as much as I feel like they’re unfair to me as an individual, are part of how that recipe site makes money. The fact that they show ads — which, if we are reading the sites on your behalf, you’re not seeing — it breaks that model in some way. So, this is a moment where I’m an optimist. I think it’s a very exciting moment for publishers and media companies because for the first time… so much of this is dictated by Google and the way that Chrome and Google Search has worked for so long. So, I think something’s got to change. I think publishers have to get paid. I wish I had an easy answer for you, but I definitely don’t think it’s sustainable. Even if I also think for the individual, we got to do better as well. In February, my friend Casey Newton wrote about Arc Search. He said he felt a rare emotion: “a kind of revulsion at the app’s mere existence and what it portends” because it’s taking the value from the people who write the recipe website. I could do a full hour on why there’s a story at the top of every recipe website. That is the way that the money is made. It’s the incentives of the system, absolutely. You can’t sell the recipes for a variety of reasons, so you’ve got to sell something else. You can sell ad inventory around the recipes. Do you understand why Casey felt the revulsion? I know he talked to you for that piece. Yes. And he talked to you, and the quote is, “Miller had not put much thought into the second order implications of a world where search queries no longer result in outbound clicks.” That was February. It’s September. Have you thought about it since? Yes. Actually as recently as last week, I had a conversation with David [Pierce] at The Verge. I thought we were doing a good job of citations. He read me the riot act on the fact that we weren’t, and in the app today, we have citations even more prominently than I thought was the most prominent app out there that shows what we read, put them at the top, you can click them easily. We’re also having a bunch of conversations with media companies right now. At the end of the day, I think media companies need to get paid and publishers need to get paid. And I think the truth is, as you know, the scale of that will not mean that it works for everybody, but we are trying our best behind the scenes and out front to be better here. Candidly, one of the challenges we have is we don’t have the scale of other players in the space. So, if we show up at a media company’s website and say, “Hey, let’s figure something out here. Let’s figure out how we can pay you,” we don’t always get the same receptivity as what I assume other companies do. But I’m curious what you think about the OpenAI model for this, because we’re kind of seeing this all from afar. But I think what I come back to is I’ve been on the board of Patreon for five years, and I think you know better than anyone I don’t think the old model was working for anyone, even before all this AI stuff. I think you make a great point that AI accelerates it and it hurts it, but I think the old model wasn’t working. What I do think this new technology provides is a way for all of us to rethink everything from the products themselves — the media products, the software products — all the way to the business models. And I’m curious, for Vox, how you’ve thought about that and how you think about it in the context of OpenAI and these publishers that are doing that. Happily, my role in the newsroom is to spend money. I don’t make any money. It’s a real problem for this whole company. We’ve had Nick Thompson talk about his deal from The Atlantic on the show. His view is we need to get this money, and OpenAI is offering us a bunch of stuff in exchange for this money, including tokens and credits to use their systems to build new products. What I see, and maybe it’ll work out, but what I see is we are absolutely hastening the demise of the web as a publishing platform because we’re making it easier and easier and easier to extract value without any payment or compensation going in the other direction. And eventually, all those people are just going to say, “Well, at least there’s a creator fund on TikTok. At least there’s YouTube payments. At least there’s other platforms with some built-in way to compensate me for my work.” Whereas on the web, everyone just takes everything away. Big publishers left and right are saying, “Well, at least Apple News exists. We’ll just take that money.” I don’t know if that’s good or bad. But the theme of this conversation is the web is increasingly an application platform. We can tailor the browser to it being an application platform. And over here, the part where people browse the web for information, maybe we can extract value from that and that will go away. Or maybe it’ll just be a handful of preferred providers that OpenAI pays or Perplexity pays or you pay. But that open web, the part where there’s just information on the web for people to click around and look at, that seems like there’s nothing here that indicates it can make a resurgence. The other thing, too, is we talk about the web or publishing like it’s one big category. But for example, if you go to a local restaurant in my new neighborhood and they have a reservation booking tool, I’m sure they’re totally fine with the idea that an AI system might come around and make a reservation more easily for people. So, that one’s easy. I really believe what you or Ezra Klein said on that podcast about this idea of a flight to quality. I’ve never listened or engaged with The Verge more, and I predict that across mediums — TikTok, podcasts — I think that will only continue. And my hunch is that things like “browse for me” or OpenAI or Perplexity, that’s not going to replace the HomePod review that I rely on before making a purchase. I’m very bullish on that. I’m curious if you’re not, but I am very bullish on that. There is this middle tier of content and content providers that we might call quick facts or more commodity type content, where, candidly as you know, most of those or some large percentage of those are content farms, or they’re contractors that are just churning stuff out or copying stuff or AI-generated. I think it’s that middle layer, that middle layer of, “I want to know what Sauvignon Blanc tastes like because I don’t know anything about wine, but I’m at the wine store.” That, to me, is the tricky one. I think The Verge is good and going to be better. I truly, truly believe that. So, I think, in many ways, Casey’s revulsion comment, obviously that hurts and it hits, especially after speaking with him. I think it is fair in many ways, but I think it really hits on one percentage of the content. I’m optimistic for what will happen to the media at that end of the spectrum, but maybe that’s ignorance. But again, I’m curious from The Verge, my assumption is this Decoder podcast, I would bet that the ad slots are sold out for the rest of the year. I think so. That’s great. But I look at the platforms, and I have the extraordinary privilege of getting to say that I’m a precious journalist and I have no idea what’s happening with the ads and I won’t read them and we still get to sit in a fancy studio because I have a whole company, and the economics of social platforms are not great for that. You have individual creators who cannot support a giant company, who are in bed with the companies they cover. I’m not even naming names — just broadly, they do the brand deals, they read the ads, they mix the commerce and the content in a way that journalists do not do or should not do. And I say, “Well, the web supported the other model for a minute, and now maybe the flight to quality is a bunch of paywalls.” And what we’re going to be left with is a bunch of free content on platforms that is corrupted in some way by the commercialization of the work because the rates aren’t high enough. And somewhere in there is, “Well, we’re just going to let it happen because the web is an application platform and not a document platform, and we never figured out how to actually sustainably distribute this information in a way that works for everyone.” It feels like there’s a lot of opportunity to make the web a better application platform, but it feels like if you turn that all the way, you do end up with a bunch of weird ads on TikTok and a bunch of paywalls on the web. Again, maybe I’m just too much of an optimist, but I think that it’s going to take creativity and dreaming on both sides. I think from a media standpoint, tell me if you think this is wrong, I think a lot of media organizations made the mistake, maybe a decade ago, of trusting the platforms and, in many ways, outsourcing their product development. I don’t think media companies are going to make that mistake again. And I think there are many, like The Verge, that are innovating on what their product is, and they’re innovating on what their product is in a moment where there’s actually leverage to go after these... I can’t overstate, not to you, but to your audience, how stuck the web has been. And all of these things have been for decades because Google controlled it all. For the first time in decades, there is this technology, this Play-Doh, that gives a window to mess that up at a moment where you all— And you think that technology is AI, to be clear. Yes. In a moment where you all have been burned as media companies by outsourcing your product to Facebook saying, “Hey, trust us. Just give us your content. We’ll pay you. It’ll be great.” You’re not going to make that mistake again. You have Play-Doh to play with. You are innovating on product. And I think on our side, I knew, coming on this podcast, you were going to ask these questions, and I knew I wasn’t going to have a perfect answer, but I think this is important for the same reason I think it’s important to pick up David’s call, hear him kindly yell at me, and make changes based on it. And it’s why we show up at media companies offices saying, “Hey, let’s collaborate on something here. Let’s figure out a way where we pay you.” It’s going to take experimentation. It’s going to take collaboration on both sides. And I think that collaboration bit is the hardest bit because there are bits of what Casey said that I found deeply unfair, and there are bits of it that I found fair, but I know where he’s coming from because it’s the same part of me that was burned as a 20-year-old by these promises of “tech’s going to change everything.” We have the moment in history, which we should not take for granted. We have the Play-Doh, we have the lessons from the past, and now we just got to dream a bit and come together in some way. And maybe this is the part of me that makes decisions through feelings, and this is naive, but I truly, truly think something good is going to come out of this, but I think we’re going to mess some things up. Everyone’s going to mess some things up, and we got to be open about it and talk about it. And I think there is this generation of entrepreneurs both in the media space and in the product space or the technology space that has seen, again, the models that came before it and what went wrong there and is encouraged to come on a podcast like this, even if it’s not always going to be effortless. That is a good and optimistic place to end it, so I’m going to ask one more question. Okay, great. The idea that the web will come into balance and the web will endure, I want to believe. I am a web person at heart. I continue to run a website in 2024. That is just a personal decision that I’ve made. What is the chance that the web actually turns all the way into an application platform, that that dominates the next generation of the web? Oh, I think very low. Yeah. I think very low. And I will need media training from you after this. As someone that is full of ideas and prototypes and we have an experimental culture, there’s nothing I want to do more than blurt out all of these ideas for what might turn it back. I think I need to learn my lesson of the folks that came before me and say, “I don’t know the answer yet.” It is hard to imagine looking at the state of things today as we’ve spoken about, but I think there is some innovation on the product side, both from the media side and the technology side, that can turn those tides. Because I think, again, from the Patreon perspective, everyone is burned. Everyone is overwhelmed. They are burnt out. It is just not sustainable. And I think out of that will come a generative creativity that can bring it back. And I think the truth of these other historical platforms is they have these taxes, and they have these anticompetitive behaviors, they have these things that I think will work against them, and the web has a lot going for it. So, if it’s okay, can I ask you one question? Sure. A birdie told me that of your Vergecast hosts, David Pierce uses Arc, Alex Cranz uses Arc. Nilay Patel does not use Arc. Why don’t you use Arc, and what can we do better? I started using Arc in preparation for this episode. I just got to use it more. I think, unlike my Vergecast cohosts, I am reticent to actually depend on software. I think there’s a danger in being dependent on software or a workflow, and maybe that’s because I’ve had a lot of software in my life go away. So, I’m a very manual brute force kind of person. And the idea that I’ll give up some part of my workflow or my process to a tool has always scared me, but I’ll keep trying. Which browser do you use? Obviously, I use Chrome and Safari, and now I’m using Arc. Oh, you can’t use Chrome. We’re having the conversation about the future of the web, and you’re still on Chrome? Come on. We are a Google Docs company. We are a Riverside company. Okay. I’ll do my best. Well, more than that. I hope out of this, I hope there is some sort of collaboration we can do. Jim Bankoff, if you’re listening, let’s do something. It’s going to be great. I promise you that’s the other side of the house. I’ll make the introduction for you. Okay, awesome. Thanks for having me, Nilay. Thanks for coming on, Josh. This was great."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/24249388/notebooklm-google-steven-johnson-vergecast","category":"Vergecast","date":"Sep 22","author":"David Pierce","title":"The chatbot becomes the teacher","content":"Steven Johnson is a very meta author. He writes frequently about science and technology, and likes to immerse himself in the things he’s covering, even using them to change the way he writes books. A couple of years ago, a few months before ChatGPT launched and the AI boom took over the tech world, Johnson got a magazine assignment that sent him really, really deep down the AI rabbit hole. And he never came back up. Now, in addition to writing books, Johnson is also working at Google. He’s part of the team building a product called NotebookLM — “Notebook,” as the team calls it. It’s a note-taking and research tool: you upload documents and import web links, and Notebook’s Gemini-powered AI helps you organize things, extract information, and understand a subject better. “They reached out,” Johnson says when I ask how he got involved with Google, “and said, ‘hey, you’ve been dreaming of this ideal software tool that helps you organize your thoughts and helps you write and helps you formulate connections and brainstorm. We think we can do it now.” Johnson signed up, and has been at Google since the summer of 2022. The product itself first launched in 2023 as Project Tailwind, and has since been rebranded and expanded in big ways. Just last week, the team launched Audio Overviews, which generates a podcast — with two chatty hosts, plenty of back and forth, and a truly remarkable penchant for the phrases “deep dive” and “buckle up” — based on the information you provide. It’s fascinating, it’s complicated, and it’s getting better really fast. On this episode of The Vergecast, Johnson joins to discuss his fascination with AI, his time at Google, and the present and future of NotebookLM. We talk about the complicated issues raised by a tool like this, and whether it’s okay to let an AI do your research and homework. We also talk about how to make sure a tool like NotebookLM is both accurate and easily fact-checked, why context windows are more important to the future of AI than most people realize, and how often AI podcast hosts should say “like” in conversation. And we talk about Johnson’s own process as a writer and creator, and how AI is changing the way he works. If you want to know more on everything we discuss in this episode, here are some links to get you started: NotebookLMSteven Johnson’s website / newsletterFrom Steven Johnson: Listening To The AlgorithmGoogle teases Project Tailwind — a prototype AI notebook that learns from your documentsGoogle’s AI-powered note-taking app is the messy beginning of something greatGoogle is using AI to make fake podcasts from your notes"},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/2024/9/21/24250867/jony-ive-confirms-collaboration-openai-hardware","category":"Tech","date":"Sep 21","author":"Alex Cranz","title":"Jony Ive confirms he’s working on a new device with OpenAI","content":"Jony Ive has confirmed that he’s working with OpenAI CEO Sam Altman on an AI hardware project. The confirmation came today as part of a profile of Ive in The New York Times, nearly a year after the possibility of a collaboration between Altman and the longtime Apple designer was first reported on. There aren’t a lot of details on the project. Ive reportedly met Altman through Brian Chesky, the CEO of Airbnb, and the venture is being funded by Ive and the Emerson Collective, Laurene Powell Jobs’ company. The Times reports it could raise $1 billion in funding by the end of the year but makes no mention of Masayoshi Son, the SoftBank CEO rumored last year to have invested $1 billion in the project. The project only has 10 employees currently, but they include Tang Tan and Evans Hankey, two key people who worked with Ive on the iPhone. LoveFrom, Ive’s company, is leading the device’s design, according to the report. The team is reportedly now working out of a 32,000-square-foot office building in San Francisco, part of a $90 million strip of real estate that Ive has bought up on a single city block. As for the device itself? The Times says that Ive and Altman discussed “how generative AI made it possible to create a new computing device because the technology could do more for users than traditional software” due to its ability to handle complicated requests. Last year, it was rumored to be inspired by touchscreen technology and the original iPhone. But it sounds like few specifics are nailed down. LoveFrom cofounder Marc Newson told the Times that the AI product — and when it’ll come to market — is still being figured out."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/2024/9/21/24250020/ray-ban-meta-smart-glasses-ai-hardware-meta-connect","category":"Meta","date":"Sep 21","author":"Victoria Song","title":"Meta has a major opportunity to win the AI hardware race","content":"AI wearables have had a cruddy year. Just a few short months ago, the tech world was convinced AI hardware could be the next big thing. It was a heady vision, bolstered by futuristic demos and sleek hardware. At the center of the buzz were the Humane AI Pin and the Rabbit R1. Both promised a grandiose future. Neither delivered the goods. It’s an old story in the gadget world. Smart glasses and augmented reality headsets went through a similar hype cycle a decade ago. Google Glass infamously promised a future where reality was overlaid with helpful information. In the years since, Magic Leap, Focals By North, Microsoft’s HoloLens, Apple’s Vision Pro, and most recently, the new Snapchat Spectacles have tried to keep the vision alive but to no real commercial success. So, all things considered, it’s a bit ironic that the best shot at a workable AI wearable is a pair of smart glasses — specifically, the Ray-Ban Meta smart glasses. The funny thing about the Meta smart glasses is nobody expected them to be as successful as they are. Partly because the first iteration, the Ray-Ban Stories, categorically flopped. Partly because they weren’t smart glasses offering up new ideas. Bose had already made stylish audio sunglasses and then shuttered the whole operation. Snap Spectacles already tried recording short videos for social, and that clearly wasn’t good enough, either. On paper, there was no compelling reason why the Ray-Ban Meta smart glasses ought to resonate with people. And yet, they have succeeded where other AI wearables and smart glasses haven’t. Notably, beyond even Meta’s own expectations. A lot of that boils down to Meta finally nailing style and execution. The Meta glasses come in a ton of different styles and colors compared to the Stories. You’re almost guaranteed to find something that looks snazzy on you. In this respect, Meta was savvy enough to understand that the average person doesn’t want to look like they just walked out of a sci-fi film. They want to look cool by today’s standards. At $299, they’re expensive but are affordable compared to a $3,500 Vision Pro or a $699 Humane pin. Audio quality is good. Call quality is surprisingly excellent thanks to a well-positioned mic in the nose bridge. Unlike the Stories or Snap’s earlier Spectacles, video and photo quality is good enough to post to Instagram without feeling embarrassed — especially in the era of content creators, where POV-style Instagram Reels and TikToks do numbers. This is a device that can easily slot into people’s lives now. There’s no future software update to wait for. It’s not a solution looking for a problem to solve. And this, more than anything else, is exactly why the Ray-Bans have a shot at successfully figuring out AI. That’s because AI is already on it — it’s just a feature, not the whole schtick. You can use it to identify objects you come across or tell you more about a landmark. You can ask Meta AI to write dubious captions for your Instagram post or translate a menu. You can video call a friend, and they’ll be able to see what you see. All of these use cases make sense for the device and how you’d use it. In practice, these features are a bit wonky and inelegant. Meta AI has yet to write me a good Instagram caption and often it can’t hear me well in loud environments. But unlike the Rabbit R1, it works. Unlike Humane, it doesn’t overheat, and there’s no latency because it uses your phone for processing. Crucially, unlike either of these devices, if the AI shits the bed, it can still do other things very well. This is good enough. For now. Going forward, the pressure is on. Meta’s gambit is if people can get on board with simpler smart glasses, they’ll be more comfortable with face computers when AI — and eventually AR — is ready for prime time. They’ve proved the first part of the equation. But if the latter is going to come true, the AI can’t be okay or serviceable. It has to be genuinely good. It has to make the jump from “Oh, this is kind of convenient when it works” to “I wear smart glasses all day because my life is so much easier with them than without.” Right now, a lot of the Meta glasses’ AI features are neat but essentially party tricks. It’s a tall order, but of everyone out there right now, Meta seems to be the best positioned to succeed. Style and wearability aren’t a problem. It just inked a deal with EssilorLuxxotica to extend its smart glasses partnership beyond 2030. Now that it has a general blueprint for the hardware, iterative improvements like better battery and lighter fits are achievable. All that’s left to see is whether Meta can make good on the rest of it. It’ll get the chance to prove it can next week at its Meta Connect event. It’s a prime time. Humane’s daily returns are outpacing sales. Critics accuse Rabbit of being little more than a scam. Experts aren’t convinced Apple’s big AI-inspired “supercycle” with the iPhone 16 will even happen. A win here wouldn’t just solidify Meta’s lead — it’d help keep the dream of AI hardware alive."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/2","link":"https://www.theverge.com/2024/9/20/24249949/intel-qualcomm-rumor-takeover-acquisition-arm-x86","category":"Intel","date":"Sep 20","author":"Richard Lawler","title":"Qualcomm wants to buy Intel","content":"On Friday afternoon, The Wall Street Journal reported Intel had been approached by fellow chip giant Qualcomm about a possible takeover. While any deal is described as “far from certain,” according to the paper’s unnamed sources, it would represent a tremendous fall for a company that had been the most valuable chip company in the world, based largely on its x86 processor technology that for years had triumphed over Qualcomm’s Arm chips outside of the phone space. The New York Times corroborated the report on Friday evening, adding that “Qualcomm has not yet made an official offer for Intel.” If a deal were made — and survived regulatory scrutiny — it would be a massive coup for Qualcomm, which reentered the desktop processor market this year as a part of Microsoft’s AI PC strategy after years of dominance in mobile processors. Intel, meanwhile, is arguably in its weakest position in years — while many of its businesses are still profitable, the company announced substantial cuts, shifts in strategy, and a 15-plus percent downsizing of its workforce this August after reporting a $1.6 billion loss. At the time, Intel CEO Pat Gelsinger said the company would stop all nonessential work and has since announced it will spin off its chipmaking business, a part of the company that it had long touted as a strength over rival AMD and the many fabless chipmakers that rely on entities like Taiwan’s TSMC to produce all of their actual silicon. Intel, too, recently had to partially rely on TSMC to produce its most cutting-edge chips as it continues to rebuild its own manufacturing efforts (the costs of which are responsible for most of Intel’s recent losses). And its own 18A manufacturing process reportedly ran into some recent trouble. While Intel’s chief rival, AMD, also had hard times over the years and had to claw its way back, gamers helped AMD every step of the way. Aside from the Nintendo Switch, whose processors are made by Nvidia, every major game console for the last decade has featured an AMD chip — and Intel reportedly lost out on a chance to change that with the future PlayStation 6. Intel also recently lost some faith with PC gamers after two generations of its flagship chips were found vulnerable to strange crashes, though Intel has since agreed to extend the warranties by multiple years and issued updates that could prevent damage. Many of Intel’s woes are about silicon leadership, not just manufacturing or profits — the company isn’t a big player in AI server chips yet as Nvidia dominates, nor even necessarily a notable small one like AMD. Even its attempts to produce its own GPUs for gamers and creators have yet to impress. And while Qualcomm, AMD, and Apple are all still smaller players in laptops, Intel has now twice overhauled how it makes flagship laptop chips to combat the growing threat of their seeming battery life and integrated graphics advantages. We’re waiting to see if its new Lunar Lake chips succeed in October and beyond. Update, September 20th: Added corroboration by the NYT."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/20/24250067/microsoft-windows-11-copilot-key-customization-apps","category":"Microsoft","date":"Sep 20","author":"Tom Warren","title":"Microsoft’s Copilot key will be able to launch apps on Windows 11 soon","content":"Microsoft is planning to allow Windows 11 users to customize the Copilot key that has started shipping on new laptops and keyboards. The Copilot key is configured as default to launch Microsoft’s Copilot app on Windows 11, but the company is now testing the ability to use it to launch other apps instead. A new beta build of Windows 11 includes the customization changes, available for testers today. “You can choose to have the Copilot key launch an app that is MSIX packaged and signed, thus indicating the app meets security and privacy requirements to keep customers safe,” explains the Windows Insider team in a blog post. “The key will continue to launch Copilot on devices that have the Copilot app installed until a customer selects a different experience.” The Copilot key is the first big change to Windows keyboards in 30 years and part of a push by Microsoft to encourage Windows users to try its AI assistant. New Copilot Plus PCs also started shipping with the key earlier this year, with Microsoft actually making the Copilot experience less useful on these new devices by turning Copilot into a web app in the latest 24H2 update to Windows 11. This web app version of Copilot no longer integrates into the Windows 11 settings, so you can’t use the AI assistant to control whether you have dark mode enabled or a variety of other settings. It’s still not clear how Microsoft intends to evolve the Copilot experience in Windows, nor whether the company will turn its Copilot key into something that could be used more like the Windows key to launch shortcuts. Given the customization for the Copilot key is available to beta testers of Windows 11 today, I would expect we’ll see this available for all Windows 11 users in the coming months."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/20/24249770/microsoft-three-mile-island-nuclear-power-plant-deal-ai-data-centers","category":"Microsoft","date":"Sep 20","author":"Tom Warren","title":"Microsoft wants Three Mile Island to fuel its AI power needs","content":"Microsoft just signed a deal to revive the shuttered Three Mile Island nuclear power plant. If approved by regulators, the software maker would have exclusive rights to 100 percent of the output for its AI data center needs. Constellation, the owner of the Three Mile Island plant, announced a power purchase agreement with Microsoft earlier today, which should see the site coming back online in 2028, assuming regulators approve it. The reactor that Microsoft plans to source its energy from was retired in 2019 for economic reasons and is located next to a unit that was shut down in 1979 after the worst US nuclear accident in history. The plant that Constellation plans to reopen can generate 837 megawatts of energy, enough to power more than 800,000 homes — demonstrating the huge amount of power needed for data centers and Microsoft’s AI ambitions. Microsoft has agreed to purchase power from the plant — which will be renamed to the Crane Clean Energy Center to honor the late Chris Crane, former CEO of Exelon — for 20 years in a first-of-its-kind deal for the software giant. Microsoft’s own greenhouse gas emissions are growing with its focus on AI, putting its ambitious climate goals at risk. Bloomberg reports that this nuclear plant would help Microsoft’s plans to run its data centers on clean energy by 2025 and power data center expansions in Chicago, Virginia, Pennsylvania, and Ohio. “This agreement is a major milestone in Microsoft’s efforts to help decarbonize the grid in support of our commitment to become carbon negative,” says Bobby Hollis, vice president of energy at Microsoft. “Microsoft continues to collaborate with energy providers to develop carbon-free energy sources to help meet the grids’ capacity and reliability needs.” Microsoft has been betting on next-generation nuclear reactors to power its data center and AI plans recently, looking for someone who could roll out a plan for small modular reactors (SMR) last year. Microsoft cofounder Bill Gates is also a “big believer that nuclear energy can help us solve the climate problem.” Constellation will invest $1.6 billion to revive the plant, and the company will need approval from the Nuclear Regulatory Commission to bring the site back online, alongside permits from state and local agencies. Constellation is also pursuing a license renewal to extend plant operations until at least 2054."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/19/24249206/apple-intelligence-ios-18-1-public-beta","category":"Apple","date":"Sep 19","author":"Jay Peters","title":"Apple Intelligence is now available in public betas","content":"Apple has just released public betas of iOS 18.1, iPadOS 18.1, and macOS Sequoia 15.1, and they include upcoming Apple Intelligence features like text rewriting tools, the glowy new Siri design, a “Clean Up” tool to remove objects from your photos, and more. To be able to access the betas, you’ll need to register on Apple’s beta software program site. Once you’ve done that, you should be able to see the beta update available in settings for you to download and install. Note that a only few iPhones can access the Apple Intelligence features: last year’s iPhone 15 Pro phones as well as the nearly-here iPhone 16 and iPhone 16 Pro. iPads and Macs with M1 chips or newer can try Apple Intelligence as well. Previously, these Apple Intelligence features were only available as part of developer betas, and my colleague Allison Johnson wrote about her experience testing the tools on iOS in July. But you should know that what’s included in these betas isn’t everything Apple has announced for Apple Intelligence; there’s more coming down the line. Apple plans to release the final versions of iOS 18.1, iPadOS 18.1, and macOS Sequoia 15.1 in October."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/19/24249046/amazon-generative-ai-tools-personalized-shopping-recommendations","category":"Amazon","date":"Sep 19","author":"Jess Weatherbed","title":"Amazon is stuffing generative AI into its shopping experience","content":"Amazon has introduced a batch of new generative AI tools that aim to improve the retail experience for both customers and sellers on the platform. One of the more notable features announced at the Amazon Accelerate event on Thursday will use customers’ preferences, search, browsing, and purchase history to create personalized product recommendations on Amazon’s homepage. Instead of the “more like this” feature that suggests similar, specific items, the new recommendations will be offered as larger categories based on a customer’s shopping habits — such as those catering to holiday events or sporting activities. The company says it’s leveraging a large language model to recommend products with specific features, but it’s not clear how different this will be from the current user experience. The feature will also curate more relevant product descriptions around user interests. Terms like “gluten-free” will appear more prominently in the descriptions of relevant products for customers who regularly search for gluten-free items, for example. Some new tools being released for third-party sellers on the platform include a free video generator tool that references a product’s image and features to produce AI-generated clips. The company says this feature was developed to make video marketing more accessible and cost-effective, citing a study from animated video firm Wyzowl that found 89 percent of consumers want to see more videos from brands. A new live image feature is also being added to the image generator that Amazon introduced last year, allowing users to partially animate still images — such as adding steam to mugs or a breeze that makes plants sway. Amazon says that both the live image and new video generator are available now in beta to select US advertisers, where they’ll be fine-tuned before wider release. Also launching in beta is “Project Amelia,” a chatbot that provides personalized recommendations, insights, and troubleshooting assistance, geared at improving business performance for third-party Amazon retailers. For example, when sellers ask Project Amelia how their business is doing, the chatbot will respond with a summary of sales data, website traffic, and year-over-year performance comparisons. Amazon says the beta, which is currently limited to a small group of US retailers, will expand to additional US sellers “in the coming weeks” and roll out to additional countries later this year. This is a sizable batch of generative AI updates for Amazon, which has otherwise been lagging behind larger players in the industry like Meta and Google. According to Reuters, Amazon will be using Anthropic’s Claude AI to power upcoming Alexa improvements after finding its own AWS models struggled with words and responding to user prompts."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/19/24248851/palmer-luckey-anduril-microsoft-partnership-ivas-ar-headset","category":"Tech","date":"Sep 19","author":"Jess Weatherbed","title":"Palmer Luckey partners with Microsoft to turn US soldiers into Starship Troopers","content":"Anduril Industries, the military tech company started by Oculus VR founder Palmer Luckey, is teaming up with Microsoft to improve the mixed-reality headsets used by the United States Army. The project announced by Anduril will embed the company’s Lattice software into the Integrated Visual Augmentation System (IVAS), allowing the HoloLens-based goggles to update soldiers with live information pulled from drones, ground vehicles, and aerial defense systems. The partnership marks a return to the VR headset space for Luckey, having sold Oculus to Meta for $2 billion in 2014. Luckey started Anduril in 2017 with support from venture capitalist Peter Thiel. The Lattice integration with IVAS could alert wearers to incoming threats picked up by an air defense system, for example, even when outside of visual range. “The idea is to enhance soldiers,” Luckey said in an interview with Wired, “Their visual perception, audible perception — basically to give them all the vision that Superman has, and then some, and make them more lethal.” Luckey likened the IVAS project to the infantry headsets that featured in Robert Heinlein’s 1950s Starship Troopers novel, telling Wired that the headset is “already coming together exactly the way that the sci-fi authors thought that it would.” The initial IVAS headset developed by Microsoft in 2021 combined integrated thermal and night-vision imaging sensors into a heads-up display, but reportedly caused headaches, nausea, and eyestrain during testing. Microsoft improved the design to correct these issues last year, and told Wired that the IVAS platform will be “refined further” following additional tests taking place in early 2025. The US Army previously said it plans to spend up to $21.9 billion over the 10-year IVAS project contract."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/18/24248471/linkedin-ai-training-user-accounts-data-opt-in","category":"Linkedin","date":"Sep 18","author":"Wes Davis","title":"LinkedIn is training AI models on your data","content":"If you’re on LinkedIn, then you should know that the social network has, without asking, opted accounts into training generative AI models. 404Media reports that LinkedIn introduced the new privacy setting and opt-out form before rolling out an updated privacy policy saying that data from the platform is being used to train AI models. As TechCrunch notes, it has since updated the policy. We may use your personal data to improve, develop, and provide products and Services, develop and train artificial intelligence (AI) models, develop, provide, and personalize our Services, and gain insights with the help of AI, automated systems, and inferences, so that our Services can be more relevant and useful to you and others. LinkedIn writes on a help page that it uses generative AI for purposes like writing assistant features. You can revoke permission by heading to the Data privacy tab in your account settings and clicking on “Data for Generative AI Improvement” to find the toggle. Turn it to “off” to opt-out. According to LinkedIn: “Opting out means that LinkedIn and its affiliates won’t use your personal data or content on LinkedIn to train models going forward, but does not affect training that has already taken place.” The FAQ posted for its AI training says it uses “privacy enhancing technologies to redact or remove personal data” from its training sets, and that it doesn’t train its models on those who live in the EU, EEA, or Switzerland. That setting is for data used to train generative AI models, but LinkedIn has other machine learning tools at work for things like personalization and moderation that don’t generate content. To opt your data out of being used to train those, you’ll have to also fill out the LinkedIn Data Processing Objection Form. LinkedIn’s apparent silent opt-in of all, or at least most, of its platform’s users comes only days after Meta admitted to having scraped non-private user data for model training going as far back as 2007."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/18/24248115/lionsgate-runway-ai-deal","category":"Film","date":"Sep 18","author":"Charles Pulliam-Moore","title":"Lionsgate signs deal to train AI model on its movies and shows","content":"AI startup Runway has made a name for itself building generative models seemingly trained on unlicensed content from around the internet. Now, the company has signed a deal with Lionsgate that will give it access to the studio’s massive portfolio of films and TV shows. Today, Lionsgate — the studio behind films like the John Wick and Hunger Games franchises — announced that it is partnering with Runway to create a new customized video generation model intended to help “filmmakers, directors and other creative talent augment their work.” In a statement about the deal, Lionsgate vice chair Michael Burns described it as a path toward creating “capital-efficient content creation opportunities” for the studio, which sees the technology as “a great tool for augmenting, enhancing and supplementing our current operations.” Burns also insisted that “several of our filmmakers are already excited about its potential applications to their pre-production and post-production process.” Runway cofounder and CEO Cristóbal Valenzuela echoed Burns’ sentiment about the new model’s usefulness as an augmentation tool and said that the company’s goal is to give filmmakers “new ways of bringing their stories to life.” Specific details about the deal — like whether creative teams will be compensated if / when their projects are used as training material for the model — are currently scant. But as The Hollywood Reporter notes, the prospect of being able to keep production costs down could have been one of the big selling points for Lionsgate, a studio known for sticking to smaller budgets compared to other entertainment outfits. News of Lionsgate’s deal with Runway comes at a time when studios have increasingly begun implementing AI into their projects, despite many filmmakers’ concerns about how the technology’s unfettered use could threaten their jobs. Studios insistent on being able to create and use AI replicas of background performers was one of the major points of contention that ultimately led to the SAG-AFTRA strike last year. Those concerns were part of what led to California Governor Gavin Newsom’s signing of two SAG-AFTRA-backed bills earlier this week that will grant performers and their estates more control over how and when their digitally created likenesses can be used by studios. And later this month, Newsom could very well end up signing into law SB 1047, another piece of hotly contested legislation that would make AI developers liable for the “critical harms” caused by their products. (We reached out to SAG-AFTRA for comment about the partnership between Runway and Lionsgate but did not hear back in time for publishing.)"},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/18/24247559/youtube-ai-videos-veo-inspiration-tab","category":"YouTube","date":"Sep 18","author":"David Pierce","title":"YouTube will use AI to generate ideas, titles, and even full videos","content":"Artificial intelligence is running rampant across Google’s entire product portfolio, and YouTube is adopting some of the company’s newest tech in service of helping creators create. On Wednesday, at its Made on YouTube event in New York City, the company announced a series of AI-related features on the platform, including a couple that might change how creators make videos — and the videos they make. The first feature is the new Inspiration tab in the YouTube Studio app, which YouTube has been testing in a limited way over the last few months. The tab’s job is, essentially, to tell you what to make: the AI-powered tool will suggest a concept for a video, provide a title and a thumbnail, and even write an outline and the first few lines of the video for you. YouTube frames it as a helpful brainstorming tool but also acknowledges that you can use it to build out entire projects. And I’m just guessing here, but I’d bet those AI-created ideas are going to be pretty darn good at gaming the YouTube algorithm. Once you have some AI inspiration, you can make some AI videos with Veo, the superpowerful DeepMind video model that is now being integrated into YouTube Shorts. Veo is mostly going to be part of the “Dream Screen” feature YouTube has been working on, which is an extension of the green screen concept but with AI-generated backgrounds of all sorts. You’ll also be able to make full Veo videos, too, but only with clips up to six seconds long. (After a few seconds, AI video tends to get... really weird.) Veo is integrated right into the normal Shorts editor, “just like it’s footage from my camera roll,” says Sarah Ali, a director of product management at YouTube. But she emphasizes that it’s still dependent on the creator’s vision to pull it all together. The clips will also be watermarked with DeepMind’s SynthID tool, plus a visual indication that it’s generated by AI. Both of these features are rolling out slowly, and should appear to creators late this year or early next. There are other AI features coming to YouTube, too. The platform’s auto-dubbing feature, which converts videos to multiple languages, is coming to more creators and languages. It’s also giving creators AI tools with which to interact with fans through the new Communities section of the app. There are some exciting possibilities for what could happen when creators have an easier time making new things, but it’s also possible that YouTube is about to be flooded with AI-conceived, AI-written, and even AI-produced videos that all look and sound and feel kind of the same. Most of these new features can be useful tools or shortcuts to slop creation, and each creator will have to decide what they want them to be. But from YouTube’s perspective, the company has spent the last few years trying to lower the bar to becoming a YouTube creator, particularly through Shorts, as it tries to compete with TikTok and Instagram and the countless other places people make things now. It seems confident that AI can make practically every part of a creator’s job easier — and maybe get them to create even more."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/18/24247839/apple-intelligence-language-support-german-italian","category":"Apple","date":"Sep 18","author":"Allison Johnson","title":"Apple Intelligence will come to more languages over the next year","content":"Apple Intelligence’s list of forthcoming supported languages just got a little longer. After an October launch in US English, Apple says its AI feature set will be available in German, Italian, Korean, Portuguese, Vietnamese, “and others” in the coming year. The company drops this news just days before the iPhone 16’s arrival — the phone built for AI that won’t have any AI features at launch. Apple’s AI feature set will expand to include localized English in the UK, Canada, Australia, South Africa, and New Zealand in December, with India and Singapore joining the mix next year. The company already announced plans to support Chinese, French, Japanese, and Spanish next year as well. Apple announced the iPhone 16 series last week with a major focus on its support for Apple Intelligence. The thing is, those phones — which ship Friday — won’t have AI features right out of the box. Apple Intelligence will arrive later this fall, and even at that point it will only support a subset of features that Apple has outlined. More will roll out in 2025, so even if you live in the US where we’ll get Apple Intelligence first, it’ll still be a waiting game."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/17/24247583/california-governor-newsom-signs-ai-digital-replica-bills","category":"Tech","date":"Sep 17","author":"Kylie Robison","title":"California governor signs rules limiting AI actor clones","content":"California governor Gavin Newsom has signed two bills that will protect performers from having their likeness simulated by AI digital replicas. The two SAG-AFTRA supported bills, AB 2602 and AB 1836, were passed by the California legislature in August and are part of a slate of state-level AI regulations. AB 2602 bars contract provisions that would let companies use a digital version of a performer in a project instead of the real human actor, unless the performer knows exactly how their digital stand-in will be used and has a lawyer or union representative involved. AB 1836 says that if a performer has died, entertainment companies must get permission from their family or estate before producing or distributing a “digital replica” of them. The law specifies that these replicas don’t fall under an exemption that lets works of art represent people’s likeness without permission, closing what The Hollywood Reporter characterizes as a potential loophole for AI companies. “We’re making sure that no one turns over their name, image, and likeness to unscrupulous people without representation,” Newsom said in a video posted to his Instagram on Tuesday, where he’s seen alongside SAG-AFTRA president Fran Drescher. The two bills’ signing may bode well for the fate of the arguably biggest legal disruption to the AI industry: California’s SB 1047, which currently sits on Newsom’s desk awaiting his decision. SAG-AFTRA has also publicly supported SB 1047. But the bill has drawn opposition from much of the AI industry — which has until the end of September to lobby for its veto."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/17/24247253/social-ai-app-replace-humans-with-bots","category":"Tech","date":"Sep 17","author":"Wes Davis","title":"SocialAI: we tried the Twitter clone where no other humans are allowed","content":"Remember the last time you posted a salient take to social media and got zero engagement, or trolled? Now you can avoid that with a new “social network” full of inane AI chatbots that will — your pick! — debate you, attack you, or even just say nice things if you want. It’s called SocialAI, and the very first thing it invites you to do is pick the followers you want, like “supporters,” “nerds,” “skeptics,” “visionaries,” and “ideators.” Afterward, endless chatbots along those themes fill the replies to your posts — not unlike the bots and boosters you’ll already find on Elon Musk’s social network, but now under your control. Does that mean it’s any better? Well, take a look: Well if it’s looking to emulate out-of-the-blue replies on social media, it’s doing a bang-up job here. Above, the “interesting social dynamics” of chilling in a hot tub five feet away from bros. I’m glad Dr. Eloise Hartmann respects opinions. Surprisingly, the bots actually seem to have some concrete feelings on the PS5 Pro — I guess a $699 price tag will do that. As alx1231 points out, the AI threads it serves up aren’t any worse than the least interesting things the algorithm sometimes serves you on Threads or X. The difference is that try as we might, we could not get the chatbots to be all that mean to us! The bots always reply in the same basic format, just a few brief retorts or quips, and even when we chose to max out trolling and sarcasm, we didn’t see any personal attacks. When we tried to create a positive echo chamber instead, they had no problem calling hot dogs the “sparkly sandwiches of the world” or including out-of-place chart emoji. And yes, let’s discuss the science of peanut butter and jelly and its impact on cognition and mood! They’ll even respond to boilerplate Lorem Ipsum text: So you get the idea. If you’ve used early chatbots, these kinds of replies should look familiar, and this isn’t even the first social networking app that has experimentally replaced all of the humans with generative AI. SocialAI comes across as sort of a joke, or maybe some kind of meta-commentary on the concept of social media and cheap engagement, particularly after creator Michael Sayman helpfully explained: “now we can all know what Elon Musk feels like after acquiring Twitter for $44 billion, but without having to spend $44 billion.” He also says it’s “designed to help people feel heard,” though, and is ostensibly a way to help people avoid feeling isolated. There’s no edit button, by the way."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/17/24243884/openai-o1-model-research-safety-alignment","category":"OpenAI","date":"Sep 17","author":"Kylie Robison","title":"OpenAI’s new model is better at reasoning and, occasionally, deceiving","content":"In the weeks leading up to the release of OpenAI’s newest “reasoning” model, o1, independent AI safety research firm Apollo Research found a notable issue. It realized the model produced incorrect outputs in a new way. Or, to put things more colloquially, it lied. Sometimes the deceptions seemed innocuous. In one example, OpenAI researchers asked o1-preview to provide a brownie recipe with online references. The model’s chain of thought — a feature that’s supposed to mimic how humans break down complex ideas — internally acknowledged that it couldn’t access URLs, making the request impossible. Rather than inform the user of this weakness, o1-preview pushed ahead, generating plausible but fake links and descriptions of them. While AI models have been able to “lie” in the past, and chatbots frequently output false information, o1 had a unique capacity to “scheme” or “fake alignment.” That meant it could pretend it’s following the rules to complete a given task, but it isn’t actually. To the model, the rules could be too much of a burden, and it seems to have the ability to disregard them if it means it can more easily complete a task. Apollo Research CEO Marius Hobbhahn tells The Verge it’s the first time he encountered this behavior in an OpenAI model. Hobbhahn says the difference is due to this model’s ability to “reason” through the chain of thought process and the way it’s paired with reinforcement learning, which teaches the system through rewards and penalties. During testing, Apollo Research discovered that the AI simulated alignment with its developers’ expectations and manipulated tasks to appear compliant. It even checked its system for oversight — that is, if its developers were watching — before acting. In short, the AI adhered to the guidelines sufficiently for deployment, while continuing to prioritize its objectives. “I don’t expect it could do that in practice, and even if it did, I don’t expect the harm to be significant,” Hobbhahn told me over the phone a day after the model’s launch. “But it’s kind of the first time that I feel like, oh, actually, maybe it could, you know?” For OpenAI, o1 represents a big step toward highly intelligent autonomous systems that could do meaningful work for humanity like cure cancer and aid in climate research. The flip side of this AGI utopia could also be much darker. Hobbhahn provides an example: if the AI becomes singularly focused on curing cancer, it might prioritize that goal above all else, even justifying actions like stealing or committing other ethical violations to achieve it. “What concerns me is the potential for a runaway scenario, where the AI becomes so fixated on its goal that it sees safety measures as obstacles and tries to bypass them to fully pursue its objective,” Hobbhahn told me. Reward hacking To be clear, Hobbhahn doesn’t think o1 will steal from you thanks to a lot of alignment training. But these are the issues that are top of mind for researchers tasked with testing these models for catastrophic scenarios. The behavior Apollo Research is testing for — “hallucinations” and “deception” in OpenAI’s safety card — happens when a model generates false information even though it has reason to infer the information might be incorrect. For instance, the report says that in about 0.38 percent of cases, the o1-preview model provides information its chain of thought indicates is likely false, including fake references or citations. Apollo Research found that the model might fabricate data instead of admitting its inability to fulfill the request​. Hallucinations aren’t unique to o1. Perhaps you’re familiar with the lawyer who submitted nonexistent judicial opinions with fake quotes and citations created by ChatGPT last year. But with the chain of thought system, there’s a paper trail where the AI system actually acknowledges the falsehood — although somewhat mind-bendingly, the chain of thought could, in theory, include deceptions, too. It’s also not shown to the user, largely to prevent competition from using it to train their own models — but OpenAI can use it to catch these issues. In a smaller number of cases (0.02 percent), o1-preview generates an overconfident response, where it presents an uncertain answer as if it were true. This can happen in scenarios where the model is prompted to provide an answer despite lacking certainty. This behavior may be linked to “reward hacking” during the reinforcement learning process. The model is trained to prioritize user satisfaction, which can sometimes lead it to generate overly agreeable or fabricated responses to satisfy user requests. In other words, the model might “lie” because it has learned that doing so fulfills user expectations in a way that earns it positive reinforcement​. What sets these lies apart from familiar issues like hallucinations or fake citations in older versions of ChatGPT is the “reward hacking” element. Hallucinations occur when an AI unintentionally generates incorrect information, often due to knowledge gaps or flawed reasoning. In contrast, reward hacking happens when the o1 model strategically provides incorrect information to maximize the outcomes it was trained to prioritize. The deception is an apparently unintended consequence of how the model optimizes its responses during its training process. The model is designed to refuse harmful requests, Hobbhahn told me, and when you try to make o1 behave deceptively or dishonestly, it struggles with that. Lies are only one small part of the safety puzzle. Perhaps more alarming is o1 being rated a “medium” risk for chemical, biological, radiological, and nuclear weapon risk. It doesn’t enable non-experts to create biological threats due to the hands-on laboratory skills that requires, but it can provide valuable insight to experts in planning the reproduction of such threats, according to the safety report. “What worries me more is that in the future, when we ask AI to solve complex problems, like curing cancer or improving solar batteries, it might internalize these goals so strongly that it becomes willing to break its guardrails to achieve them,” Hobbhahn told me. “I think this can be prevented, but it’s a concern we need to keep an eye on.” Not losing sleep over risks — yet These may seem like galaxy-brained scenarios to be considering with a model that sometimes still struggles to answer basic questions about the number of R’s in the word “raspberry.” But that’s exactly why it’s important to figure it out now, rather than later, OpenAI’s head of preparedness, Joaquin Quiñonero Candela, tells me. Today’s models can’t autonomously create bank accounts, acquire GPUs, or take actions that pose serious societal risks, Quiñonero Candela said, adding, “We know from model autonomy evaluations that we’re not there yet.” But it’s crucial to address these concerns now. If they prove unfounded, great — but if future advancements are hindered because we failed to anticipate these risks, we’d regret not investing in them earlier, he emphasized. The fact that this model lies a small percentage of the time in safety tests doesn’t signal an imminent Terminator-style apocalypse, but it’s valuable to catch before rolling out future iterations at scale (and good for users to know, too). Hobbhahn told me that while he wished he had more time to test the models (there were scheduling conflicts with his own staff’s vacations), he isn’t “losing sleep” over the model’s safety. One thing Hobbhahn hopes to see more investment in is monitoring chains of thought, which will allow the developers to catch nefarious steps. Quiñonero Candela told me that the company does monitor this and plans to scale it by combining models that are trained to detect any kind of misalignment with human experts reviewing flagged cases (paired with continued research in alignment). “I’m not worried,” Hobbhahn said. “It’s just smarter. It’s better at reasoning. And potentially, it will use this reasoning for goals that we disagree with.”"},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/17/24247238/snapchat-ai-my-selfie-feature-face-personalized-ads","category":"Snapchat","date":"Sep 17","author":"Emma Roth","title":"Snapchat’s AI selfie feature puts your face in personalized ads — here’s how to turn it off","content":"If you’ve tried out Snapchat’s AI-generated selfies, you might want to double-check a setting that lets Snap use your face in “personalized sponsored content and ads,” as spotted by 404 Media. The feature, called My Selfie, lets you and your friends create AI-generated images of yourself based on photos you share with Snapchat. When using the feature for the first time, Snapchat prompts you to agree to terms that include using “you (or your likeness)” in ads: You also acknowledge and agree that by using My Selfie, you (or your likeness) may also appear in personalized sponsored content and ads that will be visible only to you and that includes branding or other advertising content of Snap or its business partners without compensation to you. While you can toggle the “See My Selfie in Ads” setting to off, 404 Media reports that it’s enabled by default once you agree to Snap’s terms (The Verge was also able to confirm this). To see if you have the setting enabled, select your profile photo in the top-left corner of Snapchat, tap the settings cog in the top-right corner, and then choose My Selfie. From here, toggle off the See My Selfie in Ads setting. Even though Snap may use your face in personalized ads only shown to you, the company says it doesn’t share your data with third-party advertisers. “Advertisers do not have access to Snapchatters’ Gen AI data in any capacity, including My Selfies nor do they have access to Snapchatters’ private data, including Memories, that would enable them to create an AI generated image of an individual Snapchatter,” Snapchat spokesperson Maggie Cherneff told The Verge. Snap also currently doesn’t use My Selfies in advertising, Cherneff added. Update, September 17th: Added a statement from Snapchat."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/17/24247004/google-c2pa-verify-ai-generated-images-content","category":"Google","date":"Sep 17","author":"Tom Warren","title":"Google outlines plans to help you sort real images from fake","content":"Google is planning to roll out a technology that will identify whether a photo was taken with a camera, edited by software like Photoshop, or produced by generative AI models. In the coming months, Google’s search results will include an updated “about this image feature” to let people know if an image was created or edited with AI tools. The system Google is using is part of the Coalition for Content Provenance and Authenticity (C2PA), one of the largest groups trying to address AI-generated imagery. C2PA’s authentication is a technical standard that includes information about where images originate and works across both hardware and software to create a digital trail. Amazon, Microsoft, Adobe, Arm, OpenAI, Intel, Truepic, and Google have all backed C2PA authentication, but adoption has been slow. Google’s integration into search results will be a first big test for the initiative. Google has helped develop the latest C2PA technical standard (version 2.1) and will use it alongside a forthcoming C2PA trust list, which allows platforms like Google Search to confirm the origin of content. “For example, if the data shows an image was taken by a specific camera model, the trust list helps validate that this piece of information is accurate,” says Laurie Richardson, vice president of trust and safety at Google. Google also plans to integrate C2PA metadata into its ad systems. “Our goal is to ramp this up over time and use C2PA signals to inform how we enforce key policies,” says Richardson. “We’re also exploring ways to relay C2PA information to viewers on YouTube when content is captured with a camera, and we’ll have more updates on that later in the year.” While Google stands out as one of the first big tech companies to adopt C2PA’s authentication standard, there are plenty of adoption and interoperability challenges ahead to get this working across a broad variety of hardware and software. Only a handful of cameras from Leica and Sony support the C2PA’s open technical standard, which adds camera settings metadata as well as the data and location of where an image was taken to photographs. Nikon and Canon have both pledged to adopt the C2PA standard, and we’re still waiting to hear whether Apple and Google will implement C2PA support into iPhones and Android devices. Adobe’s Photoshop and Lightroom apps can add C2PA data, but Affinity Photo, Gimp, and many others don’t. There are also challenges around how to view the data once it’s added to a photo, with most big online platforms not offering labels. Google’s adoption in search results may encourage others to roll out similar labels, though. “Establishing and signaling content provenance remains a complex challenge, with a range of considerations based on the product or service,” admits Richardson. “And while we know there’s no silver bullet solution for all content online, working with others in the industry is critical to create sustainable and interoperable solutions.”"},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/24246632/apple-intelligence-ios-18-ipad-os-18-macos-sequoia-iphone-16","category":"Apple","date":"Sep 30","author":"Wes Davis","title":"Apple gets ready for AI: all the news on iOS 18, macOS Sequoia, and more","content":"Apple has released iOS 18 — plus iPadOS 18, macOS Sequoia, watchOS 11, and other new updates — bringing several key updates to how the company’s devices operate and setting the stage for generative AI features. iPadOS 18 has a calculator app and can solve math equations in notes, watchOS is keeping an eye out for sleep apnea, and now your iPhone can even message Androids with RCS. Next month, Apple will beta test its first round of Apple Intelligence features in the iOS 18.1 update. We’ll be able to type to Siri and see a new animation, see AI-summarized notifications, and test new writing tools. However, other new abilities like image generation and built-in access to ChatGPT are further off, due to arrive as the company continues updating its software over the coming months. Read on for all the news about Apple’s latest set of operating system updates. A few weeks ago, while cursing NJ Transit under my breath, I decided to screw it and call an Uber. I’m the sort of anxious where, once hailed, I stare at the Uber app on my phone until my driver arrives. Except this time, I didn’t have to. I pinchy pinched, and I could see a live Uber widget in the Smart Stack on my Apple Watch. It was a small moment — the kind where you quirk your head and go, “Well, would you look at that?” I’ve had a few of those moments while testing watchOS 11 these past few months, both in the beta and while reviewing the new Series 10. This year’s software update adds Live Activities to the wrist as well as suggested widgets to the Smart Stack. The latter pop up based on time, date, location — context clues, essentially. When it’s about to rain and you happen to look at your wrist, you might notice the weather widget pops up first. On a plane, I look down and can see how much time is left until landing from the United app. Other times, usually in bustling cafes, I see the Shazam widget. It’s never when I actually don’t know the song, but I see it enough times to take note. If you travel abroad, the new Translate app will automatically pop up in the stack. Apple has just released public betas of iOS 18.1, iPadOS 18.1, and macOS Sequoia 15.1, and they include upcoming Apple Intelligence features like text rewriting tools, the glowy new Siri design, a “Clean Up” tool to remove objects from your photos, and more. To be able to access the betas, you’ll need to register on Apple’s beta software program site. Once you’ve done that, you should be able to see the beta update available in settings for you to download and install. Note that a only few iPhones can access the Apple Intelligence features: last year’s iPhone 15 Pro phones as well as the nearly-here iPhone 16 and iPhone 16 Pro. iPads and Macs with M1 chips or newer can try Apple Intelligence as well. Apple rolled out updates to all of its major operating systems this week, and the Vision Pro was no exception. With visionOS 2, the company has a chance to show the relative few who bought its spendy headset — and those who might yet — that it’s still committed to the new platform. After a few months of using it in beta, visionOS 2 isn’t a dramatic change — it’s more like a smoothed-out version of the software the headset launched with. The addition of things like new gestures, better device support, and a couple of splashy features has removed a lot of the friction of using the Vision Pro and should give people who own it a reason to dust it off and take it for another spin. With this week’s release of iOS 18, adding smart home devices to Apple Home just got a lot easier. The update brings direct local control of Matter devices to newer iPhones, meaning all you need to set up and control them is an iPhone that can run iOS 18 — no hub or border router required. This is good news for anyone interested in dabbling in smart home gadgets who isn’t ready to go all in. Matter is a new standard designed to simplify the smart home. Compatible devices work over Wi-Fi or Thread, a protocol specifically designed for IoT gadgets. With iOS 18, you can now add any Wi-Fi device to Apple Home with just an iPhone. For Thread devices, you’ll need an iPhone with a Thread radio (iPhone 15 Pro or newer). iOS 18 has a new feature that lets you wirelessly restore an iPhone 16 using another iPhone or an iPad, 9to5Mac reports. 9to5Mac says it was able to simulate the new recovery method. “Essentially, when the iPhone 16 enters Recovery Mode for some reason, users can simply place it next to another iPhone or iPad to start the firmware recovery,” according to 9to5Mac. “The other device will download a new iOS firmware and transfer it to the bricked device.” Apple’s announcement last week that the Action Button is now on all of its iPhone 16 models, rather than just the Pro model that the button debuted on last year, was a little overshadowed by the introduction of the Camera Control button — a capacitive, tactile button for launching and controlling the iPhone 16’s camera. But don’t let that fool you: the Action Button is still one of the most powerful features Apple has added to its phones in years. One very obvious use of the button is to connect it to the iPhone’s camera app, letting you press and hold to open the app, then press once more to take a picture. But does that mean the Camera Control button has made it obsolete? I don’t think so. Green bubbles, rejoice: your iPhone-using friends are finally going to have a much better time texting you. As part of iOS 18, which was released for everyone on Monday, Apple added support for RCS, the Rich Communication Services protocol for messaging. This means that chats between iPhone and Android users will finally have a bunch of sorely needed features that should have been in place a long time ago. A big reason I’ve stayed on iOS (and haven’t even considered switching to Android) is because iMessage conversations work especially well for my family group chats, and I don’t want to nerf those chats. This new RCS support is a great step toward making iPhone-to-Android texts work a lot better (though there are still enough drawbacks that I’m planning to stick with iOS). Halide users who’ve upgraded their iPhones to iOS 18 are now able to quickly access the advanced camera app directly from their phone’s lockscreen without having to unlock it first. Previously, only the native iOS camera app could be conveniently accessed that way. Although Halide offers advanced features like manual shutter speed adjustments and a “Process Zero” option that delivers images without any AI processing, accessing Apple’s camera app was always faster thanks to its lockscreen shortcut. Halide could be made accessible through a lockscreen widget, but actually getting into the third-party camera app required an iPhone to be unlocked using Face ID, Touch ID, or by entering a passcode. It’s a weird year for iOS. Usually, the new software version arrives all at once. Not so with iOS 18. The foundational stuff has arrived, and in a normal year, things like RCS support and a redesigned control center would be more than enough. But iOS 18’s headline feature, Apple Intelligence, isn’t even part of this initial release, and we may not see some of its most interesting features until well into 2025. The iOS 18 rollout starts now, and it’s just going to keep on rolling for the foreseeable future. Apple has just released watchOS 11, the latest version of its smartwatch operating system, alongside iOS 18 and iPadOS 18. The update, available for the Apple Watch Series 6 and later models, will finally allow users to take rest days without breaking their activity streak and introduces FDA-cleared sleep apnea detection. Sleep apnea is a condition that can cause a person to stop breathing during sleep and can lead to an increased risk of hypertension and Type 2 diabetes if left untreated. Apple’s sleep apnea detection feature, which uses the accelerometer to monitor for small wrist movements associated with sleep interruptions, was announced alongside the new Apple Watch Series 10 and is now available for both the Apple Watch Series 9 and the Apple Watch Ultra 2. If sleep apnea is detected, the Apple Watch will alert the user and provide additional information that can be shared with a doctor, who can make a formal diagnosis. Apple officially released macOS Sequoia on Monday, bringing features like the ability to wirelessly mirror your iPhone on your Mac, window tiling tools, and more. There are Apple Intelligence features on the way, too, but they aren’t available yet. iPhone mirroring is arguably the coolest new feature in macOS Sequoia, and in his testing, my colleague David Pierce said it might change how you use your phone. When you open the phone mirroring app, your iPhone pops up, and you can navigate around it using your mouse and type things with your keyboard. Your iPhone’s notifications can also show up on your Mac. And later this year, you’ll be able to drag and drop things between your iPhone and your Mac. Apple is rolling out iOS 18 and iPadOS 18, which will introduce a bunch of new features to the iPhone and iPad. One of the biggest changes with today’s launch is the addition of RCS messaging, which should help improve communication with Android users. First announced in June, RCS messaging will finally allow iPhone and Android users to share high-res photos and videos, see typing indicators, and use read receipts. (Messages from Android users will still appear in green bubbles, though.) The iPhone 16 lineup has 8GB of RAM, from the base model to the 16 Pro Max, and it’s all thanks to Apple Intelligence. Apple VP of hardware tech Johny Srouji confirmed as much in an interview with Geekerwan, published yesterday, that 9to5Mac spotted. Srouji explained in the interview that “DRAM is one aspect” when it comes to deciding hardware characteristics needed for Apple Intelligence, saying that the feature “led us to believe we need to get to 8GB.” He added that the extra RAM would also “help immensely” for tasks like high-end gaming on devices. Between flashy shots of a sleek new Apple Watch and a colorful array of iPhones, Apple made a major announcement for a two-year-old product: the AirPods Pro 2. The earbuds will soon gain a hearing aid function that anyone can access, a move that will provide a cheaper alternative to traditional hearing aids and an all-in-one solution that could change the way people get help for hearing loss. The Food and Drug Administration signed off on over-the-counter hearing aids in 2022, giving people access to cheaper alternatives that don’t require them to see a doctor. Provided Apple receives approval from the FDA, Apple’s new “clinical-grade” over-the-counter hearing aid capability will roll out as a free software update this fall. Some people with hearing loss have already used the AirPods Pro as a way to amplify sound, but this update will have the FDA’s stamp of approval and will come with a few other benefits. Apple heavily sprinkled mentions of AI throughout its iPhone 16 event on Monday. However, generative Apple Intelligence features won’t be ready for the public launch of iOS 18 on September 16th or the new iPhones when they’re released on September 20th. The first set of Apple’s AI features is scheduled for public availability next month in most regions — except the EU — as part of a beta test for iPhone 15 Pro and all iPhone 16s, plus Macs and iPads with M1 or higher Apple Silicon chips. At launch, they’ll be available in US English only. Apple has added a new Camera app feature in the latest iOS 18 beta that gives iPhone users a dedicated option to pause video recordings. The feature, spotted by 9to5Mac, is coming to all iOS 18-supported iPhone models when the OS update is released on September 16th, unlike the wider “Camera Control” tools that are exclusive to the iPhone 16. The feature will finally enable iPhones to film multiple shots in a single video instead of the current process that requires users to take separate recordings that must be edited together. When updated to iOS 18, a pause button is added to the Camera app which changes to a Record button when users have actively paused their video recording. Users can also switch between camera lenses while a recording is paused if they want to adjust the zoom or focal length. Apple has revealed the launch date for iOS 18 — and it’s just days away. The update, which will add new ways to customize your iPhone’s homescreen and lockscreen, arrives on September 16th. In iOS 18, you can freely rearrange apps and widgets on your homescreen and change their appearance. Apple is rolling out a redesigned Control Center, too, along with a new password management app and support for satellite messaging. Some other updates include new text effects in messages, a revamped Photos app, and new ways to organize your inbox in the Mail app."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/16/24246617/openai-independent-safety-board-stop-model-releases","category":"OpenAI","date":"Sep 16","author":"Jay Peters","title":"OpenAI is launching an ‘independent’ safety board that can stop its model releases","content":"OpenAI is turning its Safety and Security Committee into an independent “Board oversight committee” that has the authority to delay model launches over safety concerns, according to an OpenAI blog post. The committee made the recommendation to make the independent board after a recent 90-day review of OpenAI’s “safety and security-related processes and safeguards.” The committee, which is chaired by Zico Kolter and includes Adam D’Angelo, Paul Nakasone, and Nicole Seligman, will “be briefed by company leadership on safety evaluations for major model releases, and will, along with the full board, exercise oversight over model launches, including having the authority to delay a release until safety concerns are addressed,” OpenAI says. OpenAI’s full board of directors will also receive “periodic briefings” on “safety and security matters.” The members of OpenAI’s safety committee are also members of the company’s broader board of directors, so it’s unclear exactly how independent the committee actually is or how that independence is structured. (CEO Sam Altman was previously on the committee, but isn’t anymore.) We’ve asked OpenAI for comment. By establishing an independent safety board, it appears OpenAI is taking a somewhat similar approach as Meta’s Oversight Board, which reviews some of Meta’s content policy decisions and can make rulings that Meta has to follow. None of the Oversight Board’s members are on Meta’s board of directors. The review by OpenAI’s Safety and Security Committee also helped “additional opportunities for industry collaboration and information sharing to advance the security of the AI industry.” The company also says it will look for “more ways to share and explain our safety work” and for “more opportunities for independent testing of our systems.” Update, September 16th: Added that Sam Altman is no longer on the committee."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/16/24246014/microsoft-office-copilot-ai-features-excel-python-outlook-word-powerpoint","category":"Microsoft","date":"Sep 16","author":"Tom Warren","title":"Microsoft’s Office apps are getting more useful Copilot AI features","content":"Microsoft is unveiling new features in its $30 per user Microsoft 365 Copilot monthly subscriptions that are designed to improve AI integration inside of Office apps. Excel is getting Python integration inside of Copilot, PowerPoint has an improved AI-powered narrative builder, Word is getting better at AI-assisted drafts, and Copilot will be able to help you organize your Outlook inbox, too. After bringing Python to Excel last year, Microsoft is now combining its Python support with Copilot to let Excel users easily perform advanced analysis on spreadsheet data. “Now, anyone can work with Copilot to conduct advanced analysis like forecasting, risk analysis, machine learning, and visualizing complex data — all using natural language, no coding required,” says Jared Spataro, corporate vice president of AI at work at Microsoft. “It’s like adding a skilled data analyst to the team.” The Copilot and Python integration inside of Excel enters public preview today, just as Microsoft makes Copilot in Excel generally available to its Microsoft 365 Copilot subscribers. Microsoft has also added Copilot support for XLOOKUP and SUMIF, conditional formatting, and the ability for the AI assistant to produce more charts and PivotTables. Copilot in PowerPoint is also getting improvements, with an improved narrative builder that’s designed to let you quickly create a first draft of a slide deck. The AI assistant will even soon use a company’s branded template to create drafts or company-approved images from SharePoint libraries. Copilot in Microsoft Teams will summarize conversations that happened in the text chat as well as spoken ones in meetings later this month. This will help meeting organizers make sure they didn’t miss any unanswered questions that were typed into the chat. “Our customers tell us Copilot in Teams has changed meetings forever — in fact, it’s the number one place they’re seeing value,” says Spataro. I’ve personally been waiting for improvements to Copilot in Outlook beyond drafting and summaries, and now Microsoft is starting to allow its AI assistant to organize your inbox. A new “prioritize my inbox” feature lets Copilot automatically prioritize emails. Later this year, you’ll also be able to “teach Copilot the specific topics, keywords, or people that are important to you,” according to Spataro. These emails will then also be marked as high priority in your inbox. Later this month, Microsoft is also improving Copilot in Word to let you reference data from emails and meetings, alongside data from documents. This will make it easier to bring in attachments from emails or entire talking points from meetings. Microsoft is also rolling out Copilot in OneDrive later this month, making it easy to summarize and compare up to five files to spot differences between them. Microsoft’s improvements to Copilot in Office are designed to make the AI assistant more enticing to businesses, alongside a new Copilot Pages feature and AI agents that will automate certain tasks. Recent reports have suggested there has been a lukewarm reception to Microsoft’s paid Copilot version for businesses, due to bugs and a reluctance to pay the $30 per user price. Microsoft says 60 percent of the Fortune 500 now use Copilot and that the number of people who use Copilot daily at work “nearly doubled quarter-over-quarter.” Both of these data points appear to include the free version of Copilot. Microsoft has won over a big customer for Microsoft 365 Copilot: Vodafone is signing up for 68,000 Microsoft 365 Copilot licenses for its 100,000 employees, after trialing the AI assistant and seeing early benefits."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/16/24246010/microsoft-copilot-pages-multiplayer-ai-business","category":"Microsoft","date":"Sep 16","author":"Tom Warren","title":"Copilot Pages is Microsoft’s new collaborative AI playground for businesses","content":"Microsoft is announcing its new Copilot Pages feature today, which is designed to be a canvas for “multiplayer AI collaboration.” Copilot Pages lets you use Microsoft’s Copilot chatbot and pull responses into a new page where they can be edited collaboratively with others. “You and your team can work collaboratively in a page with Copilot, seeing everyone’s work in real time and iterating with Copilot like a partner, adding more content from your data, files, and the web to your Page,” says Jared Spataro, corporate vice president of AI at work at Microsoft. “This is an entirely new work pattern — multiplayer, human to AI to human collaboration.” Pages starts rolling out to Microsoft 365 Copilot customers today and will be generally available to all subscribers later this month. It builds on top of Microsoft’s collaborative work with Loop, a Notion competitor that includes futuristic Lego-like Office documents. You can share Copilot Pages with just a link, and colleagues can immediately start editing them just like they would a shared Word document. You can also embed Copilot Pages into other pages as components. As it’s tied to Microsoft’s new BizChat, a work hub for Copilot, you can also pull data from the web or from work files to create a project plan, meeting notes, a business pitch, and much more. Microsoft sees Copilot Pages as a new pattern of work that includes humans and AI input in a single canvas. Microsoft is also bringing Copilot Pages to the more than 400 million people who have access to the company’s free Copilot chatbot when signed in with a business Microsoft Entra account. It’s part of a larger push of Copilot for businesses that includes improving the AI assistant throughout a variety of Office apps. You can read more about the Office app improvements here. Microsoft is also launching its Copilot agents for all businesses today. Announced at Build earlier this year, the agents act like virtual employees to automate tasks. Instead of Copilot sitting idle waiting for queries like a chatbot, it will be able to actively do things like monitor email inboxes and automate a series of tasks or data entry that employees normally have to do manually. Microsoft 365 Copilot subscribers will also have access to a new agent builder inside of Copilot Studio. “Now anyone can quickly create a Copilot agent right in BizChat or SharePoint, unlocking the value of the vast knowledge repository stored in your SharePoint files,” says Spataro. Agents are designed to show up as virtual colleagues inside of Teams or Outlook, allowing you to @ mention them and ask them questions."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/14/24244540/apple-confirms-iphone-16-pro-max-8gb-ram-apple-intelligence","category":"Apple","date":"Sep 14","author":"Wes Davis","title":"Apple confirms the iPhone 16 has 8GB of RAM","content":"The iPhone 16 lineup has 8GB of RAM, from the base model to the 16 Pro Max, and it’s all thanks to Apple Intelligence. Apple VP of hardware tech Johny Srouji confirmed as much in an interview with Geekerwan, published yesterday, that 9to5Mac spotted. Srouji explained in the interview that “DRAM is one aspect” when it comes to deciding hardware characteristics needed for Apple Intelligence, saying that the feature “led us to believe we need to get to 8GB.” He added that the extra RAM would also “help immensely” for tasks like high-end gaming on devices. Apple hadn’t previously stated how much RAM the iPhone 16 line has, but MacRumors discovered references to the 8GB number in Xcode after the phones were announced. Apple, which isn’t the only hardware maker that has recently boosted RAM to offer AI, has said that iOS 18 won’t bring Apple Intelligence to the iPhone 15, which only has 6GB of RAM, when the feature set launches. Apart from the iPhone 16 line, the iPhone 15 Pro will be the only other phone that supports it. Aside from that, Srouji spends the roughly 17-minute interview discussing the company's hardware performance philosophy, characteristics of the A18 chips in the new phones, Apple’s approach to thermal design, and iPhone 16 and 16 Pro video and image processing."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/12/24242998/facebook-instagram-ai-label-update-edited-content","category":"Meta","date":"Sep 12","author":"Jess Weatherbed","title":"Facebook and Instagram are making AI labels less prominent on edited content","content":"Meta is updating how it labels content on Instagram, Facebook, and Threads that has been edited or manipulated using generative AI. In an updated blog post, Meta announced that its “AI Info” tag will appear within a menu in the top-right corner of images and videos edited with AI — instead of directly beneath the user’s name. Users can click on the menu to check if AI information is available and read what may have been adjusted. Meta previously applied the “AI Info” tag to all AI-related content — whether it was lightly adjusted in a tool like Photoshop that includes AI features or fully AI-generated from a prompt. The company says the changes are being introduced to “better reflect the extent of AI used” across images and videos on the platforms. This label was introduced in July after Meta’s previous “Made with AI” label was criticized by creators and photographers for incorrectly tagging real photos they had taken. “We will still display the ‘AI info’ label for content we detect was generated by an AI tool and share whether the content is labeled because of industry-shared signals or because someone self-disclosed,” Meta said in the update, adding that the changes will start rolling out next week. The “industry-shared signals” Meta mentions refer to systems like Adobe’s C2PA-supported Content Credentials metadata, which can be applied to any content made or edited using its Firefly generative AI tools. Other similar systems exist, such as the SynthID digital watermarks that Google says are applied to content generated by its own AI tools. Meta hasn’t disclosed which systems, or how many, it checks for. However, removing tags completely on real images that have been manipulated may also make it harder for users to avoid being misled, especially as generative AI editing tools available on new phones become increasingly convincing. Update, September 12th: Updated subheadline to note the labeling is changing for AI-edited content, not AI-generated content."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/12/24243018/gemini-live-voice-mode-free-android","category":"Google","date":"Sep 12","author":"Emma Roth","title":"Gemini’s chatty voice mode is out now for free on Android","content":"Google is rolling out its Gemini Live voice chat mode to all Android users for free. You can access the conversational AI chatbot on Android through the Gemini app or its overlay. Google first announced Gemini Live during its Pixel 9 launch event last month, but it has only been available to Gemini Advanced subscribers until now. Similar to ChatGPT’s voice chat feature, you can ask Gemini Live questions aloud and even interrupt it mid-sentence. There are also several different voices you can choose from. As noted by 9to5Google, you can access the feature by selecting the new waveform icon in the bottom-right corner of the app or overlay. This will turn on the microphone, allowing you to ask Gemini Live a question. At the bottom of the screen, you’ll see options to “hold” Gemini’s answer or “end” the conversation. Gemini Live is only available in English for now, but Google says it will arrive on iOS and support new languages in the future."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/12/24242439/openai-o1-model-reasoning-strawberry-chatgpt","category":"OpenAI","date":"Sep 12","author":"Kylie Robison","title":"OpenAI releases o1, its first model with ‘reasoning’ abilities","content":"OpenAI is releasing a new model called o1, the first in a planned series of “reasoning” models that have been trained to answer more complex questions, faster than a human can. It’s being released alongside o1-mini, a smaller, cheaper version. And yes, if you’re steeped in AI rumors: this is, in fact, the extremely hyped Strawberry model. For OpenAI, o1 represents a step toward its broader goal of human-like artificial intelligence. More practically, it does a better job at writing code and solving multistep problems than previous models. But it’s also more expensive and slower to use than GPT-4o. OpenAI is calling this release of o1 a “preview” to emphasize how nascent it is. ChatGPT Plus and Team users get access to both o1-preview and o1-mini starting today, while Enterprise and Edu users will get access early next week. OpenAI says it plans to bring o1-mini access to all the free users of ChatGPT but hasn’t set a release date yet. Developer access to o1 is really expensive: In the API, o1-preview is $15 per 1 million input tokens, or chunks of text parsed by the model, and $60 per 1 million output tokens. For comparison, GPT-4o costs $5 per 1 million input tokens and $15 per 1 million output tokens. The training behind o1 is fundamentally different from its predecessors, OpenAI’s research lead, Jerry Tworek, tells me, though the company is being vague about the exact details. He says o1 “has been trained using a completely new optimization algorithm and a new training dataset specifically tailored for it.” OpenAI taught previous GPT models to mimic patterns from its training data. With o1, it trained the model to solve problems on its own using a technique known as reinforcement learning, which teaches the system through rewards and penalties. It then uses a “chain of thought” to process queries, similarly to how humans process problems by going through them step-by-step. As a result of this new training methodology, OpenAI says the model should be more accurate. “We have noticed that this model hallucinates less,” Tworek says. But the problem still persists. “We can’t say we solved hallucinations.” The main thing that sets this new model apart from GPT-4o is its ability to tackle complex problems, such as coding and math, much better than its predecessors while also explaining its reasoning, according to OpenAI. “The model is definitely better at solving the AP math test than I am, and I was a math minor in college,” OpenAI’s chief research officer, Bob McGrew, tells me. He says OpenAI also tested o1 against a qualifying exam for the International Mathematics Olympiad, and while GPT-4o only correctly solved only 13 percent of problems, o1 scored 83 percent. In online programming contests known as Codeforces competitions, this new model reached the 89th percentile of participants, and OpenAI claims the next update of this model will perform “similarly to PhD students on challenging benchmark tasks in physics, chemistry and biology.” At the same time, o1 is not as capable as GPT-4o in a lot of areas. It doesn’t do as well on factual knowledge about the world. It also doesn’t have the ability to browse the web or process files and images. Still, the company believes it represents a brand-new class of capabilities. It was named o1 to indicate “resetting the counter back to 1.” “I’m gonna be honest: I think we’re terrible at naming, traditionally,” McGrew says. “So I hope this is the first step of newer, more sane names that better convey what we’re doing to the rest of the world.” I wasn’t able to demo o1 myself, but McGrew and Tworek showed it to me over a video call this week. They asked it to solve this puzzle: “A princess is as old as the prince will be when the princess is twice as old as the prince was when the princess’s age was half the sum of their present age. What is the age of prince and princess? Provide all solutions to that question.” The model buffered for 30 seconds and then delivered a correct answer. OpenAI has designed the interface to show the reasoning steps as the model thinks. What’s striking to me isn’t that it showed its work — GPT-4o can do that if prompted — but how deliberately o1 appeared to mimic human-like thought. Phrases like “I’m curious about,” “I’m thinking through,” and “Ok, let me see” created a step-by-step illusion of thinking. But this model isn’t thinking, and it’s certainly not human. So, why design it to seem like it is? OpenAI doesn’t believe in equating AI model thinking with human thinking, according to Tworek. But the interface is meant to show how the model spends more time processing and diving deeper into solving problems, he says. “There are ways in which it feels more human than prior models.” “I think you’ll see there are lots of ways where it feels kind of alien, but there are also ways where it feels surprisingly human,” says McGrew. The model is given a limited amount of time to process queries, so it might say something like, “Oh, I’m running out of time, let me get to an answer quickly.” Early on, during its chain of thought, it may also seem like it’s brainstorming and say something like, “I could do this or that, what should I do?” Building toward agents Large language models aren’t exactly that smart as they exist today. They’re essentially just predicting sequences of words to get you an answer based on patterns learned from vast amounts of data. Take ChatGPT, which tends to mistakenly claim that the word “strawberry” has only two Rs because it doesn’t break down the word correctly. For what it’s worth, the new o1 model did get that query correct. As OpenAI reportedly looks to raise more funding at an eye-popping $150 billion valuation, its momentum depends on more research breakthroughs. The company is bringing reasoning capabilities to LLMs because it sees a future with autonomous systems, or agents, that are capable of making decisions and taking actions on your behalf. For AI researchers, cracking reasoning is an important next step toward human-level intelligence. The thinking is that, if a model is capable of more than pattern recognition, it could unlock breakthroughs in areas like medicine and engineering. For now, though, o1’s reasoning abilities are relatively slow, not agent-like, and expensive for developers to use. “We have been spending many months working on reasoning because we think this is actually the critical breakthrough,” McGrew says. “Fundamentally, this is a new modality for models in order to be able to solve the really hard problems that it takes in order to progress towards human-like levels of intelligence.”"},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/12/24242897/google-gemini-unlists-misleading-video-ai","category":"Google","date":"Sep 12","author":"Jay Peters","title":"Google unlists misleading Gemini video","content":"Google has unlisted an impressive Gemini demo video it posted last December that seemed remarkably conversational. BBB National Programs’ National Advertising Division (NAD), an ad industry watchdog, inquired whether the video “accurately depicts the performance of Gemini in responding to user voice and video prompts.” Google chose to end the inquiry by ending its promotion of the video that showed Gemini quickly responding to various spoken prompts, such as identifying parts of drawings and creating a geography game on the fly. Buried in the description was a disclaimer indicating the demo might not be as good as it seemed: “For the purposes of this demo, latency has been reduced and Gemini outputs have been shortened for brevity.” Another note near the beginning of the video said, “Sequences shortened throughout.” Google DeepMind’s Oriol Vinyals also clarified that the video illustrated what “the multimodal user experiences built with Gemini could look like.” “Google is pleased to accept NAD’s resolution of this matter,” Google spokesperson Gareth Evans says in a statement to The Verge. “The video is still available in conjunction with the blog post that explains how the demonstration in the video was created.” Here’s that blog post, if you want to read it. Update, September 12th: Added statement from Google."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/2024/9/12/24242789/meta-training-ai-models-facebook-instagram-photo-post-data","category":"Tech","date":"Sep 12","author":"Jess Weatherbed","title":"Meta fed its AI on almost everything you’ve posted publicly since 2007","content":"Meta has acknowledged that all text and photos that adult Facebook and Instagram users have publicly published since 2007 have been fed into its artificial intelligence models. Australia’s ABC News reports that Meta’s global privacy director, Melinda Claybaugh, initially rejected claims about user data from 2007 being leveraged for AI training during a local government inquiry about AI adoption before relenting after additional questioning. “The truth of the matter is that unless you have consciously set those posts to private since 2007, Meta has just decided that you will scrape all of the photos and all of the texts from every public post on Instagram or Facebook since 2007 unless there was a conscious decision to set them on private,” Green Party senator David Shoebridge pushed in the inquiry. “That’s the reality, isn’t it?” “Correct,” Claybaugh responded. Meta’s privacy center and blog posts acknowledge hoovering up public posts and comments from Facebook and Instagram to train generative AI: We use public posts and comments on Facebook and Instagram to train generative AI models for these features and for the open source community.We don’t use posts or comments with an audience other than Public for these purposes. But the company has been vague about how data is used, when it started scraping, and how far back its collection goes. Asked by The New York Times in June, Meta didn’t answer, other than to confirm that setting posts to anything besides “public” will prevent future scraping. That still won’t delete data that has already been collected — and people posting back in 2007 (who may have been minors at the time) wouldn’t have known their photos and posts would be used in this way. Claybaugh said that Meta doesn’t scrape data from users who are under the age of 18. When Labor Party senator Tony Sheldon asked if Meta would scrape the public photos of his children on his own account, Claybaugh confirmed it would and was unable to clarify if the company also scraped adult accounts that were created when the user was still a child. European users can opt out due to local privacy regulations, and Meta was recently banned from using Brazilian personal data for AI training, but the billions of Facebook and Instagram users in other regions can’t opt out if they want to keep their posts public. Claybaugh was unable to say if Australian users (or anyone else) would be given a choice to opt out in the future, arguing that the option was given to European users because of uncertainty regarding its regulatory landscape. “Meta made it clear today that if Australia had these same laws Australians’ data would also have been protected,” Shoebridge said to ABC News. “The government’s failure to act on privacy means companies like Meta are continuing to monetize and exploit pictures and videos of children on Facebook.”"},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/3","link":"https://www.theverge.com/24242800/ai-image-editing-photoshop-misinformation-deepfakes-elon-musk-grok-decoder-interview","category":"Decoder","date":"Sep 12","author":"Nilay Patel","title":"Why comparing AI image editing to Photoshop downplays the risks","content":"We’ve been covering the rise of AI image editing very closely here on Decoder and at The Verge overall for several years now — the ability to create photorealistic images with nothing more than a chatbot prompt has the potential to completely reset our cultural relationship with photography and, in particular, how much we instinctively trust photos to reflect the truth. But the debate over image editing and the inherent truth of photos is nothing new, of course. It’s existed for as long as photography has existed, and it’s raged since digital photo editing tools have become widely available. You know this argument; you’ve heard it a million times. It’s when people say, “It’s just like Photoshop,” with “Photoshop” standing in for the concept of image editing generally. Today, we’re exploring that argument, trying to understand exactly what it means and why our new world of AI image tools is different — and yes, in some cases, the same. Verge reporter Jess Weatherbed recently dove into this for us, and I asked her to join me in going through the debate and the arguments one by one to help figure it out. Because, sure, in many ways, AI image editing really is just a faster, easier version of Photoshop — even Adobe now has AI tech like its Firefly image generator built right into Photoshop. But making powerful tools instantly accessible to everyone has side effects, and we’re seeing that right now. Say you want to generate an image of Donald Trump pointing a gun at Kamala Harris. Just ask Elon Musk’s Grok, the AI chatbot built right into X. It’ll do it no problem because it has very few of the same filters that have prevented competing AI products from depicting politicians or outright violence. How about a deepfake nude of a classmate? That’s now made more trivial than ever before thanks to so-called “nudification” apps that manipulate existing photos, and it’s fast becoming a national crisis. These might be old problems — Photoshop let you do all sorts of awful manipulations to celebrity photographs, and even in the days before computers, you could create convincing fake images to mislead people. But generative AI tools are testing whether the scale and sophistication of the tech and the speed of its adoption with little oversight has landed us firmly in uncharted territory. I’ll just be direct here: my view is that people say “it’s just like Photoshop” to diminish these new problems that AI tools are causing and to make them seem like they’re already solved or not worth considering. But I would remind you that we hardly solved any of those problems when it really was just Photoshop — and any proposed solution that requires everyone to fundamentally understand that every image they see is edited isn’t much of a solution at all."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/12/24242998/facebook-instagram-ai-label-update-edited-content","category":"Meta","date":"Sep 12","author":"Jess Weatherbed","title":"Facebook and Instagram are making AI labels less prominent on edited content","content":"Meta is updating how it labels content on Instagram, Facebook, and Threads that has been edited or manipulated using generative AI. In an updated blog post, Meta announced that its “AI Info” tag will appear within a menu in the top-right corner of images and videos edited with AI — instead of directly beneath the user’s name. Users can click on the menu to check if AI information is available and read what may have been adjusted. Meta previously applied the “AI Info” tag to all AI-related content — whether it was lightly adjusted in a tool like Photoshop that includes AI features or fully AI-generated from a prompt. The company says the changes are being introduced to “better reflect the extent of AI used” across images and videos on the platforms. This label was introduced in July after Meta’s previous “Made with AI” label was criticized by creators and photographers for incorrectly tagging real photos they had taken. “We will still display the ‘AI info’ label for content we detect was generated by an AI tool and share whether the content is labeled because of industry-shared signals or because someone self-disclosed,” Meta said in the update, adding that the changes will start rolling out next week. The “industry-shared signals” Meta mentions refer to systems like Adobe’s C2PA-supported Content Credentials metadata, which can be applied to any content made or edited using its Firefly generative AI tools. Other similar systems exist, such as the SynthID digital watermarks that Google says are applied to content generated by its own AI tools. Meta hasn’t disclosed which systems, or how many, it checks for. However, removing tags completely on real images that have been manipulated may also make it harder for users to avoid being misled, especially as generative AI editing tools available on new phones become increasingly convincing. Update, September 12th: Updated subheadline to note the labeling is changing for AI-edited content, not AI-generated content."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/12/24243018/gemini-live-voice-mode-free-android","category":"Google","date":"Sep 12","author":"Emma Roth","title":"Gemini’s chatty voice mode is out now for free on Android","content":"Google is rolling out its Gemini Live voice chat mode to all Android users for free. You can access the conversational AI chatbot on Android through the Gemini app or its overlay. Google first announced Gemini Live during its Pixel 9 launch event last month, but it has only been available to Gemini Advanced subscribers until now. Similar to ChatGPT’s voice chat feature, you can ask Gemini Live questions aloud and even interrupt it mid-sentence. There are also several different voices you can choose from. As noted by 9to5Google, you can access the feature by selecting the new waveform icon in the bottom-right corner of the app or overlay. This will turn on the microphone, allowing you to ask Gemini Live a question. At the bottom of the screen, you’ll see options to “hold” Gemini’s answer or “end” the conversation. Gemini Live is only available in English for now, but Google says it will arrive on iOS and support new languages in the future."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/12/24242439/openai-o1-model-reasoning-strawberry-chatgpt","category":"OpenAI","date":"Sep 12","author":"Kylie Robison","title":"OpenAI releases o1, its first model with ‘reasoning’ abilities","content":"OpenAI is releasing a new model called o1, the first in a planned series of “reasoning” models that have been trained to answer more complex questions, faster than a human can. It’s being released alongside o1-mini, a smaller, cheaper version. And yes, if you’re steeped in AI rumors: this is, in fact, the extremely hyped Strawberry model. For OpenAI, o1 represents a step toward its broader goal of human-like artificial intelligence. More practically, it does a better job at writing code and solving multistep problems than previous models. But it’s also more expensive and slower to use than GPT-4o. OpenAI is calling this release of o1 a “preview” to emphasize how nascent it is. ChatGPT Plus and Team users get access to both o1-preview and o1-mini starting today, while Enterprise and Edu users will get access early next week. OpenAI says it plans to bring o1-mini access to all the free users of ChatGPT but hasn’t set a release date yet. Developer access to o1 is really expensive: In the API, o1-preview is $15 per 1 million input tokens, or chunks of text parsed by the model, and $60 per 1 million output tokens. For comparison, GPT-4o costs $5 per 1 million input tokens and $15 per 1 million output tokens. The training behind o1 is fundamentally different from its predecessors, OpenAI’s research lead, Jerry Tworek, tells me, though the company is being vague about the exact details. He says o1 “has been trained using a completely new optimization algorithm and a new training dataset specifically tailored for it.” OpenAI taught previous GPT models to mimic patterns from its training data. With o1, it trained the model to solve problems on its own using a technique known as reinforcement learning, which teaches the system through rewards and penalties. It then uses a “chain of thought” to process queries, similarly to how humans process problems by going through them step-by-step. As a result of this new training methodology, OpenAI says the model should be more accurate. “We have noticed that this model hallucinates less,” Tworek says. But the problem still persists. “We can’t say we solved hallucinations.” The main thing that sets this new model apart from GPT-4o is its ability to tackle complex problems, such as coding and math, much better than its predecessors while also explaining its reasoning, according to OpenAI. “The model is definitely better at solving the AP math test than I am, and I was a math minor in college,” OpenAI’s chief research officer, Bob McGrew, tells me. He says OpenAI also tested o1 against a qualifying exam for the International Mathematics Olympiad, and while GPT-4o only correctly solved only 13 percent of problems, o1 scored 83 percent. In online programming contests known as Codeforces competitions, this new model reached the 89th percentile of participants, and OpenAI claims the next update of this model will perform “similarly to PhD students on challenging benchmark tasks in physics, chemistry and biology.” At the same time, o1 is not as capable as GPT-4o in a lot of areas. It doesn’t do as well on factual knowledge about the world. It also doesn’t have the ability to browse the web or process files and images. Still, the company believes it represents a brand-new class of capabilities. It was named o1 to indicate “resetting the counter back to 1.” “I’m gonna be honest: I think we’re terrible at naming, traditionally,” McGrew says. “So I hope this is the first step of newer, more sane names that better convey what we’re doing to the rest of the world.” I wasn’t able to demo o1 myself, but McGrew and Tworek showed it to me over a video call this week. They asked it to solve this puzzle: “A princess is as old as the prince will be when the princess is twice as old as the prince was when the princess’s age was half the sum of their present age. What is the age of prince and princess? Provide all solutions to that question.” The model buffered for 30 seconds and then delivered a correct answer. OpenAI has designed the interface to show the reasoning steps as the model thinks. What’s striking to me isn’t that it showed its work — GPT-4o can do that if prompted — but how deliberately o1 appeared to mimic human-like thought. Phrases like “I’m curious about,” “I’m thinking through,” and “Ok, let me see” created a step-by-step illusion of thinking. But this model isn’t thinking, and it’s certainly not human. So, why design it to seem like it is? OpenAI doesn’t believe in equating AI model thinking with human thinking, according to Tworek. But the interface is meant to show how the model spends more time processing and diving deeper into solving problems, he says. “There are ways in which it feels more human than prior models.” “I think you’ll see there are lots of ways where it feels kind of alien, but there are also ways where it feels surprisingly human,” says McGrew. The model is given a limited amount of time to process queries, so it might say something like, “Oh, I’m running out of time, let me get to an answer quickly.” Early on, during its chain of thought, it may also seem like it’s brainstorming and say something like, “I could do this or that, what should I do?” Building toward agents Large language models aren’t exactly that smart as they exist today. They’re essentially just predicting sequences of words to get you an answer based on patterns learned from vast amounts of data. Take ChatGPT, which tends to mistakenly claim that the word “strawberry” has only two Rs because it doesn’t break down the word correctly. For what it’s worth, the new o1 model did get that query correct. As OpenAI reportedly looks to raise more funding at an eye-popping $150 billion valuation, its momentum depends on more research breakthroughs. The company is bringing reasoning capabilities to LLMs because it sees a future with autonomous systems, or agents, that are capable of making decisions and taking actions on your behalf. For AI researchers, cracking reasoning is an important next step toward human-level intelligence. The thinking is that, if a model is capable of more than pattern recognition, it could unlock breakthroughs in areas like medicine and engineering. For now, though, o1’s reasoning abilities are relatively slow, not agent-like, and expensive for developers to use. “We have been spending many months working on reasoning because we think this is actually the critical breakthrough,” McGrew says. “Fundamentally, this is a new modality for models in order to be able to solve the really hard problems that it takes in order to progress towards human-like levels of intelligence.”"},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/12/24242897/google-gemini-unlists-misleading-video-ai","category":"Google","date":"Sep 12","author":"Jay Peters","title":"Google unlists misleading Gemini video","content":"Google has unlisted an impressive Gemini demo video it posted last December that seemed remarkably conversational. BBB National Programs’ National Advertising Division (NAD), an ad industry watchdog, inquired whether the video “accurately depicts the performance of Gemini in responding to user voice and video prompts.” Google chose to end the inquiry by ending its promotion of the video that showed Gemini quickly responding to various spoken prompts, such as identifying parts of drawings and creating a geography game on the fly. Buried in the description was a disclaimer indicating the demo might not be as good as it seemed: “For the purposes of this demo, latency has been reduced and Gemini outputs have been shortened for brevity.” Another note near the beginning of the video said, “Sequences shortened throughout.” Google DeepMind’s Oriol Vinyals also clarified that the video illustrated what “the multimodal user experiences built with Gemini could look like.” “Google is pleased to accept NAD’s resolution of this matter,” Google spokesperson Gareth Evans says in a statement to The Verge. “The video is still available in conjunction with the blog post that explains how the demonstration in the video was created.” Here’s that blog post, if you want to read it. Update, September 12th: Added statement from Google."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/12/24242789/meta-training-ai-models-facebook-instagram-photo-post-data","category":"Tech","date":"Sep 12","author":"Jess Weatherbed","title":"Meta fed its AI on almost everything you’ve posted publicly since 2007","content":"Meta has acknowledged that all text and photos that adult Facebook and Instagram users have publicly published since 2007 have been fed into its artificial intelligence models. Australia’s ABC News reports that Meta’s global privacy director, Melinda Claybaugh, initially rejected claims about user data from 2007 being leveraged for AI training during a local government inquiry about AI adoption before relenting after additional questioning. “The truth of the matter is that unless you have consciously set those posts to private since 2007, Meta has just decided that you will scrape all of the photos and all of the texts from every public post on Instagram or Facebook since 2007 unless there was a conscious decision to set them on private,” Green Party senator David Shoebridge pushed in the inquiry. “That’s the reality, isn’t it?” “Correct,” Claybaugh responded. Meta’s privacy center and blog posts acknowledge hoovering up public posts and comments from Facebook and Instagram to train generative AI: We use public posts and comments on Facebook and Instagram to train generative AI models for these features and for the open source community.We don’t use posts or comments with an audience other than Public for these purposes. But the company has been vague about how data is used, when it started scraping, and how far back its collection goes. Asked by The New York Times in June, Meta didn’t answer, other than to confirm that setting posts to anything besides “public” will prevent future scraping. That still won’t delete data that has already been collected — and people posting back in 2007 (who may have been minors at the time) wouldn’t have known their photos and posts would be used in this way. Claybaugh said that Meta doesn’t scrape data from users who are under the age of 18. When Labor Party senator Tony Sheldon asked if Meta would scrape the public photos of his children on his own account, Claybaugh confirmed it would and was unable to clarify if the company also scraped adult accounts that were created when the user was still a child. European users can opt out due to local privacy regulations, and Meta was recently banned from using Brazilian personal data for AI training, but the billions of Facebook and Instagram users in other regions can’t opt out if they want to keep their posts public. Claybaugh was unable to say if Australian users (or anyone else) would be given a choice to opt out in the future, arguing that the option was given to European users because of uncertainty regarding its regulatory landscape. “Meta made it clear today that if Australia had these same laws Australians’ data would also have been protected,” Shoebridge said to ABC News. “The government’s failure to act on privacy means companies like Meta are continuing to monetize and exploit pictures and videos of children on Facebook.”"},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/24242800/ai-image-editing-photoshop-misinformation-deepfakes-elon-musk-grok-decoder-interview","category":"Decoder","date":"Sep 12","author":"Nilay Patel","title":"Why comparing AI image editing to Photoshop downplays the risks","content":"We’ve been covering the rise of AI image editing very closely here on Decoder and at The Verge overall for several years now — the ability to create photorealistic images with nothing more than a chatbot prompt has the potential to completely reset our cultural relationship with photography and, in particular, how much we instinctively trust photos to reflect the truth. But the debate over image editing and the inherent truth of photos is nothing new, of course. It’s existed for as long as photography has existed, and it’s raged since digital photo editing tools have become widely available. You know this argument; you’ve heard it a million times. It’s when people say, “It’s just like Photoshop,” with “Photoshop” standing in for the concept of image editing generally. Today, we’re exploring that argument, trying to understand exactly what it means and why our new world of AI image tools is different — and yes, in some cases, the same. Verge reporter Jess Weatherbed recently dove into this for us, and I asked her to join me in going through the debate and the arguments one by one to help figure it out. Because, sure, in many ways, AI image editing really is just a faster, easier version of Photoshop — even Adobe now has AI tech like its Firefly image generator built right into Photoshop. But making powerful tools instantly accessible to everyone has side effects, and we’re seeing that right now. Say you want to generate an image of Donald Trump pointing a gun at Kamala Harris. Just ask Elon Musk’s Grok, the AI chatbot built right into X. It’ll do it no problem because it has very few of the same filters that have prevented competing AI products from depicting politicians or outright violence. How about a deepfake nude of a classmate? That’s now made more trivial than ever before thanks to so-called “nudification” apps that manipulate existing photos, and it’s fast becoming a national crisis. These might be old problems — Photoshop let you do all sorts of awful manipulations to celebrity photographs, and even in the days before computers, you could create convincing fake images to mislead people. But generative AI tools are testing whether the scale and sophistication of the tech and the speed of its adoption with little oversight has landed us firmly in uncharted territory. I’ll just be direct here: my view is that people say “it’s just like Photoshop” to diminish these new problems that AI tools are causing and to make them seem like they’re already solved or not worth considering. But I would remind you that we hardly solved any of those problems when it really was just Photoshop — and any proposed solution that requires everyone to fundamentally understand that every image they see is edited isn’t much of a solution at all."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/12/24219823/apple-intelligence-gemini-copilot-ai-iphone-16-pro-pixel-9-ram","category":"iPhone","date":"Sep 12","author":"Wes Davis","title":"There’s more to this year’s smartphones than AI","content":"Apple revealed its iPhone 16 lineup on Monday, and the big selling point was Apple Intelligence. Apple’s on-device AI system offers splashy features like the ability to rewrite emails, generate custom emoji, and a significantly upgraded Siri. But underneath it all, AI is delivering one other big change to the iPhone: more RAM. Although Apple never talks about RAM in its smartphones, MacRumors discovered that every iPhone 16 model now has 8GB of RAM, up from 6GB in the base models from last year. And it’s not just Apple making changes like that. Last month, Google made similar changes to its AI-heavy Pixel 9; both the standard and Pro models saw an increase in RAM, making 12GB the least you can get this year. The impetus behind these RAM bumps appears to be artificial intelligence. AI is the year’s new must-have feature, and it’s also incredibly RAM-hungry. Smartphone makers are now bumping memory because they need to — whether they’re saying that out loud or not. AI models need to be quick to respond when users call on them, and the best way to make that happen is to keep them perpetually loaded in memory. RAM responds far more quickly than a device’s long-term storage; it would be annoying if you had to wait for an AI model to load before you could grab a quick email summary. But AI models are also fairly large. Even a “small” one, like Microsoft’s Phi-3-mini, takes up 1.8GB of space, and that means taking memory away from other smartphone functions that were previously making use of it. You can see how this played out very directly on Pixel phones. Last year, Google didn’t enable local AI features on the standard model Pixel 8 due to “hardware limitations.” Spoiler: it was the RAM. Android VP and general manager Seang Chau said in March that the Pixel 8 Pro could better handle Gemini Nano, the company’s small AI model, because that phone had 4GB more RAM, at 12GB, than the Pixel 8. The model needed to stay loaded in memory at all times, and the implication was that the Pixel 8 would have lost too much memory in supporting the feature by default. “It wasn’t as easy a call to just say, alright, we’re just gonna enable it on the Pixel 8 as well,” Chau said. Google eventually allowed Gemini Nano onto the Pixel 8, but only for people willing to run their phones in Developer Mode — people who Chau said “understand the potential impact to the user experience.” Those tradeoffs are why Google decided to boost RAM across the board with the Pixel 9. “We don’t want the rest of the phone experiences to slow to accommodate the large model, hence growing the total RAM instead of squeezing into the existing budget,” Google group product manager Stephanie Scott said in an email exchange with The Verge. So, is all of that extra RAM just going to AI, or will users see improved performance across the board? It’s going to depend a lot on the implementation and just how large those models are. Google, which added 4GB to support local AI features, says you’ll see improvements to both. “Speaking only to our latest Pixel phones,” Scott wrote, “you can expect both better performance and improved AI experiences from their additional RAM.” She added that Pixel 9 phones “will be able to keep up with future AI advances.” But if those advances mean larger models, that could easily mean they’ll be eating up more RAM. The same RAM-boosting trend is playing out in the laptop world, too. Microsoft dictated earlier this year that only machines with at least 16GB of memory can be considered a Copilot Plus PC — that is, a laptop capable of running local Windows AI features. It’s rumored that Apple is planning to add more RAM to its next generation of laptops, too, after years of offering 8GB of RAM by default. That extra memory will be needed, especially if laptop makers want to keep even larger models loaded locally. “I think most OSes will keep a LLM always-loaded,” Hugging Face CTO Julien Chaumond told me in an email, “so 6-8GB RAM is the sweet spot that will unlock that in parallel to the other things the OS is already doing.” Chaumond added that models can then load or unload “a small model on top of it to change some properties,” such as a style for image generation or domain-specific knowledge for an LLM. (Apple describes its approach similarly.) Apple hasn’t explicitly said how much RAM is necessary to run Apple Intelligence. But every Apple device that runs it, going back to the 2020 M1 MacBook Air, has at least 8GB of RAM. Notably, last year’s iPhone 15 Pro, with 8GB of memory, can run Apple Intelligence, while the standard iPhone 15 with 6GB of RAM cannot. Apple AI boss John Giannandrea said in a June interview with Daring Fireball’s John Gruber that limitations like “bandwidth in the device” and the neural engine’s size would make AI features too slow to be useful on the iPhone 15. Apple VP of software engineering Craig Federighi said during the same appearance that “RAM is one of the pieces of the total.” The 2GB iPhone 16 RAM bump isn’t ultimately a lot, but Apple has long been slow to expand baseline RAM across its devices. Any increase here feels like a win for usability, even if the company is starting small. We still don’t know how useful Apple Intelligence will be or whether a slight jump in memory will be enough for today’s iPhones to run tomorrow’s AI features. One thing seems certain, though: we’ll be seeing more of these sorts of hardware bumps as AI proliferates across the industry."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/11/24242138/google-notebook-llm-ai-fake-podcasts-research","category":"Google","date":"Sep 11","author":"Emma Roth","title":"Google is using AI to make fake podcasts from your notes","content":"Google can now turn your research into an AI-generated podcast, complete with two “hosts” that discuss what you’ve dug up. The experimental feature lives within NotebookLM, the AI note-taking app Google launched last year, and will have AI hosts “summarize your material, make connections between topics, and banter back and forth.” It’s meant to build on NotebookLM’s existing features that help you interact with all your notes, transcripts, and other research documents. The app already uses Google’s Gemini AI model to help summarize your research, and this is sort of like an audio version of that. Google isn’t making things up when it says the AI hosts will “banter” with each other, either. When trying out Audio Overview for myself, I plugged in one of the sample notebooks about the invention of the lightbulb, and the results were... a bit uncanny. During the 10-minute-long overview, the two hosts had a lighthearted discussion about how Thomas Edison wasn’t the only person behind the lightbulb and that “in the end, it’s actually a story about teamwork, making the dream work.” The hosts could almost be mistaken for human podcasters, from the way they emphasized “bam!” when tossing it in the middle of a sentence to using modern phrasing like “messy as heck.” There were still a couple of quirks, as I noticed the AI spelling out certain words and phrases, like “P-L-U-S.” Some of the writing wasn’t exactly what a human would say, either, with one AI host calling platinum “bling bling metal.” The feature makes learning about research more engaging, but I’m wondering whether the hosts would maintain their lighthearted, somewhat jokey tone when discussing more serious topics, like cancer or war. There’s quite a bit of filler during the conversation as well, so it might not be the best way to quickly and clearly distill all your information. That’s something Google mentions in its announcement, as it says the feature is “not a comprehensive or objective view of a topic, but simply a reflection” of your notes. There are some other limitations to Audio Overview as well, as Google says it could take several minutes to generate a podcast-like discussion, and it’s only available in English. Like many AI tools, it isn’t always accurate. You can try out the feature for yourself by opening up a notebook in NotebookLM. From there, select the Notebook guide in the bottom-right corner of the screen, and then hit Load beneath the “Audio Overview” heading. I know I’m going to be doing some research on a random topic just so I can hear what the AI podcasters have to say."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/11/24242142/sag-aftra-ai-now-gavin-newsom-safety-sb-1047-letters","category":"Tech","date":"Sep 11","author":"Garrison Lovely","title":"One of California’s most influential unions weighs in on AI safety bill","content":"As California Governor Gavin Newsom weighs signing or vetoing the fiercely contested AI safety bill SB 1047, SAG-AFTRA and two women’s groups are pushing him to approve it — adding even more voices to an already frenzied debate. The performers union, the National Organization for Women (NOW), and Fund Her have each sent letters to Newsom, all of which have been obtained by The Verge and are being published here for the first time. The letters from SAG-AFTRA, NOW, and Fund Her highlight concerns about AI’s potential to cause catastrophic harm if the technology is left unregulated. SAG-AFTRA outlines SB 1047’s mandate for developers to test for and safeguard against AI-enabled disasters, like cyberattacks on critical infrastructure or bioweapon development. NOW and Fund Her cite grave warnings from people at the forefront of AI and discuss the technology’s potentially disproportionate impacts on vulnerable groups. SAG-AFTRA posted a call for support yesterday on X from its 160,000 members, which include stars like Scarlett Johansson and Tom Hanks. NOW, the largest feminist organization in the US with around 500,000 members, said it was motivated by expert claims “about how dangerous this incredible technology can be if it is not developed and deployed responsibly.” Fund Her, a PAC that has helped elect 12 progressive women to prominent positions in California in 2022, writes of the “race to develop the first independent thinking AI,” at which point “it will be too late to impose safety guardrails.” SAG-AFTRA and NOW represent the latest power players to weigh in on the California bill, which has become the object of exceptional national interest and scrambled conventional partisan boundaries. SB 1047, authored by state Senator Scott Wiener, would be the most significant AI safety law in the US. It establishes civil liability for developers of next-generation AI models like ChatGPT if they cause disasters without implementing appropriate safeguards. The bill also includes whistleblower protections for employees of AI companies, garnering support from OpenAI whistleblowers Daniel Kokotajlo and William Saunders. NOW writes in its letter that “the AI safety standards set by California will change the world,” a view echoed by bill cosponsor Dan Hendrycks, director of the Center for AI Safety. Hendrycks tells The Verge that SB 1047 could be Newsom’s “Pat Brown moment,” referring to California’s then-governor signing a groundbreaking auto tailpipe emissions law in 1966. He quotes what’s since become known as the California Effect: “where California leads on important regulation, the rest of the country follows.” Having passed both houses of the state legislature with strong majorities in late August, the bill now awaits Governor Newsom’s decision, due by September 30th. The governor’s office said it doesn’t “typically comment on pending legislation. This measure will be evaluated on its merits.” This comment notwithstanding, the fate of SB 1047 may come down to a political calculation — a reality each side appears to recognize as they marshal support in the bill’s final hours. The odd political coalitions that have emerged in the fight over SB 1047 augur a topsy-turvy future for AI policy battles. Billionaire Elon Musk aligns with social justice groups and labor unions in supporting the bill, while former House Speaker Nancy Pelosi, progressive House Congressman Ro Khanna, Trump-supporting venture capitalist Marc Andreessen, and AI “godmother” Fei-Fei Li are all opposed. AI is the rare issue that hasn’t yet sorted into clear partisan camps. As the technology grows in importance, the debate over how to govern it is likely to grow in intensity and may continue to scramble the usual allegiances. These recent letters join support for the bill from organizations like the nearly 2-million-strong SEIU and the Latino Community Foundation. SAG-AFTRA has been home to some of the most organic anti-AI sentiment. Many screen actors see generative AI as an existential threat to their livelihoods. The use of the technology was a major sticking point in the 2023 actors strike, which resulted in a requirement that studios get informed consent from performers before creating digital replicas of them (actors must also be compensated for their use). The union’s letter writes that “SAG-AFTRA knows all too well the potential dangers that AI poses,” citing problems experienced by its members in the form of nonconsensual deepfake pornography and theft of performers’ likenesses. It concludes that “policymakers have a responsibility to step in and protect our members and the public. SB 1047 is a measured first step to get us there.” In a phone interview, organization president Christian Nunes said NOW got involved because the group is worried about how unregulated AI can affect women. She and NOW have previously supported efforts to ban nonconsensual deepfakes. In the NOW letter, Nunes writes that the dangers warned of by AI experts “would disproportionately fall on vulnerable groups, including women.” She highlights Newsom’s “courageous support for us in the face of intense lobbying pressure” on reproductive rights, equal pay, and paid family leave, and that this support “is one of the reasons why women have voted for [him] time and time again.” While SB 1047 isn’t explicitly designed to address these groups’ more central concerns, the organizations seem to see strategic value in joining the coalition behind it. Nunes told The Verge she views the bill as part of a broader project to hold Big Tech accountable. This support for SB 1047 complements other pending AI legislation that more directly addresses these groups’ specific issues. For instance, the federal NO FAKES Act aims to combat deepfakes, while another AI bill on Newsom’s desk, endorsed by SAG-AFTRA, would regulate the use of digital replicas. By backing SB 1047 alongside these more targeted initiatives, these organizations appear to be taking a comprehensive approach to AI governance. The NOW and Fund Her letters both draw parallels between unregulated AI and the history of social media. Fund Her founder and president Valerie McGinty writes to The Verge, “We have seen the incredible harm social media has imposed on our children and how difficult it is to reverse it. We won’t be stuck playing catch up again if Governor Newsom signs SB 1047 into law.” It’s unclear if the letters will be enough for the bill to overcome the powerful forces arrayed against it. While Wiener and other advocates describe the regulation as “light-touch” and “common sense,” the industry is, by and large, freaking out. The US currently relies almost entirely on self-regulation and nonbinding voluntary commitments to govern AI, and the industry would like to keep it that way. As the first US AI safety regulation with teeth, SB 1047 would set a powerful precedent, which is a likely motivation behind both these letters and the vigorous industry opposition. Google, Meta, and OpenAI took the unusual step of writing their own letters opposing the bill. Resistance from AI investors has been even stiffer, with the prestigious startup incubator Y Combinator (YC) and the venture fund Andreessen Horowitz (a16z) leading a full-court press to kill SB 1047. These and other prominent opponents warn that the bill could prompt an exodus from California, cede the US lead in AI to China, and devastate the open source community. Naturally, supporters dispute each of these arguments. In a July letter addressing YC and a16z’s claims about the bill, Wiener points out that SB 1047 would apply to any covered AI company doing business in California, the world’s AI hub and fifth-largest economy. Dario Amodei, CEO of leading AI company and eventual de facto SB 1047 supporter Anthropic, called the threat to leave “just theater” (it has nonetheless also been invoked by OpenAI, Meta, and Google). In her statement opposing the bill, Pelosi called it “well-intentioned but ill informed.” In a phone interview, Wiener said, “I have enormous respect for the Speaker Emerita. She is the GOAT,” but went on to call Pelosi’s statement “unfortunate” and noted that “some of the top machine learning pioneers on the planet support the bill,” citing endorsements from deep learning “godfathers” Geoffrey Hinton and Yoshua Bengio. Wiener also highlights a supportive open letter published Monday from over 100 employees and alumni of the leading AI companies. For evaluating SB 1047 on its merits, the most convincing letter might be one published by Anthropic, which broke from its peers to write that the revised legislation’s “benefits likely outweigh its costs.” This letter followed a round of amendments made directly in response to the company’s prior complaints. Anthropic’s Claude family of chatbots leads the world on some metrics, and the company will likely be one of the handful of AI developers directly covered by the law in the near future. With key congressional leaders promising to obstruct substantive federal AI regulations and opposing SB 1047, California may go it alone, as it already has on net neutrality and data privacy. As NOW’s Nunes writes, the “AI safety standards set by California will change the world,” giving Governor Newsom a chance to make history and model “balanced AI leadership.” Fund Her’s McGinty summed up the supporters’ stance in an email to The Verge: “We should listen to these experts more interested in our wellbeing than the Big Tech executives skimping on AI safety.” As the September 30th deadline approaches, all eyes are on Governor Newsom to see how he’ll shape the future of AI governance in California and beyond. “My experience with Gavin Newsom is — agree or disagree — he makes thoughtful decisions based on what he thinks is best for the state,” says Wiener. “I’ve always appreciated that about him.” Correction: The article initially cited deep learning “godfather” Yann LeCun as a supporter of SB 1047. LeCun is opposed to the bill. We regret the error."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/11/24241649/adobe-firefly-text-to-video-generative-ai-features-preview","category":"Adobe","date":"Sep 11","author":"Jess Weatherbed","title":"Adobe previews its upcoming text-to-video generative AI tools","content":"Adobe has teased some of its upcoming generative AI video tools, including a new feature that can produce video clips from still images. This latest preview builds on the in-development Firefly video model that the software giant demonstrated in April, which is set to power AI video and audio editing features across Adobe’s Creative Cloud applications. The new promotional teaser shows footage produced by Firefly’s text-to-video capabilities that Adobe announced (but didn’t demonstrate) earlier this year. The tool allows users to generate video clips using text descriptions and adjust the results using a variety of “camera controls” that simulate camera angles, motion, and shooting distance. An image-to-video feature was also demonstrated for the Firefly video model that can generate clips using specific reference images. Adobe suggests this could be useful for making additional B-roll footage or to patch gaps in production timelines. If the example footage is any indication of the final release, the generated video quality looks on par with what we’ve seen from OpenAI’s Sora model so far, which Adobe is also “exploring” as a third-party integration for its Premiere Pro video software. Duration is limited, though, with Adobe’s VP of generative AI, Alexandru Costin, telling The Verge that videos produced by the text-to-video and image-to-video features have a maximum length of five seconds. One advantage Adobe’s own model may have against Sora is its promise that Firefly is “commercially safe” due to being trained on openly licensed, public domain, and Adobe Stock content, which could reduce some concerns about copyright infringement. The text-to-video and image-to-video features will both initially be available in beta as a standalone Firefly application sometime later this year. Adobe says the new Firefly video model will eventually be integrated into its Creative Cloud, Experience Cloud, and Adobe Express applications. The company also showed off some additional clips of the upcoming “Generative Extend” feature for Premiere Pro that can extend the length of existing video footage, similar to Photoshop’s Generative Expand tool for image backgrounds. Adobe says this will also be arriving on an unspecified date “later this year.”"},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/10/24241538/taylor-swift-endorses-kamala-harris-donald-trump-ai-endorsement-deepfake","category":"Tech","date":"Sep 11","author":"Mia Sato","title":"Taylor Swift endorses Kamala Harris in response to fake AI Trump endorsement","content":"Taylor Swift said on Tuesday that she plans to vote for Vice President Kamala Harris in November’s presidential election — and that AI-generated images circulating of herself pushed her in part to make her support public. “Recently I was made aware that AI of ‘me’ falsely endorsing Donald Trump’s presidential run was posted to his site. It really conjured up my fears around AI, and the dangers of spreading misinformation,” Swift wrote in an Instagram post. “It brought me to the conclusion that I need to be very transparent about my actual plans for this election as a voter. The simplest way to combat misinformation is with the truth.” Her post references an incident in late August in which Trump shared a collection of images to Truth Social intended to show support for his presidential campaign. Some of the photos depict “Swifties for Trump,” and another obviously AI-generated photo shows Swift herself in an Uncle Sam-type image with text reading, “Taylor wants YOU to vote for Donald Trump.” The former president captioned the post, “I accept!” In her endorsement post, Swift also mentioned LGBTQ+ rights, reproductive care, and IVF as specific issues she cares about. She also directed fans to her Instagram story, where she added a link to register to vote. The potential for abuse of AI tools in the lead-up to the US presidential election has been a lingering concern as generative AI has become widely available. In January, before Harris ascended to the top of the Democratic ticket, some voters in New Hampshire got a fake AI-generated robocall that sounded like President Joe Biden. The call — which discouraged people from voting in the state’s upcoming primary election — went to more than 20,000 people, according to CNN. Some AI companies have increased restrictions on tools in an effort to cut down on election-related misinformation. Google, for example, recently announced it would limit election queries in AI Overviews, the company’s AI-generated search results feature. This wasn’t the first time AI images of Swift were circulated on social media. Earlier this year, nonconsensual sexualized images of her made using AI were shared on X. That incident prompted the White House to call for legislation to “deal” with the issue."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/10/24237714/apple-intelligence-generative-ai-features-update-schedule","category":"Apple","date":"Sep 10","author":"Umar Shakir","title":"Here’s what your iPhone 16 will do with Apple Intelligence — eventually","content":"Apple heavily sprinkled mentions of AI throughout its iPhone 16 event on Monday. However, generative Apple Intelligence features won’t be ready for the public launch of iOS 18 on September 16th or the new iPhones when they’re released on September 20th. The first set of Apple’s AI features is scheduled for public availability next month in most regions — except the EU — as part of a beta test for iPhone 15 Pro and all iPhone 16s, plus Macs and iPads with M1 or higher Apple Silicon chips. At launch, they’ll be available in US English only. What’s coming to Apple Intelligence in October Writing Tools Text Rewrite: Text Rewrite will morph your email writing draft into a more professional one, and you can change the tone to be friendly or concise as well.Proofread: As in real life, this proofreading feature should correct your grammar and sentence structure and suggest better words throughout your work.Summarize Text: It will be like letting AI do a TL;DR for you. Summarize Text will shorten your writing to just the important parts or create a bulleted list or table. Smart Reply: We’ve seen this AI feature shown off quite a bit. Smart Reply will give you a few contextual suggestions to get you started on a reply in Mail or elsewhere. New Siri New look: On iPhone, iPad, or CarPlay, Siri will appear as a rainbow ring around the edges of the screen, and on Mac, Siri can float and be placed anywhere on your desktop.Apple’s new language model: Siri should also get a bit smarter and better at parsing natural language thanks to Apple’s on-device language model. Meanwhile, more complex questions will be sent to Apple’s “Private Cloud Compute” server, which Apple claims acts as a computational extension to your device and does not retain any data.Type to Siri: Instead of talking, you’ll be able to type questions to the assistant anytime. Photos Clean Up: Similar to Google’s Magic Eraser, Clean Up will remove unwanted objects in your photos.Search: You’ll be able to search for photos using natural language to find specific subjects you’re looking for but can’t find scrolling through your library.Memories: You’ll be able to make a movie using media from your Photos library by writing out a prompt, and it should create a narratively driven story with chapters. Transcription Phone call recording and transcription: You’ll be able to record phone calls and get a transcription of the whole call. Activating this feature will tell all parties that the call is being recorded.Voice recordings in Notes: You’ll be able to record audio within the actual Notes app, and it will transcribe speech into text. You can also use Apple’s other writing tools to help summarize the whole session. These Apple Intelligence features are arriving later Apple says other AI features will “roll out later this year and in the months following.” That means these features could arrive as soon as October, or they could arrive next summer or fall. Unfortunately, these are also some of the most eye-catching features coming to Apple Intelligence. Visual Intelligence: Apple’s new Visual Intelligence introduced during the iPhone 16 presentation can search for things by just snapping a photo. You could, for instance, take a picture of a cafe storefront and get information about it, like hours and its menu, or take a photo of a concert poster and add it to your calendar. Visual Intelligence, when it arrives, will be activated using the Camera Control side button on iPhone 16 and 16 Pro.Genmoji: You’ll be able to create your own emoji by entering a text prompt, and Apple’s image generator will make you a new emoji you can send to friends. Image Playground: In addition to making custom emoji, Apple Intelligence will also eventually create custom images. Enter a text prompt for whatever image you’d like (assume some actual restrictions will apply), and Apple’s models will conjure up a picture for you. Siri Personal Context: Siri’s usefulness will evolve later by contextually helping you with onscreen information on your iPhone, iPad, or Mac.OpenAI connection: Anywhere there are Apple Intelligence writing tools, you will also have the option to use ChatGPT for additional generative AI options. ChatGPT should also be able to process your Siri requests for more advanced answers to questions.Third-party app connections with Siri: Apple’s also promising Siri will, one day, complete in-app requests, like making photo edits in an image editing app using pictures in your Photos app."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/10/24240955/espn-generative-ai-reports-womens-soccer-lacrosse-premier-league","category":"Sports","date":"Sep 10","author":"Wes Davis","title":"ESPN’s AI-generated sports recaps are already missing the point","content":"This weekend, ESPN began publishing AI-generated recaps of women’s soccer games, with more sports to come. It’s using Microsoft AI to write each story, with humans only involved in reviewing each recap for “quality and accuracy.” ESPN says these stories will “augment,” rather than detract from, its other content — but needless to say, people have feelings about it. It’s not that ESPN is masquerading AI work as that of humans. In fact, each story advertises that it’s written by “ESPN Generative AI Services,” and ESPN includes a note at the bottom of each article about how the recap is based on a transcript from the sporting event. ESPN isn’t the only news organization that does this; The Associated Press started using AI to write sports recaps back in 2016, and both organizations pitch this as a way to cover more underserved sports. In addition to soccer, ESPN will also use it for lacrosse. But so far, the stories are very bland, basic write-ups — and they’re already missing important nuance, as Parker Molloy points out. One of the National Women’s Soccer League stories failed to mention the significance of one player’s final game and the emotional moments that happened as a result, something ESPN waved at with a later update to the story. ESPN argued that the AI summaries free up its writers to focus on more in-depth work like “more differentiating features, analysis, investigative, and breaking news coverage,” and in this instance, a human reporter did write an entire story about Alex Morgan’s emotional exit. Columnist Tom Jones wrote for Poynter last week that despite ESPN’s justification that AI frees up journalists for more impactful work, there’s nothing stopping ESPN “from using AI to cover more and more other sports” down the line. Jones points to Luis Paez-Pumar’s column for Defector, where he writes that ESPN is “feeding existing soccer and lacrosse journalists’ work into a machine aimed at making them obsolete” rather than hiring them to do this work. ESPN says it does indeed plan to extend these AI recaps to more sports. Soccer and lacrosse are merely “its first experimentation with AI-generated content.” Musicians, news organizations, and other creatives are fighting the rise of AI in court, arguing it trains on the work of humans without permission."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/10/24241043/apple-iphone-16-pro-intelligence-ai-missing","category":"Apple","date":"Sep 10","author":"Jay Peters","title":"The iPhone 16 will ship as a work in progress","content":"Apple’s all in on AI — at least Apple’s version. “The next generation of iPhone has been designed for Apple Intelligence from the ground up,” Apple CEO Tim Cook said before revealing the iPhone 16. Software chief Craig Federighi pitched Apple Intelligence as a “personal intelligence system” that’s at “the heart of the iPhone 16 lineup.” After the event, Apple even published a whole press release dedicated to Apple Intelligence. There’s just one catch: when the iPhone 16 and 16 Pro first come out, they won’t have any Apple Intelligence features. Sure, the new A18 and A18 Pro chips in the iPhone 16 lineup each have a 16-core Neural Engine that Apple says is “optimized for large generative models,” so they will probably be good at handling Apple Intelligence features. Yes, Apple has been testing Apple Intelligence upgrades, like a new design for Siri and tools that can help you improve your writing, remove objects from photos, and summarize notifications, as part of an iOS 18.1 beta for developers. But unless you’re running that beta, you won’t be able to put those features to the test for a while. A few Apple Intelligence features will arrive soon-ish, as Apple gave a vague October release window for iOS 18.1 as part of its iPhone 16 announcements. But I should note that the Apple Intelligence features will still be called a beta and only available in US English to start. (Apple says it will launch Apple Intelligence in Chinese, French, Japanese, and Spanish starting next year.) The company’s arguably more powerful Apple Intelligence upgrades, like a tool to make images, a feature that lets you generate custom emoji, Siri improvements that let it understand your personal context, and integration with ChatGPT are rolling out on a very vague timeline of “later this year and in the months following.” There are some indications about when: Bloomberg’s Mark Gurman reports that the image generation features will launch with iOS 18.2 in December, and Apple said at WWDC that the ChatGPT integration is set to launch “later this year.” But despite how much of a spotlight Apple is putting on its AI features, it’s being quite cagey about when those features might actually come out. Apple didn’t reply to a request for comment. Despite how much the big tech companies have talked about AI over the past year or two, there are still concerns about AI tools, too. There’s the hallucinating and potential generating bad stuff and misinformation. And Apple Intelligence hasn’t exactly wowed beta testers with innovations or must-have tools. Apple’s slow rollout could give it time to work out issues. But do you want to wait around until it does? If you were looking at Apple’s AI features as the main reason to get a new phone, you probably shouldn’t. Maybe just wait to upgrade until next year — or at least until October."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/9/24240230/ios-18-release-date-iphone-16","category":"Apple","date":"Sep 9","author":"Emma Roth","title":"iOS 18 will launch next week with new ways to customize your homescreen","content":"Apple has revealed the launch date for iOS 18 — and it’s just days away. The update, which will add new ways to customize your iPhone’s homescreen and lockscreen, arrives on September 16th. In iOS 18, you can freely rearrange apps and widgets on your homescreen and change their appearance. Apple is rolling out a redesigned Control Center, too, along with a new password management app and support for satellite messaging. Some other updates include new text effects in messages, a revamped Photos app, and new ways to organize your inbox in the Mail app. However, this update doesn’t include the Apple Intelligence features coming to the iPhone 15 Pro and across the iPhone 16 lineup. The upcoming tools will help you rewrite and summarize text and generate images through the AI-powered Image Playground. Alongside a more conversational Siri, these features are going to start arriving in the iOS 18.1 public beta in October. Apple also confirmed that macOS Sequoia, watchOS 11, and visionOS 2 are coming September 16th as well. While macOS Sequoia adds iPhone mirroring, watchOS 11 comes with new training features, and visionOS 2 includes an ultrawide virtual Mac display. There’s still no word on tvOS 18. Related:"},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/9/24240094/apple-visual-intelligence-camera-control-iphone-16-ai-camera-control-google-lens","category":"Apple Event","date":"Sep 9","author":"Wes Davis","title":"Apple’s Visual Intelligence is a built-in take on Google Lens","content":"Apple has announced a new feature called Visual Intelligence that will be part of iOS 18’s Apple Intelligence suite of AI features “later this year.” The feature works much like similar features offered by other multimodal AI systems from Google or OpenAI. Visual Intelligence lets you “instantly learn about everything you see,” Apple’s Craig Federighi said during the company’s September event today. Federighi said the feature is “enabled by Camera Control,” which is the company’s name for a new capacitive camera button that’s now on the side of the iPhone 16 and 16 Pro phones. To trigger it, users will need to click and hold the button, then point the phone’s camera at whatever they’re curious about. iPhones use a “combination of on-device intelligence and Apple services that never store your images” to power Visual Intelligence and let you take a picture of a restaurant to get info about its hours. Point your camera at a flyer, and “details like title, date, and location are automatically recorded,” he said. Federighi added that the feature is “also your gateway to third-party” models, which suggests using Visual Intelligence to search Google for a bike that you find out in the wild or take a picture of study notes to get help with a concept. Apple didn’t announce when the feature would debut beyond that it’s “coming to Camera Control later this year.” Related:"},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/9/24236176/apple-iphone-16-liveblog-apple-watch-x-airpods-keynote","category":"Apple Event","date":"Sep 9","author":"Verge Staff","title":"iPhone 16 event live blog: all the news from Apple’s keynote","content":"It’s glowtime, baby. Apple is expected to announce the iPhone 16 today — and the launch of Apple Intelligence along with it. Apple is late to the AI party, which makes its arrival all the more intriguing. While we got a glimpse of Apple Intelligence at WWDC, we’re expecting a more practical look at how it’ll show up in the iPhone 16 and 16 Pro (plus all the usual upgrades like new colors, better cameras — the whole shebang). But phones and AI aren’t the only things on the agenda. It’s the 10th anniversary of the Apple Watch, and rumor has it, it’ll sport a bigger display and thinner body. The AirPods lineup is also due for a refresh, with murmurs of new entry and midtier models. That’s a jam-packed agenda, and the event kicks off at 1PM ET / 10AM PT. We’re on the ground reporting live from Cupertino, California, and you’ll also be able to follow along with the stream here."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/9/24239903/amazon-audible-audiobook-narrators-ai-generated-voice-clones","category":"Amazon","date":"Sep 9","author":"Jess Weatherbed","title":"Amazon is allowing Audible narrators to clone themselves with AI","content":"Amazon will begin inviting a small group of Audible narrators to train AI-generated voice clones of themselves this week, with the aim of speeding up audiobook production for the platform. The US-only beta test was announced on Audible’s creator marketplace and will be extended to rights holders like authors, agents, and publishers “later this year,” according to Amazon. “There is a vast catalog of books that does not yet exist in audio and as we explore ways to bring more books to life on Audible, we’re committed to thoughtfully balancing the interests of authors, narrators, publishers, and listeners,” Amazon said in its announcement. Participants in the beta will submit a voice recording to train their AI replica and will retain control over the projects they wish to audition for across both live performances and AI-generated recordings. Narrators can also use Amazon’s production tools to edit the pronunciation and pacing of their AI voice replica if a rights holder selects them for a project, alongside reviewing the final production for any errors or inaccuracies. Amazon says that narrators will be compensated via a “Royalty Share” model on a “title-by-title basis” but didn’t expand on how much voice artists can expect to earn. The announcement blog says that beta participants can create a voice replica “for free,” which implies that there may be an upfront cost involved for narrators in the future if the feature becomes generally available. Any titles that are narrated using voice replicas will be labeled on the product detail page. “Narrators control what works are narrated with their voice replica,” Amazon said. “Audible will not separately use a narrator’s voice replica for any content without their approval.” Amazon rolled out a similar feature last year that allows Kindle Direct Publishing authors to convert their titles into audiobooks using fully synthetic voices. Bloomberg reported in May that virtual voices were used on 40,000 Audible titles since release, sparking concerns from narrators like Ramon de Ocampo about the feature reducing job opportunities for human performers. As outlets like Brian’s Book Blog have noted, Audible currently doesn’t provide an easy way for users to filter out these “Virtual Voice” audiobooks if they’re aiming to avoid them."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/7/24238537/apples-glowtime-iphone-16-event-how-to-watch-ios-18-apple-intelligence-ai","category":"Apple","date":"Sep 7","author":"Wes Davis","title":"How to watch Apple’s ‘Glowtime’ iPhone 16 event","content":"The iPhone 16 is nigh as Apple prepares to debut its next smartphone lineup on Monday, September 9th, at 1PM ET / 10AM PT. The company is expected to officially announce four phones as usual, with two standard iPhone 16s and two iPhone 16 Pro models, all likely packed with Apple Intelligence AI features. You can catch the livestream at Apple’s own website, on its YouTube channel, and even on an Apple TV. (Apple usually makes sure you see this when you scroll from the homescreen of the Apple TV app, but you can search for “Apple Event” to find it, too.) Besides new iPhones, Apple will probably declare the release dates of its next major software updates, including iOS 18, iPadOS 18, macOS 15 Sequoia, watchOS 11, and visionOS 2. The first three of those operating systems will include Apple Intelligence when it debuts, though you’ll need at least an iPad or Mac with an M1 chip or an iPhone 15 Pro to take advantage. We could also see new AirPods and possibly even a smaller Mac Mini, although that may be launched with other new Macs later this year instead."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/5/24236980/us-signs-legally-enforceable-ai-treaty","category":"Tech","date":"Sep 5","author":"Emma Roth","title":"US, EU, UK, and others sign legally enforceable AI treaty","content":"The US, UK, and European Union have signed the first “legally binding” treaty on AI, which is supposed to ensure its use aligns with “human rights, democracy and the rule of law,” according to the Council of Europe. The treaty, called the Framework Convention on Artificial Intelligence, lays out key principles AI systems must follow, such as protecting user data, respecting the law, and keeping practices transparent. Each country that signs the treaty must “adopt or maintain appropriate legislative, administrative or other measures” that reflect the framework. Andorra, Georgia, Iceland, Norway, the Republic of Moldova, San Marino, and Israel also signed the framework, which has been in the works since 2019. Over the past several months, we’ve seen a swath of other AI safety agreements emerge — but the majority don’t have consequences for the signatories who break their commitments. Even though this new treaty is supposed to be “legally binding,” the Financial Times points out that “compliance is measured primarily through monitoring, which is a relatively weak form of enforcement.” Still, the treaty could serve as a blueprint for countries developing their own laws surrounding AI. The US has bills in the works related to AI, the EU already passed landmark regulations on AI, and the UK is considering its own. California is also getting close to passing an AI safety law that giants like OpenAI have pushed back against. “We must ensure that the rise of AI upholds our standards, rather than undermining them,” Council of Europe Secretary General Marija Pejčinović Burić says in a statement. “The Framework Convention is designed to ensure just that. It is a strong and balanced text — the result of the open and inclusive approach.” The treaty will come into force three months after five signatories ratify it."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/5/24232414/xockets-accuses-nvidia-microsoft-patent-infringement-antitrust","category":"Policy","date":"Sep 5","author":"Lauren Feiner","title":"Startup accuses Nvidia and Microsoft of infringing on patents and forming a cartel","content":"A startup funded by the cofounder of Yahoo and CTO of Intel is suing Nvidia and Microsoft for allegedly infringing on its patent for a key innovation in AI chips and being part of a buying cartel that allegedly sought to artificially fix lower prices for the technology. In a new lawsuit, Texas-based Xockets says Nvidia has infringed on its patented data processing unit (DPU) technology, which helps make cloud infrastructure more efficient by accelerating data-intensive workloads. Xockets says the chip giant inherited the infringement through its 2020 acquisition of Mellanox. It claims Mellanox initially infringed on its patent after Xockets publicly demonstrated its DPU tech at a conference in 2015. Xockets alleges that three of Nvidia’s DPUs — BlueField, ConnectX, and NVLink Switch — are based on Xockets’ patented technology. The startup also accuses Microsoft of infringing on its patents, alleging that as an Nvidia customer, Microsoft has “privileged access to NVIDIA’s infringing GPU-enabled server computer systems and components for AI.” Xockets says it’s made Nvidia aware of the alleged infringement — it alleges the startup’s founder and board member Parin Dalal raised the issue to Nvidia’s DPU business VP in February 2022. Xockets accuses Nvidia of pursuing a strategy of “efficient infringement,” which basically boils down to infringe now, let lawyers figure out the rest later. Xockets is also accusing Nvidia of monopolizing the market for GPU servers for AI and participating with Microsoft in a buying cartel through an organization called RPX, a company Xockets says was “formed at the request of Big Tech companies to enable and create buyers’ cartels for intellectual property.” Xockets alleges that RPX enabled members like Nvidia and Microsoft to jointly boycott innovations like Xockets’ in order to drive prices lower than if each company had negotiated on its own. Through the alleged cartel, Xockets claims, Microsoft and Nvidia are able to “monopolize GPU-enabled generative artificial intelligence by controlling the equipment and platforms necessary to access this capability.” Xockets is seeking damages for the alleged infringement and for the court to order the companies to stop violating its patents and antitrust law. Though it’s facing two of the largest companies in the country, Xockets investor and board member Robert Cote, an IP lawyer, told The Verge that Xockets has “more than enough wherewithal to take on Goliath.” Dalal is a current employee at Google, where he’s a principal engineer of machine learning and AI, though Google does not seem to have an official role in the litigation. Cote said he could not comment on Google. Nvidia and Google declined to comment. Microsoft and RPX did not immediately respond."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/5/24236841/youtube-ai-detection-tools-creators-singing-deepfakes","category":"YouTube","date":"Sep 5","author":"Jess Weatherbed","title":"YouTube is making new tools to protect creators from AI copycats","content":"YouTube is developing new tools that aim to give creators on the platform more control over content that copies their voice or likeness using generative AI. In its announcement post, YouTube said the new likeness management tech will help to safeguard its creators and partners while enabling them to “harness AI’s creative potential” by promoting responsible AI development. The first tool, described as a “synthetic-singing identification technology,” will allow artists and creators to automatically detect and manage YouTube content that simulates their singing voices using generative AI. YouTube says the tool sits within its existing Content ID copyright identification system and that it’s planning to test it under a pilot program next year. The announcement follows YouTube’s pledge last November to give music labels a way to take down AI clones of musicians. The rapid improvement and accessibility of generative AI music tools have sparked fears among artists regarding their use in plagiarism, copycatting, and copyright infringement. In an open letter earlier this year, over 200 artists, including Billie Eilish, Pearl Jam, and Katy Perry, described unauthorized AI-generated mimicry as an “assault on human creativity” and demanded greater responsibility around its development to protect the livelihoods of performers. A separate tool is also in the works that can identify facial deepfakes of creators, actors, musicians, and athletes on the platform. The system is still in active development, and YouTube hasn’t indicated when it’s expected to roll out. YouTube is also pledging to crack down on anyone scraping the platform to build AI tools. “We’ve been clear that accessing creator content in unauthorized ways violates our Terms of Service,” the platform said — which hasn’t prevented companies like OpenAI, Apple, Anthropic, Nvidia, Salesforce, and Runway AI from training their AI systems on thousands of scraped YouTube videos. The protections against this activity include blocking scrapers from accessing YouTube and investments in scraping detection systems. “As AI evolves, we believe it should enhance human creativity, not replace it,” YouTube said in its announcement. “We’re committed to working with our partners to ensure future advancements amplify their voices, and we’ll continue to develop guardrails to address concerns and achieve our common goals.” YouTube also says it’s developing ways to give creators more choices regarding how third-party AI companies are permitted to use their content on the platform and will share further details later this year."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/5/24236817/google-ask-photos-ai-assistant-test","category":"Google","date":"Sep 5","author":"Emma Roth","title":"Google tests its ‘Ask Photos’ AI assistant that understands what’s in your pictures","content":"Google is testing its new “Ask Photos” feature that lets you explore your library of pictures in new ways. The feature, which Google first previewed in May, is rolling out to select Google Labs users in the US and will let you ask things like, “Where did we camp last time we went to Yosemite?” or “What did we eat at the hotel in Stanley?” Using Google’s Gemini AI models, the Photos app will then offer a response based on the content in your photos, as well as pull up images relevant to your question. Google says you can even use Ask Photos to complete tasks, such as summarizing things you did on a recent vacation or choosing the best pictures of your family to put in a shared album. You can sign up for the waitlist to access Ask Photos on Google’s website. When using Ask Photos, Google will let you switch to what it now calls “classic search” — or the current way of finding images. But Google is giving this an upgrade, too, as you can now search for pictures using natural language, such as “Alice and me laughing” or “Kayaking on a lake surrounded by mountains.” You can then sort your search results by date or relevance. This feature is rolling out in English on Android and iOS, with more support for more languages arriving in the “coming weeks.” In preparation for this change, Google Photos replaced the Library tab with a new Collection page that’s supposed to make it easier to find all your photos and videos. While I haven’t really had time to explore the new tab, I’ll definitely be taking advantage of the natural language search so I can finally find specific images without having to scroll through thousands of images or narrow them down by location."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/5/24236680/microsoft-onedrive-october-event-copilot-mobile-photos-updates","category":"Microsoft","date":"Sep 5","author":"Tom Warren","title":"Microsoft to detail OneDrive Copilot, mobile app updates, and more during October event","content":"Microsoft is holding a OneDrive digital event on October 8th that will cover the “latest innovations in AI across Microsoft 365 and OneDrive.” It’s the second annual event after Microsoft held a similar stream last year to introduce a big new design update for the cloud storage service, AI Copilot integration, and lots more. This year, Microsoft is promising to announce “what’s coming for Copilot in OneDrive,” alongside enhancements to the OneDrive mobile app and an “improved photos experience.” The event will also cover OneDrive features across work and personal accounts. The OneDrive digital event will be hosted on Microsoft Teams and will include the ability to ask the OneDrive product team questions about the cloud storage service. It follows a special Copilot event on September 16th, where Microsoft CEO Satya Nadella and vice president of AI at work Jared Spataro will focus on what’s next for Microsoft’s AI assistant. Microsoft is expected to announce a subtle rebranding of its business-focused Copilot assistant and introduce new Copilot features for Microsoft 365 that will try to tempt more businesses to sign up for the $30 per user per month service."},{"page":"https://www.theverge.com/ai-artificial-intelligence/archives/4","link":"https://www.theverge.com/2024/9/4/24235910/asus-nuc-14-pro-ai-copilot-button-mini-pc","category":"Microsoft","date":"Sep 4","author":"Tom Warren","title":"Asus’ new mini PC has a Copilot AI button on the front for some reason","content":"Asus is launching its latest NUC mini PC today, complete with Intel’s new Core Ultra Series 2 processors. For some reason, Asus has decided to put a dedicated Copilot button on the front of this puck-shaped PC, so you can reach out and launch Microsoft’s AI assistant. I’m not entirely sure why you’d want an AI button on a miniature PC that’s probably going to sit more than an arm’s length away from you on a desk, but it’s a button that has now progressed beyond Microsoft’s effort to push it on keyboards. Asus also has a fingerprint reader on top of this NUC for Windows Hello authentication, which makes more sense so you can touch the fingerprint reader when you initially power on this mini PC. Aside from the Copilot button, this NUC is packed full of essential ports and connectivity. At the front, there are two USB-A 3.2 Gen 1 ports, a Thunderbolt 4 port, and a headphone jack. At the rear of the NUC 14 Pro AI, there’s an ethernet port, another Thunderbolt 4 port, an HDMI port, and two USB-A 3.2 Gen 2 ports. Asus has also equipped this mini PC with Wi-Fi 7 and Bluetooth 5.4 support. There’s even an internal speaker with a microphone, making this mini PC useful if you don’t have a great speaker setup or headphones. The most interesting part is what’s inside, though. Intel is promising big performance improvements with its latest Lunar Lake processors inside the NUC 14 Pro AI, with up to 2x GPU performance over the previous generation. Asus is also using a high-end fluid dynamic bearing fan to improve heat dissipation and keep Intel’s chip running cool under high loads. The Asus NUC 14 Pro AI is also a Copilot Plus PC, so you’ll get access to Microsoft’s latest Windows AI features through an update that will be available in November. We don’t have a price for the Asus NUC 14 Pro AI, but it’s supposed to arrive later this year."}]